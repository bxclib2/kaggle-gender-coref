{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train= pd.read_pickle('./temp_result/train_kaggle_processed')\n",
    "gap_test= pd.read_pickle('./temp_result/test_kaggle_processed')\n",
    "gap_valid= pd.read_pickle('./temp_result/valid_kaggle_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(A,B):\n",
    "    if A is True:\n",
    "        return 0\n",
    "    if B is True:\n",
    "        return 1\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>...</th>\n",
       "      <th>B_pos</th>\n",
       "      <th>pron_pos</th>\n",
       "      <th>A_idx</th>\n",
       "      <th>B_idx</th>\n",
       "      <th>pron_idx</th>\n",
       "      <th>A_vector</th>\n",
       "      <th>B_vector</th>\n",
       "      <th>pron_vector</th>\n",
       "      <th>seq_length</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>test-619</td>\n",
       "      <td>Fullerton's book, Jane Austen and Crime (2004)...</td>\n",
       "      <td>her</td>\n",
       "      <td>146</td>\n",
       "      <td>Fullerton</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Jane Austen</td>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025806</td>\n",
       "      <td>0.174194</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[7, 8, 9, 10]</td>\n",
       "      <td>[34]</td>\n",
       "      <td>[[0.5251928, -1.0891725, -0.38272008, -0.15374...</td>\n",
       "      <td>[[0.12799127, 0.025941074, -1.1145829, 0.20098...</td>\n",
       "      <td>[[0.047446743, 0.28261346, -0.27585685, -0.506...</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>test-353</td>\n",
       "      <td>At a popular bar frequented by fashion magazin...</td>\n",
       "      <td>she</td>\n",
       "      <td>108</td>\n",
       "      <td>Amanda</td>\n",
       "      <td>67</td>\n",
       "      <td>False</td>\n",
       "      <td>Betty</td>\n",
       "      <td>79</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>[15, 16]</td>\n",
       "      <td>[18, 19]</td>\n",
       "      <td>[26]</td>\n",
       "      <td>[[0.09754669, -0.024278898, -0.15501401, 0.017...</td>\n",
       "      <td>[[0.00817636, 0.6316583, -0.17376839, 0.220072...</td>\n",
       "      <td>[[-0.9807809, 0.6089577, 0.37208742, -0.193520...</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>test-1735</td>\n",
       "      <td>Child's friend, Harriet Winslow Sewall, arrang...</td>\n",
       "      <td>her</td>\n",
       "      <td>87</td>\n",
       "      <td>Harriet Winslow Sewall</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>Child</td>\n",
       "      <td>49</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092784</td>\n",
       "      <td>0.154639</td>\n",
       "      <td>[6, 7, 8, 9, 10, 11, 12]</td>\n",
       "      <td>[15]</td>\n",
       "      <td>[22]</td>\n",
       "      <td>[[-0.3488823, -0.16492517, 0.014858797, 0.1704...</td>\n",
       "      <td>[[0.047502562, -0.7228153, 0.430767, 0.2394658...</td>\n",
       "      <td>[[-0.7793019, 0.5476636, 0.0666106, -0.1724950...</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>test-1856</td>\n",
       "      <td>Lucy struggles with being nice and tries to ge...</td>\n",
       "      <td>her</td>\n",
       "      <td>75</td>\n",
       "      <td>Lucy</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Schroeder</td>\n",
       "      <td>48</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102273</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>[12, 13, 14, 15, 16]</td>\n",
       "      <td>[22]</td>\n",
       "      <td>[[-0.10890515, -1.071117, -0.1447686, 0.099091...</td>\n",
       "      <td>[[-0.6169741, 0.13752665, 0.14070973, -0.37765...</td>\n",
       "      <td>[[-0.6348102, 0.29169956, -0.04598154, -0.3830...</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>test-27</td>\n",
       "      <td>Kathleen first appears when Theresa visits her...</td>\n",
       "      <td>her</td>\n",
       "      <td>43</td>\n",
       "      <td>Kathleen</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Theresa</td>\n",
       "      <td>28</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>[7, 8]</td>\n",
       "      <td>[10]</td>\n",
       "      <td>[[-0.17657226, -0.62809116, -0.09252052, 0.296...</td>\n",
       "      <td>[[-0.1495751, -0.03572896, -0.070684716, -0.38...</td>\n",
       "      <td>[[-0.4388417, -0.14650187, 0.09456193, -0.0060...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                               Text Pronoun  \\\n",
       "1995   test-619  Fullerton's book, Jane Austen and Crime (2004)...     her   \n",
       "1996   test-353  At a popular bar frequented by fashion magazin...     she   \n",
       "1997  test-1735  Child's friend, Harriet Winslow Sewall, arrang...     her   \n",
       "1998  test-1856  Lucy struggles with being nice and tries to ge...     her   \n",
       "1999    test-27  Kathleen first appears when Theresa visits her...     her   \n",
       "\n",
       "      Pronoun-offset                       A  A-offset  A-coref            B  \\\n",
       "1995             146               Fullerton         0    False  Jane Austen   \n",
       "1996             108                  Amanda        67    False        Betty   \n",
       "1997              87  Harriet Winslow Sewall        16    False        Child   \n",
       "1998              75                    Lucy         0     True    Schroeder   \n",
       "1999              43                Kathleen         0     True      Theresa   \n",
       "\n",
       "      B-offset  B-coref  ...     B_pos  pron_pos                     A_idx  \\\n",
       "1995        18     True  ...  0.025806  0.174194                    [1, 2]   \n",
       "1996        79     True  ...  0.093333  0.133333                  [15, 16]   \n",
       "1997        49     True  ...  0.092784  0.154639  [6, 7, 8, 9, 10, 11, 12]   \n",
       "1998        48    False  ...  0.102273  0.159091                 [1, 2, 3]   \n",
       "1999        28    False  ...  0.057971  0.086957                 [1, 2, 3]   \n",
       "\n",
       "                     B_idx pron_idx  \\\n",
       "1995         [7, 8, 9, 10]     [34]   \n",
       "1996              [18, 19]     [26]   \n",
       "1997                  [15]     [22]   \n",
       "1998  [12, 13, 14, 15, 16]     [22]   \n",
       "1999                [7, 8]     [10]   \n",
       "\n",
       "                                               A_vector  \\\n",
       "1995  [[0.5251928, -1.0891725, -0.38272008, -0.15374...   \n",
       "1996  [[0.09754669, -0.024278898, -0.15501401, 0.017...   \n",
       "1997  [[-0.3488823, -0.16492517, 0.014858797, 0.1704...   \n",
       "1998  [[-0.10890515, -1.071117, -0.1447686, 0.099091...   \n",
       "1999  [[-0.17657226, -0.62809116, -0.09252052, 0.296...   \n",
       "\n",
       "                                               B_vector  \\\n",
       "1995  [[0.12799127, 0.025941074, -1.1145829, 0.20098...   \n",
       "1996  [[0.00817636, 0.6316583, -0.17376839, 0.220072...   \n",
       "1997  [[0.047502562, -0.7228153, 0.430767, 0.2394658...   \n",
       "1998  [[-0.6169741, 0.13752665, 0.14070973, -0.37765...   \n",
       "1999  [[-0.1495751, -0.03572896, -0.070684716, -0.38...   \n",
       "\n",
       "                                            pron_vector  seq_length  label  \n",
       "1995  [[0.047446743, 0.28261346, -0.27585685, -0.506...          38      1  \n",
       "1996  [[-0.9807809, 0.6089577, 0.37208742, -0.193520...          36      1  \n",
       "1997  [[-0.7793019, 0.5476636, 0.0666106, -0.1724950...          26      1  \n",
       "1998  [[-0.6348102, 0.29169956, -0.04598154, -0.3830...          26      0  \n",
       "1999  [[-0.4388417, -0.14650187, 0.09456193, -0.0060...          19      0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gap_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train[\"seq_length\"] = gap_train.vector.map(lambda x: x.shape[0])\n",
    "gap_valid[\"seq_length\"] = gap_valid.vector.map(lambda x: x.shape[0])\n",
    "gap_test[\"seq_length\"] = gap_test.vector.map(lambda x: x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train = gap_train.sort_values(by=['seq_length'], ascending=False)\n",
    "gap_valid = gap_valid.sort_values(by=['seq_length'], ascending=False)\n",
    "gap_test = gap_test.sort_values(by=['seq_length'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train = gap_train.reset_index(drop = True)\n",
    "gap_valid = gap_valid.reset_index()\n",
    "gap_test = gap_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train[\"label\"] = gap_train.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "#gap_test[\"label\"] = gap_test.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "gap_valid[\"label\"] = gap_valid.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        #print (\"scores\")\n",
    "        #print (scores.shape)\n",
    "        #print (\"scores\")\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "            #print (\"inside\")\n",
    "            #print (mask.size())\n",
    "            #print (\"inside\")\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        #print (x.size())\n",
    "        #print (mask.size())\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "c = copy.deepcopy\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "attn = MultiHeadedAttention(h = 8, d_model = 1024)\n",
    "ff = PositionwiseFeedForward(d_model = 1024, d_ff = 1024, dropout = 0.5)\n",
    "enc = Encoder(EncoderLayer(size = 1024, self_attn = c(attn),feed_forward = c(ff), dropout = 0.5), N = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(df,start,batch_size,dim = 1024,test = False):\n",
    "    num = df.shape[0]\n",
    "    dim = df.vector.iloc[0].shape[1]\n",
    "    #print (dim)\n",
    "    end = min(num,start + batch_size)\n",
    "    batch_data = df.iloc[start:end,:]\n",
    "    max_seq_length = df.loc[start,\"seq_length\"]\n",
    "    if max_seq_length<32:\n",
    "        max_seq_length = 32\n",
    "    padded = np.zeros((end-start,max_seq_length,dim))\n",
    "    sentence_vector = list(batch_data.vector)\n",
    "    for i,v in enumerate(sentence_vector):\n",
    "        padded[i,0:v.shape[0],:] = v \n",
    "    if test is False:\n",
    "        label = batch_data.label\n",
    "    else:\n",
    "        label = 0\n",
    "    A_idx = list(batch_data.A_idx)\n",
    "    B_idx = list (batch_data.B_idx)\n",
    "    pron_idx = list(batch_data.pron_idx)\n",
    "    mask_list  = []\n",
    "    for i,(a,b,p) in enumerate(zip(A_idx,B_idx,pron_idx)):\n",
    "        mask = np.zeros((3,max_seq_length))\n",
    "        mask[0,A_idx[i]] = 1.0\n",
    "        mask[1,B_idx[i]] = 1.0\n",
    "        mask[2,pron_idx[i]] = 1.0\n",
    "        mask_list.append(mask)\n",
    "    return torch.Tensor(padded),torch.LongTensor(np.array(label)),torch.Tensor(np.array(mask_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "class Flatten(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "    def forward(self, x):\n",
    "        N=list(x.size())[0]\n",
    "        x = x.contiguous()\n",
    "        x = x.view(N,-1)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, enc):\n",
    "        super(CNN, self).__init__()\n",
    "        self.enc = enc\n",
    "        self.cnn = nn.Sequential(\n",
    "                 Flatten(),\n",
    "                 nn.Linear(3*1024,3)\n",
    "\n",
    ")\n",
    "    def forward(self, padded, mask_list):\n",
    "        mask = (padded.mean(dim = -1) != 0).unsqueeze(-2)\n",
    "        self.out = enc(padded,mask)\n",
    "        self.out = self.out.unsqueeze(-3)\n",
    "        self.out = padded.unsqueeze(-3)\n",
    "        mask_list = mask_list.unsqueeze(-1)\n",
    "\n",
    "        self.out = (self.out*mask_list).mean(dim = 2)\n",
    "        #print (self.out.size())\n",
    "        self.cnn_out = self.cnn(self.out)\n",
    "        return self.cnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN(enc)\n",
    "# This was important from their code. \n",
    "# Initialize parameters with Glorot / fan_avg.\n",
    "for p in cnn_model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "cnn_model.cuda()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "loss_fn.cuda()\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(1024, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "optimizer = get_std_opt(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at the end of epoch 0\n",
      "1.092016577720642\n",
      "loss at the end of epoch 1\n",
      "1.0461927652359009\n",
      "loss at the end of epoch 2\n",
      "0.9984692335128784\n",
      "loss at the end of epoch 3\n",
      "0.9573780298233032\n",
      "loss at the end of epoch 4\n",
      "0.9219728112220764\n",
      "loss at the end of epoch 5\n",
      "0.8905549049377441\n",
      "loss at the end of epoch 6\n",
      "0.862732470035553\n",
      "loss at the end of epoch 7\n",
      "0.8383117318153381\n",
      "loss at the end of epoch 8\n",
      "0.8170197606086731\n",
      "loss at the end of epoch 9\n",
      "0.7983737587928772\n",
      "loss at the end of epoch 10\n",
      "0.781829833984375\n",
      "loss at the end of epoch 11\n",
      "0.7669552564620972\n",
      "loss at the end of epoch 12\n",
      "0.7535051107406616\n",
      "loss at the end of epoch 13\n",
      "0.7414067387580872\n",
      "loss at the end of epoch 14\n",
      "0.730687141418457\n",
      "loss at the end of epoch 15\n",
      "0.721389651298523\n",
      "loss at the end of epoch 16\n",
      "0.7138261795043945\n",
      "loss at the end of epoch 17\n",
      "0.708006739616394\n",
      "loss at the end of epoch 18\n",
      "0.703471839427948\n",
      "loss at the end of epoch 19\n",
      "0.6999039649963379\n",
      "loss at the end of epoch 20\n",
      "0.6970652937889099\n",
      "loss at the end of epoch 21\n",
      "0.6947788596153259\n",
      "loss at the end of epoch 22\n",
      "0.6929137110710144\n",
      "loss at the end of epoch 23\n",
      "0.6913723945617676\n",
      "loss at the end of epoch 24\n",
      "0.690081775188446\n",
      "loss at the end of epoch 25\n",
      "0.6889867782592773\n",
      "loss at the end of epoch 26\n",
      "0.6880456805229187\n",
      "loss at the end of epoch 27\n",
      "0.6872264742851257\n",
      "loss at the end of epoch 28\n",
      "0.6865044832229614\n",
      "loss at the end of epoch 29\n",
      "0.6858609318733215\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 30\n",
    "BATCH_SIZE = 8\n",
    "start = 0\n",
    "data_num = 2000\n",
    "for e in range(EPOCH):\n",
    "    start = 0\n",
    "    while start <= data_num-1:\n",
    "        padded, label, mask_list = get_batch(gap_train,start,BATCH_SIZE)\n",
    "        padded = padded.cuda()\n",
    "        label = label.cuda()\n",
    "        mask_list = mask_list.cuda()\n",
    "        cnn_model.train()\n",
    "        start = start + BATCH_SIZE\n",
    "        cnn_out = cnn_model(padded,mask_list)\n",
    "        cnn_out = cnn_out.view(-1,3)\n",
    "        #print (label.size())\n",
    "        cnn_out = torch.nn.Softmax(dim = -1)(cnn_out)\n",
    "        loss = loss_fn(cnn_out,label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print (\"loss at the end of epoch \"+str(e))\n",
    "    print (loss.item())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dataset():\n",
    "    start = 0\n",
    "    cnn_out_list = []\n",
    "    while start <= data_num-1:\n",
    "        padded, label, mask_list = get_batch(gap_test,start,BATCH_SIZE,test = True)\n",
    "        padded = padded.cuda()\n",
    "        mask_list = mask_list.cuda()\n",
    "        cnn_model.eval()\n",
    "        start = start + BATCH_SIZE\n",
    "        cnn_out = cnn_model(padded,mask_list)\n",
    "        cnn_out = cnn_out.view(-1,3)\n",
    "        cnn_out = torch.nn.Softmax(dim = 1)(cnn_out)\n",
    "        cnn_out_list.append(cnn_out.cpu().data.numpy())\n",
    "    return np.concatenate(cnn_out_list,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_out_list = eval_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.4601706e-01, 6.3741124e-01, 1.6571701e-02],\n",
       "       [4.4428149e-01, 5.5370706e-01, 2.0114044e-03],\n",
       "       [4.9062228e-01, 4.8169231e-01, 2.7685393e-02],\n",
       "       ...,\n",
       "       [6.4839027e-03, 9.9351609e-01, 2.0165412e-09],\n",
       "       [7.3773915e-01, 2.6226091e-01, 7.4873725e-11],\n",
       "       [2.3239160e-01, 7.6760650e-01, 1.9149309e-06]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_out_list = pd.DataFrame(np.array(cnn_out_list),index = gap_test[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.487733</td>\n",
       "      <td>0.512266</td>\n",
       "      <td>8.340291e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.885138</td>\n",
       "      <td>0.114859</td>\n",
       "      <td>3.337658e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.112824</td>\n",
       "      <td>0.886892</td>\n",
       "      <td>2.839654e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.185579</td>\n",
       "      <td>0.814405</td>\n",
       "      <td>1.509802e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.121560</td>\n",
       "      <td>0.878009</td>\n",
       "      <td>4.306988e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1             2\n",
       "index                                  \n",
       "0      0.487733  0.512266  8.340291e-07\n",
       "1      0.885138  0.114859  3.337658e-06\n",
       "2      0.112824  0.886892  2.839654e-04\n",
       "3      0.185579  0.814405  1.509802e-05\n",
       "4      0.121560  0.878009  4.306988e-04"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_out_list = cnn_out_list.sort_index()\n",
    "cnn_out_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.487733</td>\n",
       "      <td>0.512266</td>\n",
       "      <td>8.340291e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.885138</td>\n",
       "      <td>0.114859</td>\n",
       "      <td>3.337658e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.112824</td>\n",
       "      <td>0.886892</td>\n",
       "      <td>2.839654e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.185579</td>\n",
       "      <td>0.814405</td>\n",
       "      <td>1.509802e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.121560</td>\n",
       "      <td>0.878009</td>\n",
       "      <td>4.306988e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID         A         B       NEITHER\n",
       "0  development-1  0.487733  0.512266  8.340291e-07\n",
       "1  development-2  0.885138  0.114859  3.337658e-06\n",
       "2  development-3  0.112824  0.886892  2.839654e-04\n",
       "3  development-4  0.185579  0.814405  1.509802e-05\n",
       "4  development-5  0.121560  0.878009  4.306988e-04"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.read_csv(\"./test_and_submit/sample_submission_stage_1.csv\")\n",
    "sub_df.loc[:, ['A','B','NEITHER']] = cnn_out_list.values\n",
    "\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "sub_df.to_csv(\"./test_and_submit/submission+model+transformer@\"+str(datetime.datetime.now())+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "sub_df.to_csv(\"./test_and_submit/submission+model+mlp@\"+str(datetime.datetime.now())+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
