{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train= pd.read_pickle('./temp_result/train_kaggle_processed_PPA_PCA_PPA_neither')\n",
    "gap_test= pd.read_pickle('./temp_result/test_kaggle_processed_PPA_PCA_PPA_neither')\n",
    "gap_valid= pd.read_pickle('./temp_result/valid_kaggle_processed_PPA_PCA_PPA_neither')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = gap_train.count().values[0]\n",
    "NUM_TEST = gap_test.count().values[0]\n",
    "NUM_VALID = gap_valid.count().values[0]\n",
    "def label(A,B):\n",
    "    if A is True:\n",
    "        return 0\n",
    "    if B is True:\n",
    "        return 1\n",
    "    return 2\n",
    "def switch_label(l):\n",
    "    if l==2:\n",
    "        return 2\n",
    "    return 1-l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prediction(pred):\n",
    "    s = pred.shape[0]//2\n",
    "    pred0 = pred[0:s,:]\n",
    "    pred1 = pred[s:,:]\n",
    "    pred1 = pred1[:,[1,0,2]]\n",
    "    pred_out = pred0+pred1\n",
    "    return pred_out/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_A_B(df):\n",
    "    columnsTitles = [\"B_dist\",\"A_dist\",\"B_pos\", \"A_pos\",\"pron_pos\", \"B_vector\", \"A_vector\",\"pron_vector\",\"product_vector_B\",\"product_vector_A\",\"label\"]\n",
    "    df2=df.reindex(columns=columnsTitles).copy()\n",
    "    df2.columns = df.columns\n",
    "    df2.label = df2.label.map(switch_label)\n",
    "    return pd.concat([df,df2],axis = 0, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compute_loss(sub_df,test_data):\n",
    "    pred = torch.Tensor(np.log(sub_df.loc[:,['A','B','NEITHER']].values))\n",
    "    label = torch.LongTensor(list(test_data.label))\n",
    "    loss = torch.nn.NLLLoss()\n",
    "    loss_value = loss(pred,label).item()\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_neither(x):\n",
    "    if x.shape[0]!= 0:\n",
    "        return np.mean(x,axis = 0)\n",
    "    else:\n",
    "        return np.zeros(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 256), dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gap_train.neither_vector[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gap_train.drop(columns = ['ID', 'Text', 'Pronoun', 'vector','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map','A_idx', 'B_idx', 'pron_idx',\"neither_idx\",\"name_list\",\"neither_idx_2\",\"name_list_2\",\"neither_vector\"])\n",
    "train_data.A_vector = train_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data.B_vector = train_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data.pron_vector = train_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data.neither_vector_2 = train_data.neither_vector_2.map(lambda x:for_neither(x))\n",
    "train_data[\"product_vector_A\"] = train_data.A_vector*train_data.pron_vector\n",
    "train_data[\"product_vector_B\"] = train_data.B_vector*train_data.pron_vector\n",
    "train_data[\"product_vector_neither\"] = train_data.neither_vector_2*train_data.pron_vector\n",
    "train_data[\"label\"] = train_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "train_data = train_data.drop(columns= [\"A-coref\",\"B-coref\"])\n",
    "#train_data = switch_A_B(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = gap_test.drop(columns = ['ID', 'Text', 'Pronoun', 'vector','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map','A_idx', 'B_idx', 'pron_idx',\"neither_idx\",\"name_list\",\"neither_idx_2\",\"name_list_2\",\"neither_vector\"])\n",
    "test_data.A_vector = test_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data.B_vector = test_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data.pron_vector = test_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data.neither_vector_2 = test_data.neither_vector_2.map(lambda x:for_neither(x))\n",
    "test_data[\"product_vector_A\"] = test_data.A_vector*test_data.pron_vector\n",
    "test_data[\"product_vector_B\"] = test_data.B_vector*test_data.pron_vector\n",
    "test_data[\"product_vector_neither\"] = test_data.neither_vector_2*test_data.pron_vector\n",
    "test_data[\"label\"] = test_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "test_data = test_data.drop(columns= [\"A-coref\",\"B-coref\"])\n",
    "#test_data = switch_A_B(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = gap_valid.drop(columns = ['ID', 'Text', 'Pronoun', 'vector','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map','A_idx', 'B_idx', 'pron_idx',\"neither_idx\",\"name_list\",\"neither_idx_2\",\"name_list_2\",\"neither_vector\"])\n",
    "valid_data.A_vector = valid_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data.B_vector = valid_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data.pron_vector = valid_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data.neither_vector_2 = valid_data.neither_vector_2.map(lambda x:for_neither(x))\n",
    "valid_data[\"product_vector_A\"] = valid_data.A_vector*valid_data.pron_vector\n",
    "valid_data[\"product_vector_B\"] = valid_data.B_vector*valid_data.pron_vector\n",
    "valid_data[\"product_vector_neither\"] = valid_data.neither_vector_2*valid_data.pron_vector\n",
    "valid_data[\"label\"] = valid_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "valid_data = valid_data.drop(columns= [\"A-coref\",\"B-coref\"])\n",
    "#valid_data = switch_A_B(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_dist</th>\n",
       "      <th>B_dist</th>\n",
       "      <th>A_pos</th>\n",
       "      <th>B_pos</th>\n",
       "      <th>pron_pos</th>\n",
       "      <th>A_vector</th>\n",
       "      <th>B_vector</th>\n",
       "      <th>pron_vector</th>\n",
       "      <th>neither_vector_2</th>\n",
       "      <th>product_vector_A</th>\n",
       "      <th>product_vector_B</th>\n",
       "      <th>product_vector_neither</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.693548</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>[0.20037952, 0.2805402, -0.11013528, -0.492183...</td>\n",
       "      <td>[0.6041415, -0.13538514, 0.15207358, -0.076126...</td>\n",
       "      <td>[0.8586822, -1.2192798, 0.09194927, -0.4327072...</td>\n",
       "      <td>[-0.25571027, -0.07773098, -0.48689944, -1.220...</td>\n",
       "      <td>[0.17206234, -0.342057, -0.010126858, 0.212971...</td>\n",
       "      <td>[0.51876557, 0.16507237, 0.013983054, 0.032940...</td>\n",
       "      <td>[-0.21957387, 0.09477582, -0.044770047, 0.5279...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.030</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>[0.067682624, 0.42009318, -0.009991955, -0.659...</td>\n",
       "      <td>[-0.438681, 0.48596978, -0.02240046, 0.0839553...</td>\n",
       "      <td>[-0.67510414, -0.39920956, 0.07794422, 0.43378...</td>\n",
       "      <td>[-0.12306414, -0.019772578, -0.148793, -0.4155...</td>\n",
       "      <td>[-0.04569282, -0.16770521, -0.0007788151, -0.2...</td>\n",
       "      <td>[0.29615536, -0.19400378, -0.0017459863, 0.036...</td>\n",
       "      <td>[0.08308111, 0.007893402, -0.011597554, -0.180...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.405128</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>[-0.40055174, -0.06318157, 0.08668772, -0.0661...</td>\n",
       "      <td>[0.67819536, 0.21544528, -0.39938855, 0.618967...</td>\n",
       "      <td>[0.72358626, -1.0747175, 0.050702423, -0.89089...</td>\n",
       "      <td>[0.15148854, 0.1907654, -0.17670102, -0.126718...</td>\n",
       "      <td>[-0.28983372, 0.06790234, 0.0043952777, 0.0588...</td>\n",
       "      <td>[0.49073285, -0.23154281, -0.020249967, -0.551...</td>\n",
       "      <td>[0.10961503, -0.20501891, -0.00895917, 0.11289...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.201521</td>\n",
       "      <td>0.216730</td>\n",
       "      <td>0.224335</td>\n",
       "      <td>[-0.33478984, -0.63129133, -0.5514479, 0.59391...</td>\n",
       "      <td>[0.50680375, 0.71950006, -0.25772676, 0.375783...</td>\n",
       "      <td>[1.2850071, 0.049930945, -0.26918668, 0.253453...</td>\n",
       "      <td>[0.41241223, 0.8078015, 0.50750864, 0.03485240...</td>\n",
       "      <td>[-0.43020734, -0.031520974, 0.14844243, 0.1505...</td>\n",
       "      <td>[0.6512464, 0.035925318, 0.06937661, 0.0952435...</td>\n",
       "      <td>[0.52995265, 0.04033429, -0.13661456, 0.008833...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.056</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.321839</td>\n",
       "      <td>0.362069</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>[0.25996214, -0.15495634, -0.5617872, -0.04960...</td>\n",
       "      <td>[0.2925861, 0.2669702, 0.03382411, -0.3578499,...</td>\n",
       "      <td>[-0.87992877, -0.07835998, 0.12989467, -0.2304...</td>\n",
       "      <td>[0.28072405, 0.48110938, -0.024320625, -0.4141...</td>\n",
       "      <td>[-0.22874817, 0.012142375, -0.07297316, 0.0114...</td>\n",
       "      <td>[-0.2574549, -0.020919777, 0.0043935715, 0.082...</td>\n",
       "      <td>[-0.24701716, -0.037699718, -0.0031591195, 0.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A_dist  B_dist     A_pos     B_pos  pron_pos  \\\n",
       "0   0.018   0.004  0.693548  0.806452  0.838710   \n",
       "1   0.030   0.012  0.511111  0.711111  0.844444   \n",
       "2   0.022   0.010  0.405128  0.435897  0.461538   \n",
       "3   0.012   0.004  0.201521  0.216730  0.224335   \n",
       "4   0.056   0.042  0.321839  0.362069  0.482759   \n",
       "\n",
       "                                            A_vector  \\\n",
       "0  [0.20037952, 0.2805402, -0.11013528, -0.492183...   \n",
       "1  [0.067682624, 0.42009318, -0.009991955, -0.659...   \n",
       "2  [-0.40055174, -0.06318157, 0.08668772, -0.0661...   \n",
       "3  [-0.33478984, -0.63129133, -0.5514479, 0.59391...   \n",
       "4  [0.25996214, -0.15495634, -0.5617872, -0.04960...   \n",
       "\n",
       "                                            B_vector  \\\n",
       "0  [0.6041415, -0.13538514, 0.15207358, -0.076126...   \n",
       "1  [-0.438681, 0.48596978, -0.02240046, 0.0839553...   \n",
       "2  [0.67819536, 0.21544528, -0.39938855, 0.618967...   \n",
       "3  [0.50680375, 0.71950006, -0.25772676, 0.375783...   \n",
       "4  [0.2925861, 0.2669702, 0.03382411, -0.3578499,...   \n",
       "\n",
       "                                         pron_vector  \\\n",
       "0  [0.8586822, -1.2192798, 0.09194927, -0.4327072...   \n",
       "1  [-0.67510414, -0.39920956, 0.07794422, 0.43378...   \n",
       "2  [0.72358626, -1.0747175, 0.050702423, -0.89089...   \n",
       "3  [1.2850071, 0.049930945, -0.26918668, 0.253453...   \n",
       "4  [-0.87992877, -0.07835998, 0.12989467, -0.2304...   \n",
       "\n",
       "                                    neither_vector_2  \\\n",
       "0  [-0.25571027, -0.07773098, -0.48689944, -1.220...   \n",
       "1  [-0.12306414, -0.019772578, -0.148793, -0.4155...   \n",
       "2  [0.15148854, 0.1907654, -0.17670102, -0.126718...   \n",
       "3  [0.41241223, 0.8078015, 0.50750864, 0.03485240...   \n",
       "4  [0.28072405, 0.48110938, -0.024320625, -0.4141...   \n",
       "\n",
       "                                    product_vector_A  \\\n",
       "0  [0.17206234, -0.342057, -0.010126858, 0.212971...   \n",
       "1  [-0.04569282, -0.16770521, -0.0007788151, -0.2...   \n",
       "2  [-0.28983372, 0.06790234, 0.0043952777, 0.0588...   \n",
       "3  [-0.43020734, -0.031520974, 0.14844243, 0.1505...   \n",
       "4  [-0.22874817, 0.012142375, -0.07297316, 0.0114...   \n",
       "\n",
       "                                    product_vector_B  \\\n",
       "0  [0.51876557, 0.16507237, 0.013983054, 0.032940...   \n",
       "1  [0.29615536, -0.19400378, -0.0017459863, 0.036...   \n",
       "2  [0.49073285, -0.23154281, -0.020249967, -0.551...   \n",
       "3  [0.6512464, 0.035925318, 0.06937661, 0.0952435...   \n",
       "4  [-0.2574549, -0.020919777, 0.0043935715, 0.082...   \n",
       "\n",
       "                              product_vector_neither  label  \n",
       "0  [-0.21957387, 0.09477582, -0.044770047, 0.5279...      2  \n",
       "1  [0.08308111, 0.007893402, -0.011597554, -0.180...      1  \n",
       "2  [0.10961503, -0.20501891, -0.00895917, 0.11289...      1  \n",
       "3  [0.52995265, 0.04033429, -0.13661456, 0.008833...      0  \n",
       "4  [-0.24701716, -0.037699718, -0.0031591195, 0.0...      1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = train_data.columns[:-1]\n",
    "X_train = np.concatenate([np.array(list(train_data[col])).reshape(train_data.shape[0],-1) for col in columns],axis = 1)\n",
    "y_train = list(train_data.label)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = np.concatenate([np.array(list(valid_data[col])).reshape(valid_data.shape[0],-1) for col in columns],axis = 1)\n",
    "y_valid = list(valid_data.label)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.concatenate([np.array(list(test_data[col])).reshape(test_data.shape[0],-1) for col in columns],axis = 1)\n",
    "y_test= list(test_data.label)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open( \"./temp_result/base_model_data\", \"wb\" )\n",
    "pickle.dump(X_train,  f)\n",
    "pickle.dump(y_train,  f)\n",
    "pickle.dump(X_valid,  f)\n",
    "pickle.dump(y_valid,  f)\n",
    "pickle.dump(X_test,  f)\n",
    "pickle.dump(y_test,  f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bao/anaconda3/envs/EPFL/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial',C = 0.02).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.648500</td>\n",
       "      <td>0.243299</td>\n",
       "      <td>0.108201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.792850</td>\n",
       "      <td>0.142018</td>\n",
       "      <td>0.065131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.045926</td>\n",
       "      <td>0.932941</td>\n",
       "      <td>0.021133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.050576</td>\n",
       "      <td>0.759220</td>\n",
       "      <td>0.190203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.017313</td>\n",
       "      <td>0.911611</td>\n",
       "      <td>0.071076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID         A         B   NEITHER\n",
       "0  development-1  0.648500  0.243299  0.108201\n",
       "1  development-2  0.792850  0.142018  0.065131\n",
       "2  development-3  0.045926  0.932941  0.021133\n",
       "3  development-4  0.050576  0.759220  0.190203\n",
       "4  development-5  0.017313  0.911611  0.071076"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pred_lr = process_prediction(pred_lr)\n",
    "sub_df = pd.read_csv(\"./test_and_submit/sample_submission_stage_1.csv\")\n",
    "sub_df.loc[:, ['A','B','NEITHER']] = pred_lr\n",
    "\n",
    "\n",
    "sub_df.to_csv(\"./test_and_submit/submission+model+lr@\"+str(datetime.datetime.now())+\".csv\", index=False)\n",
    "\n",
    "sub_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5355990529060364"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(sub_df,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr_train = lr.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open( \"./temp_result/lr_result\", \"wb\" )\n",
    "pickle.dump(pred_lr_train,  f)\n",
    "pickle.dump(pred_lr,  f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(C = 7.0,verbose=True,probability = True,gamma = \"auto\",class_weight='balanced').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm = svm.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pred_svm = process_prediction(pred_svm)\n",
    "sub_df = pd.read_csv(\"./test_and_submit/sample_submission_stage_1.csv\")\n",
    "sub_df.loc[:, ['A','B','NEITHER']] = pred_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.708325</td>\n",
       "      <td>0.219804</td>\n",
       "      <td>0.071872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.801409</td>\n",
       "      <td>0.130339</td>\n",
       "      <td>0.068252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.028252</td>\n",
       "      <td>0.939505</td>\n",
       "      <td>0.032243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.042207</td>\n",
       "      <td>0.743166</td>\n",
       "      <td>0.214628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.018973</td>\n",
       "      <td>0.872788</td>\n",
       "      <td>0.108239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID         A         B   NEITHER\n",
       "0  development-1  0.708325  0.219804  0.071872\n",
       "1  development-2  0.801409  0.130339  0.068252\n",
       "2  development-3  0.028252  0.939505  0.032243\n",
       "3  development-4  0.042207  0.743166  0.214628\n",
       "4  development-5  0.018973  0.872788  0.108239"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.to_csv(\"./test_and_submit/submission+model+svm@\"+str(datetime.datetime.now())+\".csv\", index=False)\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5287905931472778"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(sub_df,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm_train = svm.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open( \"./temp_result/svm_result\", \"wb\" )\n",
    "pickle.dump(pred_svm_train,  f)\n",
    "pickle.dump(pred_svm,  f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1797, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.95),\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.layers[0].weight)\n",
    "        nn.init.xavier_uniform_(self.layers[-1].weight)\n",
    "    def forward(self, x):\n",
    "        # convert tensor (128, 1, 28, 28) --> (128, 1*28*28)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "batch_size = 25\n",
    "mlp = MLP()\n",
    "mlp.cuda()\n",
    "opt = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()#weight = torch.Tensor([1.0,1.0,5.0]))\n",
    "loss_fn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/500], loss:2.2593\n",
      "epoch [2/500], loss:1.3828\n",
      "epoch [3/500], loss:1.3110\n",
      "epoch [4/500], loss:1.2690\n",
      "epoch [5/500], loss:1.2776\n",
      "epoch [6/500], loss:1.2781\n",
      "epoch [7/500], loss:1.3250\n",
      "epoch [8/500], loss:1.1108\n",
      "epoch [9/500], loss:1.0807\n",
      "epoch [10/500], loss:1.0815\n",
      "epoch [11/500], loss:1.0329\n",
      "epoch [12/500], loss:1.0915\n",
      "epoch [13/500], loss:0.9155\n",
      "epoch [14/500], loss:1.1203\n",
      "epoch [15/500], loss:0.9897\n",
      "epoch [16/500], loss:0.9564\n",
      "epoch [17/500], loss:0.9643\n",
      "epoch [18/500], loss:0.9062\n",
      "epoch [19/500], loss:0.9481\n",
      "epoch [20/500], loss:1.0303\n",
      "epoch [21/500], loss:1.0023\n",
      "epoch [22/500], loss:0.9352\n",
      "epoch [23/500], loss:0.9819\n",
      "epoch [24/500], loss:1.0583\n",
      "epoch [25/500], loss:0.8532\n",
      "epoch [26/500], loss:0.9305\n",
      "epoch [27/500], loss:0.8078\n",
      "epoch [28/500], loss:0.8622\n",
      "epoch [29/500], loss:0.8998\n",
      "epoch [30/500], loss:0.8562\n",
      "epoch [31/500], loss:0.7669\n",
      "epoch [32/500], loss:0.8521\n",
      "epoch [33/500], loss:0.9868\n",
      "epoch [34/500], loss:0.8308\n",
      "epoch [35/500], loss:0.7436\n",
      "epoch [36/500], loss:0.9850\n",
      "epoch [37/500], loss:0.8627\n",
      "epoch [38/500], loss:0.9659\n",
      "epoch [39/500], loss:0.9535\n",
      "epoch [40/500], loss:0.8270\n",
      "epoch [41/500], loss:0.9305\n",
      "epoch [42/500], loss:0.8158\n",
      "epoch [43/500], loss:0.7994\n",
      "epoch [44/500], loss:0.9028\n",
      "epoch [45/500], loss:0.8897\n",
      "epoch [46/500], loss:0.8723\n",
      "epoch [47/500], loss:0.9187\n",
      "epoch [48/500], loss:0.7987\n",
      "epoch [49/500], loss:0.8843\n",
      "epoch [50/500], loss:0.8455\n",
      "epoch [51/500], loss:1.0116\n",
      "epoch [52/500], loss:0.8322\n",
      "epoch [53/500], loss:0.8786\n",
      "epoch [54/500], loss:0.8479\n",
      "epoch [55/500], loss:0.9905\n",
      "epoch [56/500], loss:0.8708\n",
      "epoch [57/500], loss:0.8504\n",
      "epoch [58/500], loss:0.8406\n",
      "epoch [59/500], loss:0.8810\n",
      "epoch [60/500], loss:0.9105\n",
      "epoch [61/500], loss:0.9110\n",
      "epoch [62/500], loss:0.8771\n",
      "epoch [63/500], loss:0.8056\n",
      "epoch [64/500], loss:1.0544\n",
      "epoch [65/500], loss:0.7851\n",
      "epoch [66/500], loss:0.7340\n",
      "epoch [67/500], loss:0.9929\n",
      "epoch [68/500], loss:0.7993\n",
      "epoch [69/500], loss:0.8502\n",
      "epoch [70/500], loss:0.8220\n",
      "epoch [71/500], loss:0.8577\n",
      "epoch [72/500], loss:0.8188\n",
      "epoch [73/500], loss:0.9486\n",
      "epoch [74/500], loss:0.8574\n",
      "epoch [75/500], loss:0.9663\n",
      "epoch [76/500], loss:0.9142\n",
      "epoch [77/500], loss:0.7769\n",
      "epoch [78/500], loss:0.7642\n",
      "epoch [79/500], loss:0.8060\n",
      "epoch [80/500], loss:0.9229\n",
      "epoch [81/500], loss:0.9064\n",
      "epoch [82/500], loss:0.9340\n",
      "epoch [83/500], loss:0.8406\n",
      "epoch [84/500], loss:0.7646\n",
      "epoch [85/500], loss:0.8815\n",
      "epoch [86/500], loss:1.0064\n",
      "epoch [87/500], loss:0.7789\n",
      "epoch [88/500], loss:0.9054\n",
      "epoch [89/500], loss:0.8378\n",
      "epoch [90/500], loss:0.8880\n",
      "epoch [91/500], loss:0.8210\n",
      "epoch [92/500], loss:0.7989\n",
      "epoch [93/500], loss:1.0052\n",
      "epoch [94/500], loss:0.9341\n",
      "epoch [95/500], loss:0.9170\n",
      "epoch [96/500], loss:0.7595\n",
      "epoch [97/500], loss:0.8717\n",
      "epoch [98/500], loss:1.1015\n",
      "epoch [99/500], loss:1.0129\n",
      "epoch [100/500], loss:0.8295\n",
      "epoch [101/500], loss:0.9875\n",
      "epoch [102/500], loss:0.8708\n",
      "epoch [103/500], loss:0.8333\n",
      "epoch [104/500], loss:0.8986\n",
      "epoch [105/500], loss:0.7851\n",
      "epoch [106/500], loss:0.7868\n",
      "epoch [107/500], loss:0.8348\n",
      "epoch [108/500], loss:0.8886\n",
      "epoch [109/500], loss:0.9146\n",
      "epoch [110/500], loss:0.8615\n",
      "epoch [111/500], loss:0.8787\n",
      "epoch [112/500], loss:0.9122\n",
      "epoch [113/500], loss:0.7241\n",
      "epoch [114/500], loss:1.0002\n",
      "epoch [115/500], loss:0.8609\n",
      "epoch [116/500], loss:0.8070\n",
      "epoch [117/500], loss:1.0276\n",
      "epoch [118/500], loss:0.9089\n",
      "epoch [119/500], loss:0.8947\n",
      "epoch [120/500], loss:0.8212\n",
      "epoch [121/500], loss:0.8432\n",
      "epoch [122/500], loss:0.8967\n",
      "epoch [123/500], loss:0.8334\n",
      "epoch [124/500], loss:0.9225\n",
      "epoch [125/500], loss:0.8172\n",
      "epoch [126/500], loss:0.8258\n",
      "epoch [127/500], loss:0.8970\n",
      "epoch [128/500], loss:0.8372\n",
      "epoch [129/500], loss:1.0551\n",
      "epoch [130/500], loss:0.8349\n",
      "epoch [131/500], loss:0.9737\n",
      "epoch [132/500], loss:0.8783\n",
      "epoch [133/500], loss:1.0043\n",
      "epoch [134/500], loss:0.9438\n",
      "epoch [135/500], loss:0.9096\n",
      "epoch [136/500], loss:0.8993\n",
      "epoch [137/500], loss:0.8596\n",
      "epoch [138/500], loss:0.8669\n",
      "epoch [139/500], loss:0.9328\n",
      "epoch [140/500], loss:0.8123\n",
      "epoch [141/500], loss:0.8883\n",
      "epoch [142/500], loss:0.9451\n",
      "epoch [143/500], loss:1.0689\n",
      "epoch [144/500], loss:0.9700\n",
      "epoch [145/500], loss:0.9699\n",
      "epoch [146/500], loss:0.9070\n",
      "epoch [147/500], loss:0.9023\n",
      "epoch [148/500], loss:0.8420\n",
      "epoch [149/500], loss:0.7967\n",
      "epoch [150/500], loss:0.8091\n",
      "epoch [151/500], loss:0.7724\n",
      "epoch [152/500], loss:0.7150\n",
      "epoch [153/500], loss:0.8653\n",
      "epoch [154/500], loss:0.8494\n",
      "epoch [155/500], loss:0.9726\n",
      "epoch [156/500], loss:1.0403\n",
      "epoch [157/500], loss:0.9688\n",
      "epoch [158/500], loss:0.8759\n",
      "epoch [159/500], loss:0.8101\n",
      "epoch [160/500], loss:0.7196\n",
      "epoch [161/500], loss:0.7321\n",
      "epoch [162/500], loss:0.8346\n",
      "epoch [163/500], loss:0.9185\n",
      "epoch [164/500], loss:0.9528\n",
      "epoch [165/500], loss:0.8516\n",
      "epoch [166/500], loss:0.8846\n",
      "epoch [167/500], loss:0.9025\n",
      "epoch [168/500], loss:0.8343\n",
      "epoch [169/500], loss:0.9224\n",
      "epoch [170/500], loss:1.0011\n",
      "epoch [171/500], loss:0.8758\n",
      "epoch [172/500], loss:0.9598\n",
      "epoch [173/500], loss:0.7810\n",
      "epoch [174/500], loss:0.8456\n",
      "epoch [175/500], loss:0.8641\n",
      "epoch [176/500], loss:0.8632\n",
      "epoch [177/500], loss:0.8535\n",
      "epoch [178/500], loss:0.8770\n",
      "epoch [179/500], loss:0.7937\n",
      "epoch [180/500], loss:0.8471\n",
      "epoch [181/500], loss:0.7300\n",
      "epoch [182/500], loss:0.8558\n",
      "epoch [183/500], loss:0.9683\n",
      "epoch [184/500], loss:0.8186\n",
      "epoch [185/500], loss:0.7533\n",
      "epoch [186/500], loss:0.7820\n",
      "epoch [187/500], loss:0.8910\n",
      "epoch [188/500], loss:0.8030\n",
      "epoch [189/500], loss:1.0052\n",
      "epoch [190/500], loss:0.8490\n",
      "epoch [191/500], loss:0.8170\n",
      "epoch [192/500], loss:0.9311\n",
      "epoch [193/500], loss:0.9096\n",
      "epoch [194/500], loss:0.8085\n",
      "epoch [195/500], loss:0.8968\n",
      "epoch [196/500], loss:1.0823\n",
      "epoch [197/500], loss:0.8853\n",
      "epoch [198/500], loss:1.0613\n",
      "epoch [199/500], loss:0.8404\n",
      "epoch [200/500], loss:0.9004\n",
      "epoch [201/500], loss:0.9103\n",
      "epoch [202/500], loss:0.9886\n",
      "epoch [203/500], loss:0.9885\n",
      "epoch [204/500], loss:0.9211\n",
      "epoch [205/500], loss:0.9570\n",
      "epoch [206/500], loss:0.9542\n",
      "epoch [207/500], loss:0.8826\n",
      "epoch [208/500], loss:0.9978\n",
      "epoch [209/500], loss:0.9016\n",
      "epoch [210/500], loss:0.7953\n",
      "epoch [211/500], loss:0.8804\n",
      "epoch [212/500], loss:0.8710\n",
      "epoch [213/500], loss:0.8559\n",
      "epoch [214/500], loss:0.9188\n",
      "epoch [215/500], loss:0.9117\n",
      "epoch [216/500], loss:0.9305\n",
      "epoch [217/500], loss:0.8087\n",
      "epoch [218/500], loss:0.8114\n",
      "epoch [219/500], loss:0.9240\n",
      "epoch [220/500], loss:0.8687\n",
      "epoch [221/500], loss:0.9290\n",
      "epoch [222/500], loss:0.9262\n",
      "epoch [223/500], loss:0.7495\n",
      "epoch [224/500], loss:0.9896\n",
      "epoch [225/500], loss:0.7989\n",
      "epoch [226/500], loss:0.8130\n",
      "epoch [227/500], loss:1.0422\n",
      "epoch [228/500], loss:0.8702\n",
      "epoch [229/500], loss:0.8911\n",
      "epoch [230/500], loss:0.8928\n",
      "epoch [231/500], loss:0.8377\n",
      "epoch [232/500], loss:0.7846\n",
      "epoch [233/500], loss:0.8167\n",
      "epoch [234/500], loss:0.8660\n",
      "epoch [235/500], loss:0.7934\n",
      "epoch [236/500], loss:0.8940\n",
      "epoch [237/500], loss:0.7856\n",
      "epoch [238/500], loss:0.8135\n",
      "epoch [239/500], loss:0.7705\n",
      "epoch [240/500], loss:0.8941\n",
      "epoch [241/500], loss:0.9141\n",
      "epoch [242/500], loss:0.8963\n",
      "epoch [243/500], loss:0.8573\n",
      "epoch [244/500], loss:0.8798\n",
      "epoch [245/500], loss:0.8476\n",
      "epoch [246/500], loss:0.9129\n",
      "epoch [247/500], loss:0.9989\n",
      "epoch [248/500], loss:1.0452\n",
      "epoch [249/500], loss:0.9232\n",
      "epoch [250/500], loss:0.9880\n",
      "epoch [251/500], loss:0.9651\n",
      "epoch [252/500], loss:0.8292\n",
      "epoch [253/500], loss:0.8159\n",
      "epoch [254/500], loss:0.8921\n",
      "epoch [255/500], loss:0.9509\n",
      "epoch [256/500], loss:0.7889\n",
      "epoch [257/500], loss:0.8381\n",
      "epoch [258/500], loss:0.9573\n",
      "epoch [259/500], loss:0.9979\n",
      "epoch [260/500], loss:0.9554\n",
      "epoch [261/500], loss:1.0266\n",
      "epoch [262/500], loss:0.9871\n",
      "epoch [263/500], loss:0.9065\n",
      "epoch [264/500], loss:0.8808\n",
      "epoch [265/500], loss:0.9196\n",
      "epoch [266/500], loss:0.9287\n",
      "epoch [267/500], loss:0.9779\n",
      "epoch [268/500], loss:1.1363\n",
      "epoch [269/500], loss:0.8862\n",
      "epoch [270/500], loss:0.8595\n",
      "epoch [271/500], loss:0.8488\n",
      "epoch [272/500], loss:0.7896\n",
      "epoch [273/500], loss:0.8529\n",
      "epoch [274/500], loss:0.9252\n",
      "epoch [275/500], loss:0.9291\n",
      "epoch [276/500], loss:0.9175\n",
      "epoch [277/500], loss:0.9201\n",
      "epoch [278/500], loss:0.8166\n",
      "epoch [279/500], loss:0.8994\n",
      "epoch [280/500], loss:0.9190\n",
      "epoch [281/500], loss:1.0212\n",
      "epoch [282/500], loss:0.9570\n",
      "epoch [283/500], loss:0.7881\n",
      "epoch [284/500], loss:0.8063\n",
      "epoch [285/500], loss:0.7091\n",
      "epoch [286/500], loss:0.8729\n",
      "epoch [287/500], loss:1.0706\n",
      "epoch [288/500], loss:0.8980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [289/500], loss:0.9418\n",
      "epoch [290/500], loss:0.9568\n",
      "epoch [291/500], loss:0.8981\n",
      "epoch [292/500], loss:0.8141\n",
      "epoch [293/500], loss:0.9330\n",
      "epoch [294/500], loss:0.7256\n",
      "epoch [295/500], loss:0.8271\n",
      "epoch [296/500], loss:0.8947\n",
      "epoch [297/500], loss:0.9807\n",
      "epoch [298/500], loss:0.9346\n",
      "epoch [299/500], loss:0.9394\n",
      "epoch [300/500], loss:0.9471\n",
      "epoch [301/500], loss:0.8795\n",
      "epoch [302/500], loss:0.9478\n",
      "epoch [303/500], loss:0.9960\n",
      "epoch [304/500], loss:0.8462\n",
      "epoch [305/500], loss:0.8689\n",
      "epoch [306/500], loss:1.1023\n",
      "epoch [307/500], loss:0.9656\n",
      "epoch [308/500], loss:1.0978\n",
      "epoch [309/500], loss:0.9976\n",
      "epoch [310/500], loss:0.8809\n",
      "epoch [311/500], loss:0.8406\n",
      "epoch [312/500], loss:0.8487\n",
      "epoch [313/500], loss:0.8777\n",
      "epoch [314/500], loss:0.8479\n",
      "epoch [315/500], loss:0.9001\n",
      "epoch [316/500], loss:0.8964\n",
      "epoch [317/500], loss:0.8930\n",
      "epoch [318/500], loss:0.9417\n",
      "epoch [319/500], loss:0.8804\n",
      "epoch [320/500], loss:0.8252\n",
      "epoch [321/500], loss:0.9746\n",
      "epoch [322/500], loss:0.9319\n",
      "epoch [323/500], loss:0.9658\n",
      "epoch [324/500], loss:0.9529\n",
      "epoch [325/500], loss:0.9135\n",
      "epoch [326/500], loss:0.8978\n",
      "epoch [327/500], loss:0.8709\n",
      "epoch [328/500], loss:0.8692\n",
      "epoch [329/500], loss:0.9418\n",
      "epoch [330/500], loss:0.8709\n",
      "epoch [331/500], loss:0.8604\n",
      "epoch [332/500], loss:0.8645\n",
      "epoch [333/500], loss:0.9567\n",
      "epoch [334/500], loss:0.9246\n",
      "epoch [335/500], loss:0.9191\n",
      "epoch [336/500], loss:0.8992\n",
      "epoch [337/500], loss:0.8371\n",
      "epoch [338/500], loss:0.8508\n",
      "epoch [339/500], loss:0.8624\n",
      "epoch [340/500], loss:0.9464\n",
      "epoch [341/500], loss:0.9273\n",
      "epoch [342/500], loss:1.0892\n",
      "epoch [343/500], loss:0.9921\n",
      "epoch [344/500], loss:0.9538\n",
      "epoch [345/500], loss:0.8966\n",
      "epoch [346/500], loss:0.8697\n",
      "epoch [347/500], loss:0.9832\n",
      "epoch [348/500], loss:0.8376\n",
      "epoch [349/500], loss:0.9395\n",
      "epoch [350/500], loss:1.1337\n",
      "epoch [351/500], loss:0.7921\n",
      "epoch [352/500], loss:0.8813\n",
      "epoch [353/500], loss:0.9557\n",
      "epoch [354/500], loss:0.7913\n",
      "epoch [355/500], loss:1.0953\n",
      "epoch [356/500], loss:0.8529\n",
      "epoch [357/500], loss:0.7966\n",
      "epoch [358/500], loss:0.9513\n",
      "epoch [359/500], loss:0.8882\n",
      "epoch [360/500], loss:0.8851\n",
      "epoch [361/500], loss:0.7162\n",
      "epoch [362/500], loss:0.8598\n",
      "epoch [363/500], loss:0.8945\n",
      "epoch [364/500], loss:0.9423\n",
      "epoch [365/500], loss:0.8224\n",
      "epoch [366/500], loss:0.9409\n",
      "epoch [367/500], loss:0.9706\n",
      "epoch [368/500], loss:0.8069\n",
      "epoch [369/500], loss:0.8859\n",
      "epoch [370/500], loss:0.7980\n",
      "epoch [371/500], loss:0.7528\n",
      "epoch [372/500], loss:0.9812\n",
      "epoch [373/500], loss:0.8728\n",
      "epoch [374/500], loss:0.8443\n",
      "epoch [375/500], loss:0.8475\n",
      "epoch [376/500], loss:0.8460\n",
      "epoch [377/500], loss:0.9538\n",
      "epoch [378/500], loss:1.0166\n",
      "epoch [379/500], loss:1.0234\n",
      "epoch [380/500], loss:0.7937\n",
      "epoch [381/500], loss:0.7860\n",
      "epoch [382/500], loss:0.8764\n",
      "epoch [383/500], loss:0.9517\n",
      "epoch [384/500], loss:0.8777\n",
      "epoch [385/500], loss:0.8301\n",
      "epoch [386/500], loss:0.8458\n",
      "epoch [387/500], loss:0.9225\n",
      "epoch [388/500], loss:0.9320\n",
      "epoch [389/500], loss:0.8558\n",
      "epoch [390/500], loss:1.1505\n",
      "epoch [391/500], loss:0.8437\n",
      "epoch [392/500], loss:0.9838\n",
      "epoch [393/500], loss:0.8652\n",
      "epoch [394/500], loss:0.8497\n",
      "epoch [395/500], loss:0.9639\n",
      "epoch [396/500], loss:0.8671\n",
      "epoch [397/500], loss:0.9770\n",
      "epoch [398/500], loss:0.9503\n",
      "epoch [399/500], loss:0.9148\n",
      "epoch [400/500], loss:1.0595\n",
      "epoch [401/500], loss:0.8687\n",
      "epoch [402/500], loss:0.9168\n",
      "epoch [403/500], loss:0.9105\n",
      "epoch [404/500], loss:1.0437\n",
      "epoch [405/500], loss:0.8484\n",
      "epoch [406/500], loss:0.9472\n",
      "epoch [407/500], loss:0.7784\n",
      "epoch [408/500], loss:0.8646\n",
      "epoch [409/500], loss:0.9602\n",
      "epoch [410/500], loss:0.8472\n",
      "epoch [411/500], loss:1.1476\n",
      "epoch [412/500], loss:0.8188\n",
      "epoch [413/500], loss:1.0166\n",
      "epoch [414/500], loss:0.9435\n",
      "epoch [415/500], loss:0.8161\n",
      "epoch [416/500], loss:1.1114\n",
      "epoch [417/500], loss:0.8693\n",
      "epoch [418/500], loss:0.8855\n",
      "epoch [419/500], loss:0.9202\n",
      "epoch [420/500], loss:0.8638\n",
      "epoch [421/500], loss:0.9927\n",
      "epoch [422/500], loss:0.8809\n",
      "epoch [423/500], loss:0.9473\n",
      "epoch [424/500], loss:0.8556\n",
      "epoch [425/500], loss:0.8746\n",
      "epoch [426/500], loss:0.9213\n",
      "epoch [427/500], loss:0.9455\n",
      "epoch [428/500], loss:0.8935\n",
      "epoch [429/500], loss:0.9351\n",
      "epoch [430/500], loss:1.0316\n",
      "epoch [431/500], loss:1.0050\n",
      "epoch [432/500], loss:0.7700\n",
      "epoch [433/500], loss:0.9538\n",
      "epoch [434/500], loss:0.9573\n",
      "epoch [435/500], loss:0.9641\n",
      "epoch [436/500], loss:1.0026\n",
      "epoch [437/500], loss:0.9706\n",
      "epoch [438/500], loss:0.8875\n",
      "epoch [439/500], loss:0.8575\n",
      "epoch [440/500], loss:0.7965\n",
      "epoch [441/500], loss:0.8691\n",
      "epoch [442/500], loss:0.8612\n",
      "epoch [443/500], loss:0.9816\n",
      "epoch [444/500], loss:0.8439\n",
      "epoch [445/500], loss:0.9294\n",
      "epoch [446/500], loss:0.8283\n",
      "epoch [447/500], loss:0.8539\n",
      "epoch [448/500], loss:0.8664\n",
      "epoch [449/500], loss:0.9309\n",
      "epoch [450/500], loss:0.9203\n",
      "epoch [451/500], loss:0.8370\n",
      "epoch [452/500], loss:0.7489\n",
      "epoch [453/500], loss:0.9705\n",
      "epoch [454/500], loss:0.7912\n",
      "epoch [455/500], loss:0.9540\n",
      "epoch [456/500], loss:0.9676\n",
      "epoch [457/500], loss:0.9018\n",
      "epoch [458/500], loss:1.0040\n",
      "epoch [459/500], loss:0.9183\n",
      "epoch [460/500], loss:0.8252\n",
      "epoch [461/500], loss:0.8460\n",
      "epoch [462/500], loss:0.8497\n",
      "epoch [463/500], loss:0.9247\n",
      "epoch [464/500], loss:0.9818\n",
      "epoch [465/500], loss:0.8371\n",
      "epoch [466/500], loss:1.0283\n",
      "epoch [467/500], loss:0.8784\n",
      "epoch [468/500], loss:0.8758\n",
      "epoch [469/500], loss:0.9129\n",
      "epoch [470/500], loss:0.9541\n",
      "epoch [471/500], loss:0.9411\n",
      "epoch [472/500], loss:0.9756\n",
      "epoch [473/500], loss:0.9391\n",
      "epoch [474/500], loss:0.8003\n",
      "epoch [475/500], loss:0.8732\n",
      "epoch [476/500], loss:0.8493\n",
      "epoch [477/500], loss:1.0069\n",
      "epoch [478/500], loss:1.1581\n",
      "epoch [479/500], loss:0.9260\n",
      "epoch [480/500], loss:0.9631\n",
      "epoch [481/500], loss:0.9782\n",
      "epoch [482/500], loss:0.9236\n",
      "epoch [483/500], loss:0.8779\n",
      "epoch [484/500], loss:0.9929\n",
      "epoch [485/500], loss:0.7751\n",
      "epoch [486/500], loss:0.9863\n",
      "epoch [487/500], loss:0.8483\n",
      "epoch [488/500], loss:0.8252\n",
      "epoch [489/500], loss:0.9002\n",
      "epoch [490/500], loss:0.9433\n",
      "epoch [491/500], loss:0.9466\n",
      "epoch [492/500], loss:0.9121\n",
      "epoch [493/500], loss:1.0550\n",
      "epoch [494/500], loss:1.0253\n",
      "epoch [495/500], loss:0.9117\n",
      "epoch [496/500], loss:0.8369\n",
      "epoch [497/500], loss:0.9646\n",
      "epoch [498/500], loss:0.9530\n",
      "epoch [499/500], loss:1.1084\n",
      "epoch [500/500], loss:0.8745\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPOCHS):\n",
    "    for b in range(0,X_train.shape[0],batch_size):\n",
    "        mlp.train()\n",
    "        batch_data = X_train[b:b+batch_size,:]\n",
    "        batch_label = y_train[b:b+batch_size]\n",
    "        output = mlp(torch.Tensor(batch_data).cuda())\n",
    "        batch_label = torch.LongTensor(batch_label).cuda()\n",
    "        loss = loss_fn(output,batch_label)\n",
    "        l2_norm = torch.norm(mlp.layers[-1].weight, p=2)\n",
    "        loss += l2_norm*0.09\n",
    "        l2_norm = torch.norm(mlp.layers[0].weight, p=2)\n",
    "        loss += l2_norm*0.03\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(e + 1, EPOCHS, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.eval()\n",
    "pred_mlp = torch.nn.Softmax(dim = 1)(mlp(torch.Tensor(X_test[:,:]).cuda())).cpu().data.numpy()\n",
    "pred_mlp_train = torch.nn.Softmax(dim = 1)(mlp(torch.Tensor(X_train[:,:]).cuda())).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_mlp = process_prediction(pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.496124</td>\n",
       "      <td>0.332154</td>\n",
       "      <td>0.171722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.896544</td>\n",
       "      <td>0.063167</td>\n",
       "      <td>0.040289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.069529</td>\n",
       "      <td>0.900002</td>\n",
       "      <td>0.030469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.106871</td>\n",
       "      <td>0.713545</td>\n",
       "      <td>0.179584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>0.976605</td>\n",
       "      <td>0.014022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>development-6</td>\n",
       "      <td>0.996813</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.001635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>development-7</td>\n",
       "      <td>0.742837</td>\n",
       "      <td>0.118898</td>\n",
       "      <td>0.138265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID         A         B   NEITHER\n",
       "0  development-1  0.496124  0.332154  0.171722\n",
       "1  development-2  0.896544  0.063167  0.040289\n",
       "2  development-3  0.069529  0.900002  0.030469\n",
       "3  development-4  0.106871  0.713545  0.179584\n",
       "4  development-5  0.009373  0.976605  0.014022\n",
       "5  development-6  0.996813  0.001552  0.001635\n",
       "6  development-7  0.742837  0.118898  0.138265"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.read_csv(\"./test_and_submit/sample_submission_stage_1.csv\")\n",
    "sub_df.loc[:, ['A','B','NEITHER']] = pred_mlp\n",
    "sub_df.to_csv(\"./test_and_submit/submission+model+mlp@\"+str(datetime.datetime.now())+\".csv\", index=False)\n",
    "sub_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5617367625236511"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(sub_df,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open( \"./temp_result/mlp_result\", \"wb\" )\n",
    "pickle.dump(pred_mlp_train,  f)\n",
    "pickle.dump(pred_mlp,  f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open( \"./temp_result/label\", \"wb\" )\n",
    "pickle.dump(y_train,  f)\n",
    "pickle.dump(y_test,  f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
