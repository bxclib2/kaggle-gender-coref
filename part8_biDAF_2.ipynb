{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train= pd.read_pickle('./temp_result/train_kaggle_processed_PPA_PCA_PPA_neither')\n",
    "gap_test= pd.read_pickle('./temp_result/test_kaggle_processed_PPA_PCA_PPA_neither')\n",
    "gap_valid= pd.read_pickle('./temp_result/valid_kaggle_processed_PPA_PCA_PPA_neither')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = gap_train.count().values[0]\n",
    "NUM_TEST = gap_test.count().values[0]\n",
    "NUM_VALID = gap_valid.count().values[0]\n",
    "def label(A,B):\n",
    "    if A is True:\n",
    "        return 0\n",
    "    if B is True:\n",
    "        return 1\n",
    "    return 2\n",
    "def switch_label(l):\n",
    "    if l==2:\n",
    "        return 2\n",
    "    return 1-l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prediction(pred):\n",
    "    s = pred.shape[0]//2\n",
    "    pred0 = pred[0:s,:]\n",
    "    pred1 = pred[s:,:]\n",
    "    pred1 = pred1[:,[1,0,2]]\n",
    "    pred_out = pred0+pred1\n",
    "    return pred_out/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_A_B(df):\n",
    "    columnsTitles = [\"B_dist\",\"A_dist\",\"B_pos\", \"A_pos\",\"pron_pos\", \"B_vector\", \"A_vector\",\"pron_vector\",\"product_vector_B\",\"product_vector_A\",\"label\"]\n",
    "    df2=df.reindex(columns=columnsTitles).copy()\n",
    "    df2.columns = df.columns\n",
    "    df2.label = df2.label.map(switch_label)\n",
    "    return pd.concat([df,df2],axis = 0, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compute_loss(sub_df,test_data):\n",
    "    pred = torch.Tensor(np.log(sub_df.loc[:,['A','B','NEITHER']].values))\n",
    "    label = torch.LongTensor(list(test_data.label))\n",
    "    loss = torch.nn.NLLLoss()\n",
    "    loss_value = loss(pred,label).item()\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gap_train.drop(columns = ['ID', 'Text', 'Pronoun','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map', 'pron_idx',\"name_list\"])\n",
    "train_data.A_vector = train_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data.B_vector = train_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data.pron_vector = train_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data[\"product_vector_A\"] = train_data.A_vector*train_data.pron_vector\n",
    "train_data[\"product_vector_B\"] = train_data.B_vector*train_data.pron_vector\n",
    "train_data[\"label\"] = train_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "train_data = train_data.drop(columns= [\"A-coref\",\"B-coref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = gap_test.drop(columns = ['ID', 'Text', 'Pronoun','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map', 'pron_idx',\"name_list\"])\n",
    "test_data.A_vector = test_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data.B_vector = test_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data.pron_vector = test_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data[\"product_vector_A\"] = test_data.A_vector*test_data.pron_vector\n",
    "test_data[\"product_vector_B\"] = test_data.B_vector*test_data.pron_vector\n",
    "test_data[\"label\"] = test_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "test_data = test_data.drop(columns= [\"A-coref\",\"B-coref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = gap_valid.drop(columns = ['ID', 'Text', 'Pronoun','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map', 'pron_idx',\"name_list\"])\n",
    "valid_data.A_vector = valid_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data.B_vector = valid_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data.pron_vector = valid_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data[\"product_vector_A\"] = valid_data.A_vector*valid_data.pron_vector\n",
    "valid_data[\"product_vector_B\"] = valid_data.B_vector*valid_data.pron_vector\n",
    "valid_data[\"label\"] = valid_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "valid_data = valid_data.drop(columns= [\"A-coref\",\"B-coref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def masked_softmax(logits, mask, dim=-1, log_softmax=False):\n",
    "    \"\"\"Take the softmax of `logits` over given dimension, and set\n",
    "    entries to 0 wherever `mask` is 0.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Inputs to the softmax function.\n",
    "        mask (torch.Tensor): Same shape as `logits`, with 0 indicating\n",
    "            positions that should be assigned 0 probability in the output.\n",
    "        dim (int): Dimension over which to take softmax.\n",
    "        log_softmax (bool): Take log-softmax rather than regular softmax.\n",
    "            E.g., some PyTorch functions such as `F.nll_loss` expect log-softmax.\n",
    "\n",
    "    Returns:\n",
    "        probs (torch.Tensor): Result of taking masked softmax over the logits.\n",
    "    \"\"\"\n",
    "    mask = mask.type(torch.float32)\n",
    "    masked_logits = mask * logits + (1 - mask) * -1e30\n",
    "    softmax_fn = F.log_softmax if log_softmax else F.softmax\n",
    "    probs = softmax_fn(masked_logits, dim)\n",
    "\n",
    "    return probs\n",
    "class BiDAFAttention(nn.Module):\n",
    "    \"\"\"Bidirectional attention originally used by BiDAF.\n",
    "\n",
    "    Bidirectional attention computes attention in two directions:\n",
    "    The context attends to the query and the query attends to the context.\n",
    "    The output of this layer is the concatenation of [context, c2q_attention,\n",
    "    context * c2q_attention, context * q2c_attention]. This concatenation allows\n",
    "    the attention vector at each timestep, along with the embeddings from\n",
    "    previous layers, to flow through the attention layer to the modeling layer.\n",
    "    The output has shape (batch_size, context_len, 8 * hidden_size).\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): Size of hidden activations.\n",
    "        drop_prob (float): Probability of zero-ing out activations.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, drop_prob=0.70):\n",
    "        super(BiDAFAttention, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.c_weight = nn.Parameter(torch.zeros(hidden_size, 1))\n",
    "        self.q_weight = nn.Parameter(torch.zeros(hidden_size, 1))\n",
    "        self.cq_weight = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "        for weight in (self.c_weight, self.q_weight, self.cq_weight):\n",
    "            nn.init.xavier_uniform_(weight)\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        self.drop1 = nn.Dropout(self.drop_prob)  # (bs, c_len, hid_size)\n",
    "        self.drop2 = nn.Dropout(self.drop_prob)\n",
    "        self.output_layer1 = nn.Linear(4*hidden_size,64)\n",
    "        self.drop3 = nn.Dropout(self.drop_prob)\n",
    "        self.output_layer2 = nn.Linear(64,1)\n",
    "        self.drop4 = nn.Dropout(self.drop_prob)\n",
    "        nn.init.xavier_uniform_(self.output_layer1.weight)\n",
    "        nn.init.xavier_uniform_(self.output_layer2.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, c, q, c_mask, q_mask):\n",
    "        batch_size, c_len, _ = c.size()\n",
    "        q_len = q.size(1)\n",
    "        s = self.get_similarity_matrix(c, q)        # (batch_size, c_len, q_len)\n",
    "        c_mask = c_mask.view(batch_size, c_len, 1)  # (batch_size, c_len, 1)\n",
    "        q_mask = q_mask.view(batch_size, 1, q_len)  # (batch_size, 1, q_len)\n",
    "        s1 = masked_softmax(s, q_mask, dim=2)       # (batch_size, c_len, q_len)\n",
    "        s2 = masked_softmax(s, c_mask, dim=1)       # (batch_size, c_len, q_len)\n",
    "\n",
    "        # (bs, c_len, q_len) x (bs, q_len, hid_size) => (bs, c_len, hid_size)\n",
    "        a = torch.bmm(s1, q)\n",
    "        # (bs, c_len, c_len) x (bs, c_len, hid_size) => (bs, c_len, hid_size)\n",
    "        b = torch.bmm(torch.bmm(s1, s2.transpose(1, 2)), c)\n",
    "\n",
    "        \n",
    "        x = torch.cat([c, a, c * a, c * b], dim=2)  # (bs, c_len, 4 * hid_size)\n",
    "        \n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        x = self.output_layer1(x)\n",
    "        \n",
    "        x = torch.nn.ELU()(x)\n",
    "        \n",
    "        x = self.drop4(x)\n",
    "        \n",
    "        x = self.output_layer2(x)\n",
    "        \n",
    "        return x.squeeze(-1)\n",
    "\n",
    "    def get_similarity_matrix(self, c, q):\n",
    "        \"\"\"Get the \"similarity matrix\" between context and query (using the\n",
    "        terminology of the BiDAF paper).\n",
    "\n",
    "        A naive implementation as described in BiDAF would concatenate the\n",
    "        three vectors then project the result with a single weight matrix. This\n",
    "        method is a more memory-efficient implementation of the same operation.\n",
    "\n",
    "        See Also:\n",
    "            Equation 1 in https://arxiv.org/abs/1611.01603\n",
    "        \"\"\"\n",
    "        c_len, q_len = c.size(1), q.size(1)\n",
    "        c = self.drop1(c)  # (bs, c_len, hid_size)\n",
    "        q = self.drop2(q)  # (bs, q_len, hid_size)\n",
    "        #print (c.size())\n",
    "        #print (q.size())\n",
    "        # Shapes: (batch_size, c_len, q_len)\n",
    "        s0 = torch.matmul(c, self.c_weight).expand([-1, -1, q_len])\n",
    "        s1 = torch.matmul(q, self.q_weight).transpose(1, 2)\\\n",
    "                                           .expand([-1, c_len, -1])\n",
    "        s2 = torch.matmul(c * self.cq_weight, q.transpose(1, 2))\n",
    "        s = s0 + s1 + s2 + self.bias\n",
    "\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidaf = BiDAFAttention(256).cuda()\n",
    "EPOCHS = 150\n",
    "batch_size = 25\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight = torch.Tensor([1.0,1.0,1.0])).cuda()\n",
    "opt = torch.optim.Adam(bidaf.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector</th>\n",
       "      <th>A_dist</th>\n",
       "      <th>B_dist</th>\n",
       "      <th>A_pos</th>\n",
       "      <th>B_pos</th>\n",
       "      <th>pron_pos</th>\n",
       "      <th>A_idx</th>\n",
       "      <th>B_idx</th>\n",
       "      <th>A_vector</th>\n",
       "      <th>B_vector</th>\n",
       "      <th>pron_vector</th>\n",
       "      <th>name_list_2</th>\n",
       "      <th>neither_idx</th>\n",
       "      <th>neither_idx_2</th>\n",
       "      <th>neither_vector</th>\n",
       "      <th>neither_vector_2</th>\n",
       "      <th>product_vector_A</th>\n",
       "      <th>product_vector_B</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.85816926, -0.45762736, -0.08299036, -0.275...</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.693548</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>[52, 53, 54, 55, 56, 57, 58, 59]</td>\n",
       "      <td>[63, 64, 65]</td>\n",
       "      <td>[0.20037952, 0.2805402, -0.11013528, -0.492183...</td>\n",
       "      <td>[0.6041415, -0.13538514, 0.15207358, -0.076126...</td>\n",
       "      <td>[0.8586822, -1.2192798, 0.09194927, -0.4327072...</td>\n",
       "      <td>[(III, 182)]</td>\n",
       "      <td>[39, 40, 41, 42, 43, 44, 45, 46]</td>\n",
       "      <td>[45, 46]</td>\n",
       "      <td>[[0.611566, -0.3358844, 0.46167314, -0.0807262...</td>\n",
       "      <td>[[-0.5177313, 0.72737604, -0.21791872, -1.0356...</td>\n",
       "      <td>[0.17206234, -0.342057, -0.010126858, 0.212971...</td>\n",
       "      <td>[0.51876557, 0.16507237, 0.013983054, 0.032940...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.2511816, 0.24685939, -0.3399855, -0.624934...</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>[34, 35]</td>\n",
       "      <td>[46, 47, 48]</td>\n",
       "      <td>[0.067682624, 0.42009318, -0.009991955, -0.659...</td>\n",
       "      <td>[-0.438681, 0.48596978, -0.02240046, 0.0839553...</td>\n",
       "      <td>[-0.67510414, -0.39920956, 0.07794422, 0.43378...</td>\n",
       "      <td>[(Nott, 9), (Philip, 58)]</td>\n",
       "      <td>[1, 2, 3, 4, 5, 19, 20, 21]</td>\n",
       "      <td>[4, 5, 19, 20, 21]</td>\n",
       "      <td>[[0.036648497, 0.8636709, -0.18831435, -0.6663...</td>\n",
       "      <td>[[-0.18851164, 0.22483511, -0.3515694, -0.3107...</td>\n",
       "      <td>[-0.04569282, -0.16770521, -0.0007788151, -0.2...</td>\n",
       "      <td>[0.29615536, -0.19400378, -0.0017459863, 0.036...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.8131293, 0.4440992, 0.7549018, 0.4792695, ...</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.405128</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>[89, 90, 91, 92, 93, 94, 95]</td>\n",
       "      <td>[99, 100]</td>\n",
       "      <td>[-0.40055174, -0.06318157, 0.08668772, -0.0661...</td>\n",
       "      <td>[0.67819536, 0.21544528, -0.39938855, 0.618967...</td>\n",
       "      <td>[0.72358626, -1.0747175, 0.050702423, -0.89089...</td>\n",
       "      <td>[(Tony, 52), (Todd, 57), (Angela, 179), (Angel...</td>\n",
       "      <td>[15, 16, 17, 18, 44, 45, 73, 74, 86, 87, 88, 9...</td>\n",
       "      <td>[15, 16, 17, 18, 44, 45, 73, 74, 86, 87, 88, 9...</td>\n",
       "      <td>[[0.5654289, 0.7731928, 0.3528939, 0.48137867,...</td>\n",
       "      <td>[[0.5654289, 0.7731928, 0.3528939, 0.48137867,...</td>\n",
       "      <td>[-0.28983372, 0.06790234, 0.0043952777, 0.0588...</td>\n",
       "      <td>[0.49073285, -0.23154281, -0.020249967, -0.551...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[1.033058, -0.108201064, -0.1557167, 0.394294...</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.201521</td>\n",
       "      <td>0.216730</td>\n",
       "      <td>0.224335</td>\n",
       "      <td>[66, 67, 68]</td>\n",
       "      <td>[73, 74, 75]</td>\n",
       "      <td>[-0.33478984, -0.63129133, -0.5514479, 0.59391...</td>\n",
       "      <td>[0.50680375, 0.71950006, -0.25772676, 0.375783...</td>\n",
       "      <td>[1.2850071, 0.049930945, -0.26918668, 0.253453...</td>\n",
       "      <td>[(Peter, 114), (Peter, 359)]</td>\n",
       "      <td>[23, 24, 25, 26, 27, 31, 32, 33, 47, 48, 49, 8...</td>\n",
       "      <td>[23, 24, 88, 89]</td>\n",
       "      <td>[[0.5191963, 1.1370064, 0.015690625, -0.098785...</td>\n",
       "      <td>[[0.5191963, 1.1370064, 0.015690625, -0.098785...</td>\n",
       "      <td>[-0.43020734, -0.031520974, 0.14844243, 0.1505...</td>\n",
       "      <td>[0.6512464, 0.035925318, 0.06937661, 0.0952435...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[1.3460386, 0.037629873, 0.55879486, -0.42726...</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.321839</td>\n",
       "      <td>0.362069</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>[71, 72, 73, 74]</td>\n",
       "      <td>[80, 81, 82, 83]</td>\n",
       "      <td>[0.25996214, -0.15495634, -0.5617872, -0.04960...</td>\n",
       "      <td>[0.2925861, 0.2669702, 0.03382411, -0.3578499,...</td>\n",
       "      <td>[-0.87992877, -0.07835998, 0.12989467, -0.2304...</td>\n",
       "      <td>[(Karen, 14), (Blixen, 20), (Marshall, 198), (...</td>\n",
       "      <td>[5, 6, 7, 8, 9, 23, 24, 25, 26, 46, 47, 48, 49...</td>\n",
       "      <td>[5, 6, 7, 8, 9, 46, 47, 48, 49, 50, 52, 53, 64...</td>\n",
       "      <td>[[0.49518594, 0.5576514, 0.16942936, -0.599541...</td>\n",
       "      <td>[[0.49518594, 0.5576514, 0.16942936, -0.599541...</td>\n",
       "      <td>[-0.22874817, 0.012142375, -0.07297316, 0.0114...</td>\n",
       "      <td>[-0.2574549, -0.020919777, 0.0043935715, 0.082...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              vector  A_dist  B_dist  \\\n",
       "0  [[0.85816926, -0.45762736, -0.08299036, -0.275...   0.018   0.004   \n",
       "1  [[0.2511816, 0.24685939, -0.3399855, -0.624934...   0.030   0.012   \n",
       "2  [[0.8131293, 0.4440992, 0.7549018, 0.4792695, ...   0.022   0.010   \n",
       "3  [[1.033058, -0.108201064, -0.1557167, 0.394294...   0.012   0.004   \n",
       "4  [[1.3460386, 0.037629873, 0.55879486, -0.42726...   0.056   0.042   \n",
       "\n",
       "      A_pos     B_pos  pron_pos                             A_idx  \\\n",
       "0  0.693548  0.806452  0.838710  [52, 53, 54, 55, 56, 57, 58, 59]   \n",
       "1  0.511111  0.711111  0.844444                          [34, 35]   \n",
       "2  0.405128  0.435897  0.461538      [89, 90, 91, 92, 93, 94, 95]   \n",
       "3  0.201521  0.216730  0.224335                      [66, 67, 68]   \n",
       "4  0.321839  0.362069  0.482759                  [71, 72, 73, 74]   \n",
       "\n",
       "              B_idx                                           A_vector  \\\n",
       "0      [63, 64, 65]  [0.20037952, 0.2805402, -0.11013528, -0.492183...   \n",
       "1      [46, 47, 48]  [0.067682624, 0.42009318, -0.009991955, -0.659...   \n",
       "2         [99, 100]  [-0.40055174, -0.06318157, 0.08668772, -0.0661...   \n",
       "3      [73, 74, 75]  [-0.33478984, -0.63129133, -0.5514479, 0.59391...   \n",
       "4  [80, 81, 82, 83]  [0.25996214, -0.15495634, -0.5617872, -0.04960...   \n",
       "\n",
       "                                            B_vector  \\\n",
       "0  [0.6041415, -0.13538514, 0.15207358, -0.076126...   \n",
       "1  [-0.438681, 0.48596978, -0.02240046, 0.0839553...   \n",
       "2  [0.67819536, 0.21544528, -0.39938855, 0.618967...   \n",
       "3  [0.50680375, 0.71950006, -0.25772676, 0.375783...   \n",
       "4  [0.2925861, 0.2669702, 0.03382411, -0.3578499,...   \n",
       "\n",
       "                                         pron_vector  \\\n",
       "0  [0.8586822, -1.2192798, 0.09194927, -0.4327072...   \n",
       "1  [-0.67510414, -0.39920956, 0.07794422, 0.43378...   \n",
       "2  [0.72358626, -1.0747175, 0.050702423, -0.89089...   \n",
       "3  [1.2850071, 0.049930945, -0.26918668, 0.253453...   \n",
       "4  [-0.87992877, -0.07835998, 0.12989467, -0.2304...   \n",
       "\n",
       "                                         name_list_2  \\\n",
       "0                                       [(III, 182)]   \n",
       "1                          [(Nott, 9), (Philip, 58)]   \n",
       "2  [(Tony, 52), (Todd, 57), (Angela, 179), (Angel...   \n",
       "3                       [(Peter, 114), (Peter, 359)]   \n",
       "4  [(Karen, 14), (Blixen, 20), (Marshall, 198), (...   \n",
       "\n",
       "                                         neither_idx  \\\n",
       "0                   [39, 40, 41, 42, 43, 44, 45, 46]   \n",
       "1                        [1, 2, 3, 4, 5, 19, 20, 21]   \n",
       "2  [15, 16, 17, 18, 44, 45, 73, 74, 86, 87, 88, 9...   \n",
       "3  [23, 24, 25, 26, 27, 31, 32, 33, 47, 48, 49, 8...   \n",
       "4  [5, 6, 7, 8, 9, 23, 24, 25, 26, 46, 47, 48, 49...   \n",
       "\n",
       "                                       neither_idx_2  \\\n",
       "0                                           [45, 46]   \n",
       "1                                 [4, 5, 19, 20, 21]   \n",
       "2  [15, 16, 17, 18, 44, 45, 73, 74, 86, 87, 88, 9...   \n",
       "3                                   [23, 24, 88, 89]   \n",
       "4  [5, 6, 7, 8, 9, 46, 47, 48, 49, 50, 52, 53, 64...   \n",
       "\n",
       "                                      neither_vector  \\\n",
       "0  [[0.611566, -0.3358844, 0.46167314, -0.0807262...   \n",
       "1  [[0.036648497, 0.8636709, -0.18831435, -0.6663...   \n",
       "2  [[0.5654289, 0.7731928, 0.3528939, 0.48137867,...   \n",
       "3  [[0.5191963, 1.1370064, 0.015690625, -0.098785...   \n",
       "4  [[0.49518594, 0.5576514, 0.16942936, -0.599541...   \n",
       "\n",
       "                                    neither_vector_2  \\\n",
       "0  [[-0.5177313, 0.72737604, -0.21791872, -1.0356...   \n",
       "1  [[-0.18851164, 0.22483511, -0.3515694, -0.3107...   \n",
       "2  [[0.5654289, 0.7731928, 0.3528939, 0.48137867,...   \n",
       "3  [[0.5191963, 1.1370064, 0.015690625, -0.098785...   \n",
       "4  [[0.49518594, 0.5576514, 0.16942936, -0.599541...   \n",
       "\n",
       "                                    product_vector_A  \\\n",
       "0  [0.17206234, -0.342057, -0.010126858, 0.212971...   \n",
       "1  [-0.04569282, -0.16770521, -0.0007788151, -0.2...   \n",
       "2  [-0.28983372, 0.06790234, 0.0043952777, 0.0588...   \n",
       "3  [-0.43020734, -0.031520974, 0.14844243, 0.1505...   \n",
       "4  [-0.22874817, 0.012142375, -0.07297316, 0.0114...   \n",
       "\n",
       "                                    product_vector_B  label  \n",
       "0  [0.51876557, 0.16507237, 0.013983054, 0.032940...      2  \n",
       "1  [0.29615536, -0.19400378, -0.0017459863, 0.036...      1  \n",
       "2  [0.49073285, -0.23154281, -0.020249967, -0.551...      1  \n",
       "3  [0.6512464, 0.035925318, 0.06937661, 0.0952435...      0  \n",
       "4  [-0.2574549, -0.020919777, 0.0043935715, 0.082...      1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/150], loss:0.8644\n",
      "epoch [2/150], loss:0.6550\n",
      "epoch [3/150], loss:0.5771\n",
      "epoch [4/150], loss:0.5999\n",
      "epoch [5/150], loss:0.5070\n",
      "epoch [6/150], loss:0.6008\n",
      "epoch [7/150], loss:0.4492\n",
      "epoch [8/150], loss:0.5219\n",
      "epoch [9/150], loss:0.3119\n",
      "epoch [10/150], loss:0.4693\n",
      "epoch [11/150], loss:0.5171\n",
      "epoch [12/150], loss:0.6217\n",
      "epoch [13/150], loss:0.5523\n",
      "epoch [14/150], loss:0.5282\n",
      "epoch [15/150], loss:0.6149\n",
      "epoch [16/150], loss:0.4722\n",
      "epoch [17/150], loss:0.5975\n",
      "epoch [18/150], loss:0.4465\n",
      "epoch [19/150], loss:0.4211\n",
      "epoch [20/150], loss:0.5615\n",
      "epoch [21/150], loss:0.4551\n",
      "epoch [22/150], loss:0.4754\n",
      "epoch [23/150], loss:0.6134\n",
      "epoch [24/150], loss:0.4234\n",
      "epoch [25/150], loss:0.4318\n",
      "epoch [26/150], loss:0.4605\n",
      "epoch [27/150], loss:0.5851\n",
      "epoch [28/150], loss:0.4535\n",
      "epoch [29/150], loss:0.4005\n",
      "epoch [30/150], loss:0.4447\n",
      "epoch [31/150], loss:0.6894\n",
      "epoch [32/150], loss:0.3318\n",
      "epoch [33/150], loss:0.4391\n",
      "epoch [34/150], loss:0.3353\n",
      "epoch [35/150], loss:0.4041\n",
      "epoch [36/150], loss:0.4132\n",
      "epoch [37/150], loss:0.4031\n",
      "epoch [38/150], loss:0.4564\n",
      "epoch [39/150], loss:0.3137\n",
      "epoch [40/150], loss:0.5048\n",
      "epoch [41/150], loss:0.4130\n",
      "epoch [42/150], loss:0.3401\n",
      "epoch [43/150], loss:0.5627\n",
      "epoch [44/150], loss:0.4129\n",
      "epoch [45/150], loss:0.5066\n",
      "epoch [46/150], loss:0.3890\n",
      "epoch [47/150], loss:0.5093\n",
      "epoch [48/150], loss:0.4904\n",
      "epoch [49/150], loss:0.3563\n",
      "epoch [50/150], loss:0.3966\n",
      "epoch [51/150], loss:0.5955\n",
      "epoch [52/150], loss:0.4365\n",
      "epoch [53/150], loss:0.4861\n",
      "epoch [54/150], loss:0.4805\n",
      "epoch [55/150], loss:0.3848\n",
      "epoch [56/150], loss:0.4193\n",
      "epoch [57/150], loss:0.4895\n",
      "epoch [58/150], loss:0.5355\n",
      "epoch [59/150], loss:0.5113\n",
      "epoch [60/150], loss:0.4805\n",
      "epoch [61/150], loss:0.2735\n",
      "epoch [62/150], loss:0.3329\n",
      "epoch [63/150], loss:0.4077\n",
      "epoch [64/150], loss:0.4385\n",
      "epoch [65/150], loss:0.4757\n",
      "epoch [66/150], loss:0.6359\n",
      "epoch [67/150], loss:0.3694\n",
      "epoch [68/150], loss:0.3452\n",
      "epoch [69/150], loss:0.5005\n",
      "epoch [70/150], loss:0.3668\n",
      "epoch [71/150], loss:0.5106\n",
      "epoch [72/150], loss:0.4449\n",
      "epoch [73/150], loss:0.4844\n",
      "epoch [74/150], loss:0.5255\n",
      "epoch [75/150], loss:0.3369\n",
      "epoch [76/150], loss:0.3207\n",
      "epoch [77/150], loss:0.2980\n",
      "epoch [78/150], loss:0.4780\n",
      "epoch [79/150], loss:0.4333\n",
      "epoch [80/150], loss:0.3682\n",
      "epoch [81/150], loss:0.4983\n",
      "epoch [82/150], loss:0.2254\n",
      "epoch [83/150], loss:0.4026\n",
      "epoch [84/150], loss:0.5357\n",
      "epoch [85/150], loss:0.4974\n",
      "epoch [86/150], loss:0.4858\n",
      "epoch [87/150], loss:0.3634\n",
      "epoch [88/150], loss:0.4143\n",
      "epoch [89/150], loss:0.4458\n",
      "epoch [90/150], loss:0.4716\n",
      "epoch [91/150], loss:0.5712\n",
      "epoch [92/150], loss:0.3476\n",
      "epoch [93/150], loss:0.4056\n",
      "epoch [94/150], loss:0.4220\n",
      "epoch [95/150], loss:0.4939\n",
      "epoch [96/150], loss:0.6897\n",
      "epoch [97/150], loss:0.3566\n",
      "epoch [98/150], loss:0.4560\n",
      "epoch [99/150], loss:0.3840\n",
      "epoch [100/150], loss:0.3835\n",
      "epoch [101/150], loss:0.3185\n",
      "epoch [102/150], loss:0.4905\n",
      "epoch [103/150], loss:0.4127\n",
      "epoch [104/150], loss:0.4487\n",
      "epoch [105/150], loss:0.4513\n",
      "epoch [106/150], loss:0.3654\n",
      "epoch [107/150], loss:0.4111\n",
      "epoch [108/150], loss:0.3620\n",
      "epoch [109/150], loss:0.3206\n",
      "epoch [110/150], loss:0.4149\n",
      "epoch [111/150], loss:0.4196\n",
      "epoch [112/150], loss:0.5194\n",
      "epoch [113/150], loss:0.3741\n",
      "epoch [114/150], loss:0.2805\n",
      "epoch [115/150], loss:0.4342\n",
      "epoch [116/150], loss:0.4552\n",
      "epoch [117/150], loss:0.4525\n",
      "epoch [118/150], loss:0.3277\n",
      "epoch [119/150], loss:0.2667\n",
      "epoch [120/150], loss:0.3566\n",
      "epoch [121/150], loss:0.2122\n",
      "epoch [122/150], loss:0.3435\n",
      "epoch [123/150], loss:0.4094\n",
      "epoch [124/150], loss:0.3148\n",
      "epoch [125/150], loss:0.3408\n",
      "epoch [126/150], loss:0.5322\n",
      "epoch [127/150], loss:0.2705\n",
      "epoch [128/150], loss:0.4302\n",
      "epoch [129/150], loss:0.3388\n",
      "epoch [130/150], loss:0.3615\n",
      "epoch [131/150], loss:0.5130\n",
      "epoch [132/150], loss:0.2923\n",
      "epoch [133/150], loss:0.5658\n",
      "epoch [134/150], loss:0.4809\n",
      "epoch [135/150], loss:0.4264\n",
      "epoch [136/150], loss:0.4545\n",
      "epoch [137/150], loss:0.4994\n",
      "epoch [138/150], loss:0.5018\n",
      "epoch [139/150], loss:0.5913\n",
      "epoch [140/150], loss:0.3591\n",
      "epoch [141/150], loss:0.3609\n",
      "epoch [142/150], loss:0.3868\n",
      "epoch [143/150], loss:0.4817\n",
      "epoch [144/150], loss:0.4437\n",
      "epoch [145/150], loss:0.3961\n",
      "epoch [146/150], loss:0.1942\n",
      "epoch [147/150], loss:0.4396\n",
      "epoch [148/150], loss:0.4491\n",
      "epoch [149/150], loss:0.3317\n",
      "epoch [150/150], loss:0.4187\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "for e in range(EPOCHS):\n",
    "    for b in range(0,train_data.shape[0],batch_size):\n",
    "        bidaf.train()\n",
    "        batch_data = train_data.vector[b:b+batch_size]\n",
    "        batch_label = train_data.label[b:b+batch_size]\n",
    "        batch_pron = train_data.pron_vector[b:b+batch_size]\n",
    "        batch_pron = torch.Tensor(np.array(list(batch_pron.values))).unsqueeze(1).cuda()\n",
    "        batch_data = pad_sequence([torch.Tensor(v) for v in batch_data]).cuda().transpose(0,1)\n",
    "        batch_padding = batch_data.mean(dim=1,keepdim = True)#torch.zeros(batch_data.size()[0],1,batch_data.size()[2]).cuda()*0.001\n",
    "        batch_data = torch.cat([batch_padding,batch_data],dim = 1)\n",
    "        batch_label = torch.LongTensor(list(batch_label)).cuda()\n",
    "        c_mask = torch.zeros_like(batch_data.mean(-1,keepdim = True)) != batch_data.mean(-1,keepdim = True)\n",
    "        q_mask = torch.zeros_like(batch_pron.mean(-1,keepdim = True)) != batch_pron.mean(-1,keepdim = True)\n",
    "        c_mask = c_mask.cuda()\n",
    "        q_mask = q_mask.cuda()\n",
    "        #print (batch_data.mean(-1,keepdim = True).shape)\n",
    "        output = bidaf(batch_data,batch_pron,c_mask,q_mask)\n",
    "        mask_A = [np.array(v)+1 for v in list(train_data.A_idx[b:b+batch_size])]\n",
    "        mask_B = [np.array(v)+1 for v in list(train_data.B_idx[b:b+batch_size])]\n",
    "        mask_neither = [np.array(v)+1 for v in list(train_data.neither_idx[b:b+batch_size])]\n",
    "        #neither_prob = output[:,0]\n",
    "        prob_list = []\n",
    "        for i,(v_A,v_B,v_neither) in enumerate(zip(mask_A,mask_B,mask_neither)):\n",
    "            v_A = torch.LongTensor(v_A).cuda()\n",
    "            A_prob_ = output[i,v_A].mean()\n",
    "            v_B = torch.LongTensor(v_B).cuda()\n",
    "            B_prob_ = output[i,v_B].mean()\n",
    "            #other_prob = output[i,:].sum() - A_prob_ - B_prob_\n",
    "            v_neither = list(v_neither)\n",
    "            v_neither.append(0)\n",
    "            #print (v_neither)\n",
    "            v_neither = torch.LongTensor(v_neither).cuda()\n",
    "            other_prob = output[i,v_neither].mean()\n",
    "            prob_list.append(torch.cat([A_prob_.view(1,1),B_prob_.view(1,1),other_prob.view(1,1)]).view(-1,3))\n",
    "        #print (prob_list)\n",
    "        pred_train = torch.cat(prob_list,dim = 0)\n",
    "        #print (pred_train.size())\n",
    "            \n",
    "        #batch_label = torch.LongTensor(batch_label).cuda()\n",
    "        loss = loss_fn(pred_train,batch_label)\n",
    "        #l2_norm = torch.norm(mlp.layers[-1].weight, p=2)\n",
    "        #loss += l2_norm*0.09\n",
    "        #l2_norm = torch.norm(mlp.layers[0].weight, p=2)\n",
    "        #loss += l2_norm*0.03\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(e + 1, EPOCHS, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bidaf_train = []    \n",
    "for b in range(0,train_data.shape[0],batch_size):\n",
    "    bidaf.eval()\n",
    "    batch_data = train_data.vector[b:b+batch_size]\n",
    "    batch_label = train_data.label[b:b+batch_size]\n",
    "    batch_pron = train_data.pron_vector[b:b+batch_size]\n",
    "    batch_pron = torch.Tensor(np.array(list(batch_pron.values))).unsqueeze(1).cuda()\n",
    "    batch_data = pad_sequence([torch.Tensor(v) for v in batch_data]).cuda().transpose(0,1)\n",
    "    batch_padding = batch_data.mean(dim=1,keepdim = True)#batch_padding = torch.zeros(batch_data.size()[0],1,batch_data.size()[2]).cuda()*0.001\n",
    "    batch_data = torch.cat([batch_padding,batch_data],dim = 1)\n",
    "    batch_label = torch.LongTensor(list(batch_label)).cuda()\n",
    "    c_mask = torch.zeros_like(batch_data.mean(-1,keepdim = True)) != batch_data.mean(-1,keepdim = True)\n",
    "    q_mask = torch.zeros_like(batch_pron.mean(-1,keepdim = True)) != batch_pron.mean(-1,keepdim = True)\n",
    "    c_mask = c_mask.cuda()\n",
    "    q_mask = q_mask.cuda()\n",
    "    #print (batch_data.mean(-1,keepdim = True).shape)\n",
    "    output = bidaf(batch_data,batch_pron,c_mask,q_mask)\n",
    "    mask_A = [np.array(v)+1 for v in list(train_data.A_idx[b:b+batch_size])]\n",
    "    mask_B = [np.array(v)+1 for v in list(train_data.B_idx[b:b+batch_size])]\n",
    "    mask_neither = [np.array(v)+1 for v in list(train_data.neither_idx[b:b+batch_size])]\n",
    "    #neither_prob = output[:,0]\n",
    "    prob_list = []\n",
    "    for i,(v_A,v_B,v_neither) in enumerate(zip(mask_A,mask_B,mask_neither)):\n",
    "        v_A = torch.LongTensor(v_A).cuda()\n",
    "        A_prob_ = output[i,v_A].mean()\n",
    "        v_B = torch.LongTensor(v_B).cuda()\n",
    "        B_prob_ = output[i,v_B].mean()\n",
    "        #other_prob = output[i,:].sum() - A_prob_ - B_prob_\n",
    "        v_neither = list(v_neither)\n",
    "        v_neither.append(0)\n",
    "        #print (v_neither)\n",
    "        v_neither = torch.LongTensor(v_neither).cuda()\n",
    "        other_prob = output[i,v_neither].mean()\n",
    "        prob_list.append(torch.cat([A_prob_.view(1,1),B_prob_.view(1,1),other_prob.view(1,1)]).view(-1,3))\n",
    "    #print (prob_list)\n",
    "    pred_bidaf_ = torch.cat(prob_list,dim = 0)\n",
    "    pred_bidaf_train.append(pred_bidaf_)\n",
    "pred_bidaf_train = torch.nn.Softmax(dim=1)(torch.cat(pred_bidaf_train,dim = 0)).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bidaf = []    \n",
    "for b in range(0,test_data.shape[0],batch_size):\n",
    "    bidaf.eval()\n",
    "    batch_data = test_data.vector[b:b+batch_size]\n",
    "    batch_label = test_data.label[b:b+batch_size]\n",
    "    batch_pron = test_data.pron_vector[b:b+batch_size]\n",
    "    batch_pron = torch.Tensor(np.array(list(batch_pron.values))).unsqueeze(1).cuda()\n",
    "    batch_data = pad_sequence([torch.Tensor(v) for v in batch_data]).cuda().transpose(0,1)\n",
    "    batch_padding = batch_data.mean(dim=1,keepdim = True)#batch_padding = torch.zeros(batch_data.size()[0],1,batch_data.size()[2]).cuda()*0.001\n",
    "    batch_data = torch.cat([batch_padding,batch_data],dim = 1)\n",
    "    batch_label = torch.LongTensor(list(batch_label)).cuda()\n",
    "    c_mask = torch.zeros_like(batch_data.mean(-1,keepdim = True)) != batch_data.mean(-1,keepdim = True)\n",
    "    q_mask = torch.zeros_like(batch_pron.mean(-1,keepdim = True)) != batch_pron.mean(-1,keepdim = True)\n",
    "    c_mask = c_mask.cuda()\n",
    "    q_mask = q_mask.cuda()\n",
    "    #print (batch_data.mean(-1,keepdim = True).shape)\n",
    "    output = bidaf(batch_data,batch_pron,c_mask,q_mask)\n",
    "    mask_A = [np.array(v)+1 for v in list(test_data.A_idx[b:b+batch_size])]\n",
    "    mask_B = [np.array(v)+1 for v in list(test_data.B_idx[b:b+batch_size])]\n",
    "    mask_neither = [np.array(v)+1 for v in list(test_data.neither_idx[b:b+batch_size])]\n",
    "    #neither_prob = output[:,0]\n",
    "    prob_list = []\n",
    "    for i,(v_A,v_B,v_neither) in enumerate(zip(mask_A,mask_B,mask_neither)):\n",
    "        v_A = torch.LongTensor(v_A).cuda()\n",
    "        A_prob_ = output[i,v_A].mean()\n",
    "        v_B = torch.LongTensor(v_B).cuda()\n",
    "        B_prob_ = output[i,v_B].mean()\n",
    "        #other_prob = output[i,:].sum() - A_prob_ - B_prob_\n",
    "        v_neither = list(v_neither)\n",
    "        v_neither.append(0)\n",
    "        #print (v_neither)\n",
    "        v_neither = torch.LongTensor(v_neither).cuda()\n",
    "        other_prob = output[i,v_neither].mean()\n",
    "        prob_list.append(torch.cat([A_prob_.view(1,1),B_prob_.view(1,1),other_prob.view(1,1)]).view(-1,3))\n",
    "    #print (prob_list)\n",
    "    pred_bidaf_ = torch.cat(prob_list,dim = 0)\n",
    "    pred_bidaf.append(pred_bidaf_)\n",
    "pred_bidaf = torch.nn.Softmax(dim=1)(torch.cat(pred_bidaf,dim = 0)).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open( \"./temp_result/biDAF_result_2\", \"wb\" )\n",
    "pickle.dump(pred_bidaf_train,  f)\n",
    "pickle.dump(pred_bidaf,  f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bidaf = np.clip(pred_bidaf,1e-15,1-1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.587262</td>\n",
       "      <td>0.277048</td>\n",
       "      <td>0.135690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.993745</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.002097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.009459</td>\n",
       "      <td>0.900975</td>\n",
       "      <td>0.089567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.009563</td>\n",
       "      <td>0.800297</td>\n",
       "      <td>0.190140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.998760</td>\n",
       "      <td>0.001006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>development-6</td>\n",
       "      <td>0.994175</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>0.003107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>development-7</td>\n",
       "      <td>0.444239</td>\n",
       "      <td>0.169771</td>\n",
       "      <td>0.385990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>development-8</td>\n",
       "      <td>0.136010</td>\n",
       "      <td>0.848826</td>\n",
       "      <td>0.015164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>development-9</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>0.956923</td>\n",
       "      <td>0.039186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>development-10</td>\n",
       "      <td>0.322818</td>\n",
       "      <td>0.538040</td>\n",
       "      <td>0.139142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>development-11</td>\n",
       "      <td>0.083233</td>\n",
       "      <td>0.725817</td>\n",
       "      <td>0.190949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>development-12</td>\n",
       "      <td>0.908124</td>\n",
       "      <td>0.071682</td>\n",
       "      <td>0.020194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>development-13</td>\n",
       "      <td>0.835997</td>\n",
       "      <td>0.075621</td>\n",
       "      <td>0.088382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>development-14</td>\n",
       "      <td>0.666795</td>\n",
       "      <td>0.267823</td>\n",
       "      <td>0.065382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>development-15</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>0.867694</td>\n",
       "      <td>0.073326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>development-16</td>\n",
       "      <td>0.003262</td>\n",
       "      <td>0.994879</td>\n",
       "      <td>0.001858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>development-17</td>\n",
       "      <td>0.960777</td>\n",
       "      <td>0.015449</td>\n",
       "      <td>0.023773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>development-18</td>\n",
       "      <td>0.168917</td>\n",
       "      <td>0.031597</td>\n",
       "      <td>0.799486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>development-19</td>\n",
       "      <td>0.399007</td>\n",
       "      <td>0.403132</td>\n",
       "      <td>0.197860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>development-20</td>\n",
       "      <td>0.017380</td>\n",
       "      <td>0.945372</td>\n",
       "      <td>0.037248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>development-21</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.999647</td>\n",
       "      <td>0.000348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>development-22</td>\n",
       "      <td>0.912186</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.086994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>development-23</td>\n",
       "      <td>0.996468</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.003495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>development-24</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>0.956380</td>\n",
       "      <td>0.041271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>development-25</td>\n",
       "      <td>0.964083</td>\n",
       "      <td>0.007313</td>\n",
       "      <td>0.028604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>development-26</td>\n",
       "      <td>0.051618</td>\n",
       "      <td>0.771942</td>\n",
       "      <td>0.176440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>development-27</td>\n",
       "      <td>0.394865</td>\n",
       "      <td>0.542701</td>\n",
       "      <td>0.062434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>development-28</td>\n",
       "      <td>0.581942</td>\n",
       "      <td>0.286040</td>\n",
       "      <td>0.132018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>development-29</td>\n",
       "      <td>0.350432</td>\n",
       "      <td>0.499170</td>\n",
       "      <td>0.150398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>development-30</td>\n",
       "      <td>0.945330</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.053643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>development-31</td>\n",
       "      <td>0.651703</td>\n",
       "      <td>0.261965</td>\n",
       "      <td>0.086332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>development-32</td>\n",
       "      <td>0.447340</td>\n",
       "      <td>0.056375</td>\n",
       "      <td>0.496285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>development-33</td>\n",
       "      <td>0.022741</td>\n",
       "      <td>0.948469</td>\n",
       "      <td>0.028790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>development-34</td>\n",
       "      <td>0.143136</td>\n",
       "      <td>0.196154</td>\n",
       "      <td>0.660711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>development-35</td>\n",
       "      <td>0.902668</td>\n",
       "      <td>0.033683</td>\n",
       "      <td>0.063649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>development-36</td>\n",
       "      <td>0.108456</td>\n",
       "      <td>0.857757</td>\n",
       "      <td>0.033787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>development-37</td>\n",
       "      <td>0.069590</td>\n",
       "      <td>0.924144</td>\n",
       "      <td>0.006266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>development-38</td>\n",
       "      <td>0.842735</td>\n",
       "      <td>0.074307</td>\n",
       "      <td>0.082958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID         A         B   NEITHER\n",
       "0    development-1  0.587262  0.277048  0.135690\n",
       "1    development-2  0.993745  0.004157  0.002097\n",
       "2    development-3  0.009459  0.900975  0.089567\n",
       "3    development-4  0.009563  0.800297  0.190140\n",
       "4    development-5  0.000234  0.998760  0.001006\n",
       "5    development-6  0.994175  0.002718  0.003107\n",
       "6    development-7  0.444239  0.169771  0.385990\n",
       "7    development-8  0.136010  0.848826  0.015164\n",
       "8    development-9  0.003892  0.956923  0.039186\n",
       "9   development-10  0.322818  0.538040  0.139142\n",
       "10  development-11  0.083233  0.725817  0.190949\n",
       "11  development-12  0.908124  0.071682  0.020194\n",
       "12  development-13  0.835997  0.075621  0.088382\n",
       "13  development-14  0.666795  0.267823  0.065382\n",
       "14  development-15  0.058980  0.867694  0.073326\n",
       "15  development-16  0.003262  0.994879  0.001858\n",
       "16  development-17  0.960777  0.015449  0.023773\n",
       "17  development-18  0.168917  0.031597  0.799486\n",
       "18  development-19  0.399007  0.403132  0.197860\n",
       "19  development-20  0.017380  0.945372  0.037248\n",
       "20  development-21  0.000005  0.999647  0.000348\n",
       "21  development-22  0.912186  0.000820  0.086994\n",
       "22  development-23  0.996468  0.000037  0.003495\n",
       "23  development-24  0.002349  0.956380  0.041271\n",
       "24  development-25  0.964083  0.007313  0.028604\n",
       "25  development-26  0.051618  0.771942  0.176440\n",
       "26  development-27  0.394865  0.542701  0.062434\n",
       "27  development-28  0.581942  0.286040  0.132018\n",
       "28  development-29  0.350432  0.499170  0.150398\n",
       "29  development-30  0.945330  0.001027  0.053643\n",
       "30  development-31  0.651703  0.261965  0.086332\n",
       "31  development-32  0.447340  0.056375  0.496285\n",
       "32  development-33  0.022741  0.948469  0.028790\n",
       "33  development-34  0.143136  0.196154  0.660711\n",
       "34  development-35  0.902668  0.033683  0.063649\n",
       "35  development-36  0.108456  0.857757  0.033787\n",
       "36  development-37  0.069590  0.924144  0.006266\n",
       "37  development-38  0.842735  0.074307  0.082958"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pred_lr = process_prediction(pred_lr)\n",
    "sub_df = pd.read_csv(\"./test_and_submit/sample_submission_stage_1.csv\")\n",
    "sub_df.loc[:, ['A','B','NEITHER']] = pred_bidaf\n",
    "\n",
    "\n",
    "sub_df.to_csv(\"./test_and_submit/submission+model+bidaf@\"+str(datetime.datetime.now())+\".csv\", index=False)\n",
    "\n",
    "sub_df.head(38)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4821200668811798"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(sub_df,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
