{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "spacy_model = \"en_core_web_sm\"\n",
    "nlp = spacy.load(spacy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train= pd.read_pickle('./temp_result/train_kaggle_cropped')\n",
    "gap_test= pd.read_pickle('./temp_result/test_kaggle_cropped')\n",
    "gap_valid= pd.read_pickle('./temp_result/valid_kaggle_cropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_map(sentence,token_list):\n",
    "    token_map = {}\n",
    "    i = 0\n",
    "    #print (token_list)\n",
    "    token_list = token_list[1:-1]\n",
    "    #print (token_list)\n",
    "    for t in token_list:\n",
    "        #print (i)\n",
    "        if t!= \"#\":\n",
    "            t = t.strip(\"#\")\n",
    "        while sentence[i:i+len(t)].lower()!=t:\n",
    "            #print (sentence[i:i+len(t)].lower())\n",
    "            i = i + 1\n",
    "        token_map[i] = t\n",
    "        #print (token_map)\n",
    "        i = i + len(t)\n",
    "    return token_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_map(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    sentence_map = {}\n",
    "    i = 0\n",
    "\n",
    "    for s in doc.sents:\n",
    "        s = str(s)\n",
    "        while sentence[i:i+len(s)]!=s:\n",
    "            i = i + 1\n",
    "        sentence_map[i] = s\n",
    "        i = i + len(s)\n",
    "    return sentence_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train['token_map'] = gap_train.apply(lambda x: get_token_map(x.Text, x.tokens), axis=1)\n",
    "gap_test['token_map'] = gap_test.apply(lambda x: get_token_map(x.Text, x.tokens), axis=1)\n",
    "gap_valid['token_map'] = gap_valid.apply(lambda x: get_token_map(x.Text, x.tokens), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train['sentence_map'] = gap_train.Text.map(get_sentence_map)\n",
    "gap_test['sentence_map'] = gap_test.Text.map(get_sentence_map)\n",
    "gap_valid['sentence_map'] = gap_valid.Text.map(get_sentence_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(sentence,A,B):\n",
    "    start = min(A,B)\n",
    "    end = max(A,B)\n",
    "    dist = nlp(sentence[start:end])\n",
    "    return (B-A)/abs(B-A)*len(dist)/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train['A_dist'] = gap_train.apply(lambda x: get_distance(x.Text, x[\"A-offset\"],x['Pronoun-offset']), axis=1)\n",
    "gap_test['A_dist'] = gap_test.apply(lambda x: get_distance(x.Text, x[\"A-offset\"],x['Pronoun-offset']), axis=1)\n",
    "gap_valid['A_dist'] = gap_valid.apply(lambda x: get_distance(x.Text, x[\"A-offset\"],x['Pronoun-offset']), axis=1)\n",
    "gap_train['B_dist'] = gap_train.apply(lambda x: get_distance(x.Text, x[\"B-offset\"],x['Pronoun-offset']), axis=1)\n",
    "gap_test['B_dist'] = gap_test.apply(lambda x: get_distance(x.Text, x[\"B-offset\"],x['Pronoun-offset']), axis=1)\n",
    "gap_valid['B_dist'] = gap_valid.apply(lambda x: get_distance(x.Text, x[\"B-offset\"],x['Pronoun-offset']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_pos(sentence,offset,sentence_map):\n",
    "    for i in sorted(sentence_map.keys()):\n",
    "        if offset >= i:\n",
    "            break\n",
    "    return len(nlp(sentence[i:offset]))/len(sentence_map[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train['A_pos'] = gap_train.apply(lambda x: get_relative_pos(x.Text, x[\"A-offset\"],x['sentence_map']), axis=1)\n",
    "gap_test['A_pos'] = gap_test.apply(lambda x: get_relative_pos(x.Text, x[\"A-offset\"],x['sentence_map']), axis=1)\n",
    "gap_valid['A_pos'] = gap_valid.apply(lambda x: get_relative_pos(x.Text, x[\"A-offset\"],x['sentence_map']), axis=1)\n",
    "gap_train['B_pos'] = gap_train.apply(lambda x: get_relative_pos(x.Text, x[\"B-offset\"],x['sentence_map']), axis=1)\n",
    "gap_test['B_pos'] = gap_test.apply(lambda x: get_relative_pos(x.Text, x[\"B-offset\"],x['sentence_map']), axis=1)\n",
    "gap_valid['B_pos'] = gap_valid.apply(lambda x: get_relative_pos(x.Text, x[\"B-offset\"],x['sentence_map']), axis=1)\n",
    "gap_train['pron_pos'] = gap_train.apply(lambda x: get_relative_pos(x.Text, x[\"Pronoun-offset\"],x['sentence_map']), axis=1)\n",
    "gap_test['pron_pos'] = gap_test.apply(lambda x: get_relative_pos(x.Text, x[\"Pronoun-offset\"],x['sentence_map']), axis=1)\n",
    "gap_valid['pron_pos'] = gap_valid.apply(lambda x: get_relative_pos(x.Text, x[\"Pronoun-offset\"],x['sentence_map']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_index(name,offset,token_map):\n",
    "    name = \"\".join(name.lower().split(\" \"))\n",
    "    idx = 0\n",
    "    s = \"\"\n",
    "    res = []\n",
    "    for i in sorted(token_map.keys()):\n",
    "        idx = idx + 1\n",
    "        if i < offset:\n",
    "            continue\n",
    "        else:\n",
    "            s = s+token_map[i]\n",
    "            res.append(idx)\n",
    "            if s == name:\n",
    "                break\n",
    "    return np.array(res)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train['A_idx'] = gap_train.apply(lambda x: get_vector_index(x.A, x[\"A-offset\"],x['token_map']), axis=1)\n",
    "gap_test['A_idx'] = gap_test.apply(lambda x: get_vector_index(x.A, x[\"A-offset\"],x['token_map']), axis=1)\n",
    "gap_valid['A_idx'] = gap_valid.apply(lambda x: get_vector_index(x.A, x[\"A-offset\"],x['token_map']), axis=1)\n",
    "gap_train['B_idx'] = gap_train.apply(lambda x: get_vector_index(x.B, x[\"B-offset\"],x['token_map']), axis=1)\n",
    "gap_test['B_idx'] = gap_test.apply(lambda x: get_vector_index(x.B, x[\"B-offset\"],x['token_map']), axis=1)\n",
    "gap_valid['B_idx'] = gap_valid.apply(lambda x: get_vector_index(x.B, x[\"B-offset\"],x['token_map']), axis=1)\n",
    "gap_train['pron_idx'] = gap_train.apply(lambda x: get_vector_index(x.Pronoun, x[\"Pronoun-offset\"],x['token_map']), axis=1)\n",
    "gap_test['pron_idx'] = gap_test.apply(lambda x: get_vector_index(x.Pronoun, x[\"Pronoun-offset\"],x['token_map']), axis=1)\n",
    "gap_valid['pron_idx'] = gap_valid.apply(lambda x: get_vector_index(x.Pronoun, x[\"Pronoun-offset\"],x['token_map']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train['A_vector'] = gap_train.apply(lambda x: x[\"vector\"][x['A_idx'],:], axis=1)\n",
    "gap_test['A_vector'] = gap_test.apply(lambda x: x[\"vector\"][x['A_idx'],:], axis=1)\n",
    "gap_valid['A_vector'] = gap_valid.apply(lambda x: x[\"vector\"][x['A_idx'],:], axis=1)\n",
    "gap_train['B_vector'] = gap_train.apply(lambda x: x[\"vector\"][x['B_idx'],:], axis=1)\n",
    "gap_test['B_vector'] = gap_test.apply(lambda x: x[\"vector\"][x['B_idx'],:], axis=1)\n",
    "gap_valid['B_vector'] = gap_valid.apply(lambda x: x[\"vector\"][x['B_idx'],:], axis=1)\n",
    "gap_train['pron_vector'] = gap_train.apply(lambda x: x[\"vector\"][x['pron_idx'],:], axis=1)\n",
    "gap_test['pron_vector'] = gap_test.apply(lambda x: x[\"vector\"][x['pron_idx'],:], axis=1)\n",
    "gap_valid['pron_vector'] = gap_valid.apply(lambda x: x[\"vector\"][x['pron_idx'],:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>...</th>\n",
       "      <th>B_dist</th>\n",
       "      <th>A_pos</th>\n",
       "      <th>B_pos</th>\n",
       "      <th>pron_pos</th>\n",
       "      <th>A_idx</th>\n",
       "      <th>B_idx</th>\n",
       "      <th>pron_idx</th>\n",
       "      <th>A_vector</th>\n",
       "      <th>B_vector</th>\n",
       "      <th>pron_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test-1</td>\n",
       "      <td>Upon their acceptance into the Kontinental Hoc...</td>\n",
       "      <td>His</td>\n",
       "      <td>383</td>\n",
       "      <td>Bob Suter</td>\n",
       "      <td>352</td>\n",
       "      <td>False</td>\n",
       "      <td>Dehner</td>\n",
       "      <td>366</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.464052</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>[96, 97, 98, 99]</td>\n",
       "      <td>[101, 102, 103]</td>\n",
       "      <td>[109]</td>\n",
       "      <td>[[-0.23477754, -0.6348008, -0.22518, -0.033234...</td>\n",
       "      <td>[[-0.4742566, 0.17008828, -0.39835194, -0.6357...</td>\n",
       "      <td>[[-0.58112746, -0.31262234, 0.6013951, -0.5986...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test-2</td>\n",
       "      <td>Between the years 1979-1981, River won four lo...</td>\n",
       "      <td>him</td>\n",
       "      <td>430</td>\n",
       "      <td>Alonso</td>\n",
       "      <td>353</td>\n",
       "      <td>True</td>\n",
       "      <td>Alfredo Di St*fano</td>\n",
       "      <td>390</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.253623</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>0.300725</td>\n",
       "      <td>[92, 93, 94]</td>\n",
       "      <td>[100, 101, 102, 103, 104, 105, 106, 107, 108]</td>\n",
       "      <td>[113]</td>\n",
       "      <td>[[-0.37184137, -1.0916334, -0.3613571, -0.4031...</td>\n",
       "      <td>[[0.02601213, -0.046142176, 0.22424094, -0.601...</td>\n",
       "      <td>[[-0.311719, 0.7609918, 0.34107757, 0.17696574...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test-3</td>\n",
       "      <td>Though his emigration from the country has aff...</td>\n",
       "      <td>He</td>\n",
       "      <td>312</td>\n",
       "      <td>Ali Aladhadh</td>\n",
       "      <td>256</td>\n",
       "      <td>True</td>\n",
       "      <td>Saddam</td>\n",
       "      <td>295</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>[63, 64, 65, 66, 67, 68]</td>\n",
       "      <td>[75, 76]</td>\n",
       "      <td>[81]</td>\n",
       "      <td>[[0.27122185, -0.44208205, -0.7428727, 0.18834...</td>\n",
       "      <td>[[-0.637481, 0.540418, 0.4081558, -1.191327, 0...</td>\n",
       "      <td>[[-0.08557334, 0.08147873, 0.049174372, 0.4888...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test-4</td>\n",
       "      <td>At the trial, Pisciotta said: ``Those who have...</td>\n",
       "      <td>his</td>\n",
       "      <td>526</td>\n",
       "      <td>Alliata</td>\n",
       "      <td>377</td>\n",
       "      <td>False</td>\n",
       "      <td>Pisciotta</td>\n",
       "      <td>536</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.222951</td>\n",
       "      <td>0.321311</td>\n",
       "      <td>0.314754</td>\n",
       "      <td>[108, 109, 110]</td>\n",
       "      <td>[143, 144, 145, 146]</td>\n",
       "      <td>[141]</td>\n",
       "      <td>[[-0.2239024, 0.60971785, -0.035572127, -1.431...</td>\n",
       "      <td>[[0.970224, -0.96855867, -0.72124153, -0.06507...</td>\n",
       "      <td>[[-0.40841633, 0.23993859, 0.16797814, -0.2843...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test-5</td>\n",
       "      <td>It is about a pair of United States Navy shore...</td>\n",
       "      <td>his</td>\n",
       "      <td>406</td>\n",
       "      <td>Eddie</td>\n",
       "      <td>421</td>\n",
       "      <td>True</td>\n",
       "      <td>Rock Reilly</td>\n",
       "      <td>559</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>0.607527</td>\n",
       "      <td>0.456989</td>\n",
       "      <td>[113, 114]</td>\n",
       "      <td>[142, 143, 144, 145]</td>\n",
       "      <td>[110]</td>\n",
       "      <td>[[0.16017668, 0.596208, 0.15257946, -0.0950187...</td>\n",
       "      <td>[[0.31907678, 0.81342745, -1.005262, 0.2917817...</td>\n",
       "      <td>[[-0.6242132, 0.7426739, -0.61988443, -0.65061...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                               Text Pronoun  \\\n",
       "0  test-1  Upon their acceptance into the Kontinental Hoc...     His   \n",
       "1  test-2  Between the years 1979-1981, River won four lo...     him   \n",
       "2  test-3  Though his emigration from the country has aff...      He   \n",
       "3  test-4  At the trial, Pisciotta said: ``Those who have...     his   \n",
       "4  test-5  It is about a pair of United States Navy shore...     his   \n",
       "\n",
       "   Pronoun-offset             A  A-offset  A-coref                   B  \\\n",
       "0             383     Bob Suter       352    False              Dehner   \n",
       "1             430        Alonso       353     True  Alfredo Di St*fano   \n",
       "2             312  Ali Aladhadh       256     True              Saddam   \n",
       "3             526       Alliata       377    False           Pisciotta   \n",
       "4             406         Eddie       421     True         Rock Reilly   \n",
       "\n",
       "   B-offset  B-coref  ... B_dist     A_pos     B_pos  pron_pos  \\\n",
       "0       366     True  ...  0.008  0.444444  0.464052  0.490196   \n",
       "1       390    False  ...  0.014  0.253623  0.275362  0.300725   \n",
       "2       295    False  ...  0.008  0.408333  0.475000  0.508333   \n",
       "3       536     True  ... -0.004  0.222951  0.321311  0.314754   \n",
       "4       559    False  ... -0.056  0.473118  0.607527  0.456989   \n",
       "\n",
       "                      A_idx                                          B_idx  \\\n",
       "0          [96, 97, 98, 99]                                [101, 102, 103]   \n",
       "1              [92, 93, 94]  [100, 101, 102, 103, 104, 105, 106, 107, 108]   \n",
       "2  [63, 64, 65, 66, 67, 68]                                       [75, 76]   \n",
       "3           [108, 109, 110]                           [143, 144, 145, 146]   \n",
       "4                [113, 114]                           [142, 143, 144, 145]   \n",
       "\n",
       "   pron_idx                                           A_vector  \\\n",
       "0     [109]  [[-0.23477754, -0.6348008, -0.22518, -0.033234...   \n",
       "1     [113]  [[-0.37184137, -1.0916334, -0.3613571, -0.4031...   \n",
       "2      [81]  [[0.27122185, -0.44208205, -0.7428727, 0.18834...   \n",
       "3     [141]  [[-0.2239024, 0.60971785, -0.035572127, -1.431...   \n",
       "4     [110]  [[0.16017668, 0.596208, 0.15257946, -0.0950187...   \n",
       "\n",
       "                                            B_vector  \\\n",
       "0  [[-0.4742566, 0.17008828, -0.39835194, -0.6357...   \n",
       "1  [[0.02601213, -0.046142176, 0.22424094, -0.601...   \n",
       "2  [[-0.637481, 0.540418, 0.4081558, -1.191327, 0...   \n",
       "3  [[0.970224, -0.96855867, -0.72124153, -0.06507...   \n",
       "4  [[0.31907678, 0.81342745, -1.005262, 0.2917817...   \n",
       "\n",
       "                                         pron_vector  \n",
       "0  [[-0.58112746, -0.31262234, 0.6013951, -0.5986...  \n",
       "1  [[-0.311719, 0.7609918, 0.34107757, 0.17696574...  \n",
       "2  [[-0.08557334, 0.08147873, 0.049174372, 0.4888...  \n",
       "3  [[-0.40841633, 0.23993859, 0.16797814, -0.2843...  \n",
       "4  [[-0.6242132, 0.7426739, -0.61988443, -0.65061...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gap_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train.to_pickle('./temp_result/train_kaggle_processed')\n",
    "gap_test.to_pickle('./temp_result/test_kaggle_processed')\n",
    "gap_valid.to_pickle('./temp_result/valid_kaggle_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Text', 'Pronoun', 'Pronoun-offset', 'A', 'A-offset', 'A-coref',\n",
       "       'B', 'B-offset', 'B-coref', 'URL', 'vector', 'tokens', 'token_map',\n",
       "       'sentence_map', 'A_dist', 'B_dist', 'A_pos', 'B_pos', 'pron_pos',\n",
       "       'A_idx', 'B_idx', 'pron_idx', 'A_vector', 'B_vector', 'pron_vector'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gap_train.columns\n",
    "# vector all bert vectors\n",
    "# tokens the list of tokens\n",
    "# token_map  a dict from the position of the tokens to tokens\n",
    "# sentence_map a dict from the position of the entence to entence\n",
    "# A_dist A_dist = (the number of words between the A and the pron)/500. If the pron. appears after the A, then it is positive and vice versa.\n",
    "# A_pos (the number of words between the A and the head of the sentence contains A)/the number of the words of the sentence contains A\n",
    "# A_idx the index of the vector of A\n",
    "# A_vector the bert vector which corresponds to A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
