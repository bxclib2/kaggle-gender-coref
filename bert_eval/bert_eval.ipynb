{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'file_path': 'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv',\n",
       "  'pickle_path': '../temp_result/train_data.pkl',\n",
       "  'pickle_path_augument': '../temp_result/train_data_a.pkl'},\n",
       " 'test': {'file_path': 'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv',\n",
       "  'pickle_path': '../temp_result/test_data.pkl',\n",
       "  'pickle_path_augument': '../temp_result/test_data_a.pkl'},\n",
       " 'valid': {'file_path': 'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv',\n",
       "  'pickle_path': '../temp_result/valid_data.pkl',\n",
       "  'pickle_path_augument': '../temp_result/valid_data_a.pkl'},\n",
       " 'stage2': {'file_path': 'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv',\n",
       "  'pickle_path': '../temp_result/stage2_data.pkl'}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import datetime\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils.get_settings import parse\n",
    "\n",
    "settings = parse(\"../utils\")\n",
    "\n",
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, bert_hidden_size):\n",
    "        super().__init__()\n",
    "        self.bert_hidden_size = bert_hidden_size\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(bert_hidden_size, 1),\n",
    "\n",
    "        )\n",
    "        \n",
    "    def forward(self, vector):\n",
    "        return self.fc(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump,load\n",
    "model_name = \"bert_large_rd0\"\n",
    "bert_model = load(model_name+\".joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert,mlp = bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_data = pd.read_pickle(settings[\"stage2\"][\"pickle_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing data is: 454\n"
     ]
    }
   ],
   "source": [
    "# form test_data\n",
    "test_data['indexed_tokens'] = test_data['bert_tokens_torch'].apply(lambda x: tokenizer.convert_tokens_to_ids(x))\n",
    "test_indexed_tokens = list(test_data['indexed_tokens'])\n",
    "test_offsets_A = list(test_data['A_idx_bert_torch'])\n",
    "test_offsets_B = list(test_data['B_idx_bert_torch'])\n",
    "test_offsets_pron = list(test_data['pron_idx_bert_torch'])\n",
    "test_offsets_other = list(test_data['neither_idx_bert_torch'])\n",
    "test_offsets_cls = [np.array([0])]*len(test_offsets_other)\n",
    "print('Number of testing data is:',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vectors(bert_last_layer, offsets, offsets_mask):\n",
    "    extracted_vectors = bert_last_layer.gather(\n",
    "        1, offsets.unsqueeze(2).expand(-1, -1, 1))\n",
    "    vector_masked = extracted_vectors.mul(offsets_mask.unsqueeze(2).float()).sum(1)/(offsets_mask.sum(1).unsqueeze(1).float())\n",
    "    return vector_masked\n",
    "  \n",
    "def get_padded_and_mask(sequence, left, right):\n",
    "    seq_ori = sequence[left:right]\n",
    "    seq_padded = pad_sequence([torch.Tensor(v) for v in seq_ori],batch_first = True).cuda().long()\n",
    "    mask = (seq_padded != 0).cuda().long()\n",
    "    return seq_padded, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "re = []\n",
    "bert.eval()\n",
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    for b in range(0,len(test_offsets_cls),1):\n",
    "        start = b\n",
    "        end = min(len(test_offsets_cls),b+1)\n",
    "        tr_X_padded, attention_mask = get_padded_and_mask(test_indexed_tokens, start, end)\n",
    "        tr_A_padded, tr_A_mask = get_padded_and_mask(test_offsets_A, start, end)\n",
    "        tr_B_padded, tr_B_mask = get_padded_and_mask(test_offsets_B, start, end)\n",
    "        tr_other_padded, tr_other_mask = get_padded_and_mask([np.array(list(i)+list(j)) for i,j in zip(test_offsets_cls,test_offsets_pron)], start, end)\n",
    "        segments_tensors = torch.zeros(tr_X_padded.size()).cuda().long()\n",
    "\n",
    "        encoded_bert, _ = bert(tr_X_padded, segments_tensors,attention_mask,output_all_encoded_layers=False)\n",
    "\n",
    "        pred_ = mlp(encoded_bert)\n",
    "        pred = torch.cat([extract_vectors(pred_, tr_A_padded, tr_A_mask),\\\n",
    "                                  extract_vectors(pred_, tr_B_padded, tr_B_mask),extract_vectors(pred_, tr_other_padded, tr_other_mask)],1)\n",
    "        pred_data = pred.data.cpu().numpy()\n",
    "        re.append(pred_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../ensemble/bert_large_rd0+stage2.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "pred = softmax(np.concatenate(re).reshape(-1,3),axis = 1)\n",
    "dump(pred,\"../ensemble/\"+ model_name+'+stage2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
