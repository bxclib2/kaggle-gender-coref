{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train= pd.read_pickle('./temp_result/train_kaggle_processed_PPA_PCA_PPA')\n",
    "gap_test= pd.read_pickle('./temp_result/test_kaggle_processed_PPA_PCA_PPA')\n",
    "gap_valid= pd.read_pickle('./temp_result/valid_kaggle_processed_PPA_PCA_PPA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(A,B):\n",
    "    if A is True:\n",
    "        return 0\n",
    "    if B is True:\n",
    "        return 1\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compute_loss(sub_df,test_data):\n",
    "    pred = torch.Tensor(np.log(sub_df.loc[:,['A','B','NEITHER']].values))\n",
    "    label = torch.LongTensor(list(test_data.label))\n",
    "    loss = torch.nn.NLLLoss()\n",
    "    loss_value = loss(pred,label).item()\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gap_train.drop(columns = ['ID', 'Text', 'Pronoun', 'vector','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map','A_idx', 'B_idx', 'pron_idx'])\n",
    "train_data.A_vector = train_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data.B_vector = train_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data.pron_vector = train_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data[\"product_vector_A\"] = train_data.A_vector*train_data.pron_vector\n",
    "train_data[\"product_vector_B\"] = train_data.B_vector*train_data.pron_vector\n",
    "train_data[\"label\"] = train_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "train_data = train_data.drop(columns= [\"A-coref\",\"B-coref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = gap_test.drop(columns = ['ID', 'Text', 'Pronoun', 'vector','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map','A_idx', 'B_idx', 'pron_idx'])\n",
    "test_data.A_vector = test_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data.B_vector = test_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data.pron_vector = test_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data[\"product_vector_A\"] = test_data.A_vector*test_data.pron_vector\n",
    "test_data[\"product_vector_B\"] = test_data.B_vector*test_data.pron_vector\n",
    "test_data[\"label\"] = test_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "test_data = test_data.drop(columns= [\"A-coref\",\"B-coref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = gap_valid.drop(columns = ['ID', 'Text', 'Pronoun', 'vector','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map','A_idx', 'B_idx', 'pron_idx'])\n",
    "valid_data.A_vector = valid_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data.B_vector = valid_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data.pron_vector = valid_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data[\"product_vector_A\"] = valid_data.A_vector*valid_data.pron_vector\n",
    "valid_data[\"product_vector_B\"] = valid_data.B_vector*valid_data.pron_vector\n",
    "valid_data[\"label\"] = valid_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "valid_data = valid_data.drop(columns= [\"A-coref\",\"B-coref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = train_data.columns[:-1]\n",
    "X_train = np.concatenate([np.array(list(train_data[col])).reshape(train_data.shape[0],-1) for col in columns],axis = 1)\n",
    "y_train = list(train_data.label)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = np.concatenate([np.array(list(valid_data[col])).reshape(valid_data.shape[0],-1) for col in columns],axis = 1)\n",
    "y_valid = list(valid_data.label)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.concatenate([np.array(list(test_data[col])).reshape(test_data.shape[0],-1) for col in columns],axis = 1)\n",
    "y_test= list(test_data.label)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_dist</th>\n",
       "      <th>B_dist</th>\n",
       "      <th>A_pos</th>\n",
       "      <th>B_pos</th>\n",
       "      <th>pron_pos</th>\n",
       "      <th>A_vector</th>\n",
       "      <th>B_vector</th>\n",
       "      <th>pron_vector</th>\n",
       "      <th>product_vector_A</th>\n",
       "      <th>product_vector_B</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.464052</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>[-0.16186124, 0.10288441, -0.13267949, 0.09381...</td>\n",
       "      <td>[0.46889028, 0.719243, -0.12934478, 0.2970564,...</td>\n",
       "      <td>[0.6202348, -0.49657542, 0.5204739, -0.7274118...</td>\n",
       "      <td>[-0.10039197, -0.051089868, -0.06905621, -0.06...</td>\n",
       "      <td>[0.29082206, -0.3571584, -0.06732058, -0.21608...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.026</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.253623</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>0.300725</td>\n",
       "      <td>[-0.1530692, -0.45684996, -0.10911498, -0.1501...</td>\n",
       "      <td>[0.38673186, 0.017928414, 0.034514368, 0.03494...</td>\n",
       "      <td>[0.6709301, -0.5158502, 0.73226666, -0.5642047...</td>\n",
       "      <td>[-0.10269873, 0.23566614, -0.07990126, 0.08469...</td>\n",
       "      <td>[0.25947005, -0.009248376, 0.025273722, -0.019...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.024</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>[0.15905416, 0.3745935, 0.068947755, 0.3243495...</td>\n",
       "      <td>[0.14589697, 0.29442316, -0.3740158, 0.4216888...</td>\n",
       "      <td>[0.37514523, -0.4730158, 0.10452151, -0.033686...</td>\n",
       "      <td>[0.05966841, -0.17718863, 0.007206524, -0.0109...</td>\n",
       "      <td>[0.054732554, -0.1392668, -0.039092697, -0.014...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.222951</td>\n",
       "      <td>0.321311</td>\n",
       "      <td>0.314754</td>\n",
       "      <td>[0.05937913, 0.2836007, -0.36608246, 0.7199421...</td>\n",
       "      <td>[-0.6133348, -0.49770007, -0.5726678, 0.862215...</td>\n",
       "      <td>[0.6802153, -0.041674256, 0.36125743, 0.468463...</td>\n",
       "      <td>[0.040390592, -0.011818848, -0.13225001, 0.337...</td>\n",
       "      <td>[-0.4171997, 0.02074128, -0.2068805, 0.4039163...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>0.607527</td>\n",
       "      <td>0.456989</td>\n",
       "      <td>[0.19817783, 0.30553705, 0.70740294, -1.038957...</td>\n",
       "      <td>[-0.02850709, -0.038969748, 0.3848874, -0.7527...</td>\n",
       "      <td>[0.31947732, -0.5680447, 1.1254325, -0.7034413...</td>\n",
       "      <td>[0.06331332, -0.17355871, 0.79613423, 0.730845...</td>\n",
       "      <td>[-0.009107368, 0.02213656, 0.43316478, 0.52953...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A_dist  B_dist     A_pos     B_pos  pron_pos  \\\n",
       "0   0.014   0.008  0.444444  0.464052  0.490196   \n",
       "1   0.026   0.014  0.253623  0.275362  0.300725   \n",
       "2   0.024   0.008  0.408333  0.475000  0.508333   \n",
       "3   0.056  -0.004  0.222951  0.321311  0.314754   \n",
       "4  -0.006  -0.056  0.473118  0.607527  0.456989   \n",
       "\n",
       "                                            A_vector  \\\n",
       "0  [-0.16186124, 0.10288441, -0.13267949, 0.09381...   \n",
       "1  [-0.1530692, -0.45684996, -0.10911498, -0.1501...   \n",
       "2  [0.15905416, 0.3745935, 0.068947755, 0.3243495...   \n",
       "3  [0.05937913, 0.2836007, -0.36608246, 0.7199421...   \n",
       "4  [0.19817783, 0.30553705, 0.70740294, -1.038957...   \n",
       "\n",
       "                                            B_vector  \\\n",
       "0  [0.46889028, 0.719243, -0.12934478, 0.2970564,...   \n",
       "1  [0.38673186, 0.017928414, 0.034514368, 0.03494...   \n",
       "2  [0.14589697, 0.29442316, -0.3740158, 0.4216888...   \n",
       "3  [-0.6133348, -0.49770007, -0.5726678, 0.862215...   \n",
       "4  [-0.02850709, -0.038969748, 0.3848874, -0.7527...   \n",
       "\n",
       "                                         pron_vector  \\\n",
       "0  [0.6202348, -0.49657542, 0.5204739, -0.7274118...   \n",
       "1  [0.6709301, -0.5158502, 0.73226666, -0.5642047...   \n",
       "2  [0.37514523, -0.4730158, 0.10452151, -0.033686...   \n",
       "3  [0.6802153, -0.041674256, 0.36125743, 0.468463...   \n",
       "4  [0.31947732, -0.5680447, 1.1254325, -0.7034413...   \n",
       "\n",
       "                                    product_vector_A  \\\n",
       "0  [-0.10039197, -0.051089868, -0.06905621, -0.06...   \n",
       "1  [-0.10269873, 0.23566614, -0.07990126, 0.08469...   \n",
       "2  [0.05966841, -0.17718863, 0.007206524, -0.0109...   \n",
       "3  [0.040390592, -0.011818848, -0.13225001, 0.337...   \n",
       "4  [0.06331332, -0.17355871, 0.79613423, 0.730845...   \n",
       "\n",
       "                                    product_vector_B  label  \n",
       "0  [0.29082206, -0.3571584, -0.06732058, -0.21608...      1  \n",
       "1  [0.25947005, -0.009248376, 0.025273722, -0.019...      0  \n",
       "2  [0.054732554, -0.1392668, -0.039092697, -0.014...      0  \n",
       "3  [-0.4171997, 0.02074128, -0.2068805, 0.4039163...      1  \n",
       "4  [-0.009107368, 0.02213656, 0.43316478, 0.52953...      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pickle\\nf = open( \"./temp_result/base_model_data_PPA\", \"wb\" )\\npickle.dump(X_train,  f)\\npickle.dump(y_train,  f)\\npickle.dump(X_valid,  f)\\npickle.dump(y_valid,  f)\\npickle.dump(X_test,  f)\\nf.close()\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pickle\n",
    "f = open( \"./temp_result/base_model_data_PPA\", \"wb\" )\n",
    "pickle.dump(X_train,  f)\n",
    "pickle.dump(y_train,  f)\n",
    "pickle.dump(X_valid,  f)\n",
    "pickle.dump(y_valid,  f)\n",
    "pickle.dump(X_test,  f)\n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bao/anaconda3/envs/EPFL/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X_train, y_train)\n",
    "\n",
    "\n",
    "pred_lr = lr.predict_proba(X_test)\n",
    "\n",
    "sub_df = pd.read_csv(\"./test_and_submit/sample_submission_stage_1.csv\")\n",
    "sub_df.loc[:, ['A','B','NEITHER']] = pred_lr\n",
    "\n",
    "sub_df.head()\n",
    "\n",
    "\n",
    "sub_df.to_csv(\"./test_and_submit/submission+model+lr@\"+str(datetime.datetime.now())+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9079073667526245"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(sub_df,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(C = 7.0,verbose=True,probability = True,gamma = \"auto\").fit(X_train, y_train)\n",
    "\n",
    "\n",
    "pred_svm = svm.predict_proba(X_test)\n",
    "\n",
    "sub_df = pd.read_csv(\"./test_and_submit/sample_submission_stage_1.csv\")\n",
    "sub_df.loc[:, ['A','B','NEITHER']] = pred_svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.749392</td>\n",
       "      <td>0.179868</td>\n",
       "      <td>0.070740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.785804</td>\n",
       "      <td>0.120283</td>\n",
       "      <td>0.093913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.050113</td>\n",
       "      <td>0.921653</td>\n",
       "      <td>0.028234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.035816</td>\n",
       "      <td>0.764283</td>\n",
       "      <td>0.199901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.026254</td>\n",
       "      <td>0.787368</td>\n",
       "      <td>0.186377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID         A         B   NEITHER\n",
       "0  development-1  0.749392  0.179868  0.070740\n",
       "1  development-2  0.785804  0.120283  0.093913\n",
       "2  development-3  0.050113  0.921653  0.028234\n",
       "3  development-4  0.035816  0.764283  0.199901\n",
       "4  development-5  0.026254  0.787368  0.186377"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.to_csv(\"./test_and_submit/submission+model+svm@\"+str(datetime.datetime.now())+\".csv\", index=False)\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5225046277046204"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(sub_df,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(256*3, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.9),\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # convert tensor (128, 1, 28, 28) --> (128, 1*28*28)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "batch_size = 25\n",
    "mlp = MLP()\n",
    "mlp.cuda()\n",
    "opt = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/500], loss:1.1197\n",
      "epoch [2/500], loss:1.1078\n",
      "epoch [3/500], loss:0.9292\n",
      "epoch [4/500], loss:0.9320\n",
      "epoch [5/500], loss:0.9490\n",
      "epoch [6/500], loss:0.8478\n",
      "epoch [7/500], loss:0.8758\n",
      "epoch [8/500], loss:0.8940\n",
      "epoch [9/500], loss:0.8505\n",
      "epoch [10/500], loss:0.9135\n",
      "epoch [11/500], loss:0.8450\n",
      "epoch [12/500], loss:0.8836\n",
      "epoch [13/500], loss:0.7930\n",
      "epoch [14/500], loss:0.9137\n",
      "epoch [15/500], loss:0.8335\n",
      "epoch [16/500], loss:0.8048\n",
      "epoch [17/500], loss:0.8803\n",
      "epoch [18/500], loss:0.7324\n",
      "epoch [19/500], loss:0.9046\n",
      "epoch [20/500], loss:0.8613\n",
      "epoch [21/500], loss:0.8023\n",
      "epoch [22/500], loss:0.8345\n",
      "epoch [23/500], loss:0.8045\n",
      "epoch [24/500], loss:0.7543\n",
      "epoch [25/500], loss:0.9346\n",
      "epoch [26/500], loss:0.9208\n",
      "epoch [27/500], loss:0.7963\n",
      "epoch [28/500], loss:0.9074\n",
      "epoch [29/500], loss:0.7712\n",
      "epoch [30/500], loss:0.7533\n",
      "epoch [31/500], loss:0.7973\n",
      "epoch [32/500], loss:0.8136\n",
      "epoch [33/500], loss:0.8781\n",
      "epoch [34/500], loss:0.7810\n",
      "epoch [35/500], loss:0.7616\n",
      "epoch [36/500], loss:0.8876\n",
      "epoch [37/500], loss:0.7900\n",
      "epoch [38/500], loss:0.8232\n",
      "epoch [39/500], loss:0.8283\n",
      "epoch [40/500], loss:0.8337\n",
      "epoch [41/500], loss:0.8794\n",
      "epoch [42/500], loss:0.6849\n",
      "epoch [43/500], loss:0.8598\n",
      "epoch [44/500], loss:0.7864\n",
      "epoch [45/500], loss:0.8813\n",
      "epoch [46/500], loss:0.7841\n",
      "epoch [47/500], loss:0.8585\n",
      "epoch [48/500], loss:0.8243\n",
      "epoch [49/500], loss:0.8134\n",
      "epoch [50/500], loss:0.7599\n",
      "epoch [51/500], loss:0.7895\n",
      "epoch [52/500], loss:0.7763\n",
      "epoch [53/500], loss:0.9314\n",
      "epoch [54/500], loss:0.7347\n",
      "epoch [55/500], loss:0.8529\n",
      "epoch [56/500], loss:0.8353\n",
      "epoch [57/500], loss:0.7468\n",
      "epoch [58/500], loss:0.7826\n",
      "epoch [59/500], loss:0.8838\n",
      "epoch [60/500], loss:0.8163\n",
      "epoch [61/500], loss:0.8973\n",
      "epoch [62/500], loss:0.9109\n",
      "epoch [63/500], loss:0.7495\n",
      "epoch [64/500], loss:0.7098\n",
      "epoch [65/500], loss:0.7928\n",
      "epoch [66/500], loss:0.7868\n",
      "epoch [67/500], loss:0.7920\n",
      "epoch [68/500], loss:0.7790\n",
      "epoch [69/500], loss:0.7091\n",
      "epoch [70/500], loss:0.8524\n",
      "epoch [71/500], loss:0.7439\n",
      "epoch [72/500], loss:0.8280\n",
      "epoch [73/500], loss:0.7643\n",
      "epoch [74/500], loss:0.8318\n",
      "epoch [75/500], loss:0.7867\n",
      "epoch [76/500], loss:0.8310\n",
      "epoch [77/500], loss:0.7584\n",
      "epoch [78/500], loss:0.8205\n",
      "epoch [79/500], loss:0.8107\n",
      "epoch [80/500], loss:0.7083\n",
      "epoch [81/500], loss:0.7723\n",
      "epoch [82/500], loss:0.8213\n",
      "epoch [83/500], loss:0.7185\n",
      "epoch [84/500], loss:0.9369\n",
      "epoch [85/500], loss:0.7664\n",
      "epoch [86/500], loss:0.9319\n",
      "epoch [87/500], loss:0.9155\n",
      "epoch [88/500], loss:0.7923\n",
      "epoch [89/500], loss:0.7799\n",
      "epoch [90/500], loss:0.8445\n",
      "epoch [91/500], loss:0.8373\n",
      "epoch [92/500], loss:0.8010\n",
      "epoch [93/500], loss:0.8376\n",
      "epoch [94/500], loss:0.7983\n",
      "epoch [95/500], loss:0.8248\n",
      "epoch [96/500], loss:0.8391\n",
      "epoch [97/500], loss:0.7740\n",
      "epoch [98/500], loss:0.8958\n",
      "epoch [99/500], loss:0.7694\n",
      "epoch [100/500], loss:0.8380\n",
      "epoch [101/500], loss:0.8876\n",
      "epoch [102/500], loss:0.7444\n",
      "epoch [103/500], loss:0.8000\n",
      "epoch [104/500], loss:0.7554\n",
      "epoch [105/500], loss:0.7422\n",
      "epoch [106/500], loss:1.0118\n",
      "epoch [107/500], loss:0.8365\n",
      "epoch [108/500], loss:0.7953\n",
      "epoch [109/500], loss:0.7429\n",
      "epoch [110/500], loss:0.7943\n",
      "epoch [111/500], loss:0.8457\n",
      "epoch [112/500], loss:0.8127\n",
      "epoch [113/500], loss:0.7733\n",
      "epoch [114/500], loss:0.8297\n",
      "epoch [115/500], loss:0.9101\n",
      "epoch [116/500], loss:0.8239\n",
      "epoch [117/500], loss:0.7759\n",
      "epoch [118/500], loss:0.7458\n",
      "epoch [119/500], loss:0.7883\n",
      "epoch [120/500], loss:0.7672\n",
      "epoch [121/500], loss:0.7753\n",
      "epoch [122/500], loss:0.8033\n",
      "epoch [123/500], loss:0.8237\n",
      "epoch [124/500], loss:0.7688\n",
      "epoch [125/500], loss:0.7377\n",
      "epoch [126/500], loss:0.7721\n",
      "epoch [127/500], loss:0.8273\n",
      "epoch [128/500], loss:0.8403\n",
      "epoch [129/500], loss:0.7745\n",
      "epoch [130/500], loss:0.7297\n",
      "epoch [131/500], loss:0.7834\n",
      "epoch [132/500], loss:0.8284\n",
      "epoch [133/500], loss:0.7901\n",
      "epoch [134/500], loss:0.8803\n",
      "epoch [135/500], loss:0.9333\n",
      "epoch [136/500], loss:0.7543\n",
      "epoch [137/500], loss:0.7178\n",
      "epoch [138/500], loss:0.8759\n",
      "epoch [139/500], loss:0.8977\n",
      "epoch [140/500], loss:0.8921\n",
      "epoch [141/500], loss:0.8656\n",
      "epoch [142/500], loss:0.8738\n",
      "epoch [143/500], loss:0.7927\n",
      "epoch [144/500], loss:0.7878\n",
      "epoch [145/500], loss:0.8151\n",
      "epoch [146/500], loss:0.8146\n",
      "epoch [147/500], loss:0.7938\n",
      "epoch [148/500], loss:0.7733\n",
      "epoch [149/500], loss:0.7553\n",
      "epoch [150/500], loss:0.8085\n",
      "epoch [151/500], loss:0.8367\n",
      "epoch [152/500], loss:0.7875\n",
      "epoch [153/500], loss:0.8181\n",
      "epoch [154/500], loss:0.8361\n",
      "epoch [155/500], loss:0.8053\n",
      "epoch [156/500], loss:0.7682\n",
      "epoch [157/500], loss:0.7850\n",
      "epoch [158/500], loss:0.7738\n",
      "epoch [159/500], loss:0.9196\n",
      "epoch [160/500], loss:0.7260\n",
      "epoch [161/500], loss:0.8669\n",
      "epoch [162/500], loss:0.8429\n",
      "epoch [163/500], loss:0.7563\n",
      "epoch [164/500], loss:0.7796\n",
      "epoch [165/500], loss:0.7640\n",
      "epoch [166/500], loss:0.7506\n",
      "epoch [167/500], loss:0.8641\n",
      "epoch [168/500], loss:0.8849\n",
      "epoch [169/500], loss:0.7982\n",
      "epoch [170/500], loss:0.7494\n",
      "epoch [171/500], loss:0.7565\n",
      "epoch [172/500], loss:0.8423\n",
      "epoch [173/500], loss:0.9117\n",
      "epoch [174/500], loss:0.8304\n",
      "epoch [175/500], loss:0.8153\n",
      "epoch [176/500], loss:0.9017\n",
      "epoch [177/500], loss:0.9369\n",
      "epoch [178/500], loss:0.7926\n",
      "epoch [179/500], loss:0.7870\n",
      "epoch [180/500], loss:0.9677\n",
      "epoch [181/500], loss:0.7458\n",
      "epoch [182/500], loss:0.8482\n",
      "epoch [183/500], loss:0.7101\n",
      "epoch [184/500], loss:0.8880\n",
      "epoch [185/500], loss:0.7919\n",
      "epoch [186/500], loss:0.8363\n",
      "epoch [187/500], loss:0.7175\n",
      "epoch [188/500], loss:0.8350\n",
      "epoch [189/500], loss:0.8571\n",
      "epoch [190/500], loss:0.7264\n",
      "epoch [191/500], loss:0.7777\n",
      "epoch [192/500], loss:0.9464\n",
      "epoch [193/500], loss:0.7846\n",
      "epoch [194/500], loss:0.9080\n",
      "epoch [195/500], loss:0.8241\n",
      "epoch [196/500], loss:0.8324\n",
      "epoch [197/500], loss:0.9216\n",
      "epoch [198/500], loss:0.7677\n",
      "epoch [199/500], loss:0.7882\n",
      "epoch [200/500], loss:0.7509\n",
      "epoch [201/500], loss:0.7782\n",
      "epoch [202/500], loss:0.8105\n",
      "epoch [203/500], loss:0.9630\n",
      "epoch [204/500], loss:0.7801\n",
      "epoch [205/500], loss:0.8742\n",
      "epoch [206/500], loss:0.8377\n",
      "epoch [207/500], loss:0.8146\n",
      "epoch [208/500], loss:0.9525\n",
      "epoch [209/500], loss:0.8201\n",
      "epoch [210/500], loss:0.7491\n",
      "epoch [211/500], loss:0.7207\n",
      "epoch [212/500], loss:0.7948\n",
      "epoch [213/500], loss:0.8756\n",
      "epoch [214/500], loss:0.7974\n",
      "epoch [215/500], loss:0.7715\n",
      "epoch [216/500], loss:0.7145\n",
      "epoch [217/500], loss:0.7483\n",
      "epoch [218/500], loss:0.8901\n",
      "epoch [219/500], loss:0.8473\n",
      "epoch [220/500], loss:0.7373\n",
      "epoch [221/500], loss:0.7668\n",
      "epoch [222/500], loss:0.9033\n",
      "epoch [223/500], loss:0.7641\n",
      "epoch [224/500], loss:0.8894\n",
      "epoch [225/500], loss:0.7391\n",
      "epoch [226/500], loss:0.7723\n",
      "epoch [227/500], loss:0.8209\n",
      "epoch [228/500], loss:0.9393\n",
      "epoch [229/500], loss:0.7836\n",
      "epoch [230/500], loss:0.7360\n",
      "epoch [231/500], loss:0.8231\n",
      "epoch [232/500], loss:0.9015\n",
      "epoch [233/500], loss:0.8240\n",
      "epoch [234/500], loss:0.8505\n",
      "epoch [235/500], loss:0.9742\n",
      "epoch [236/500], loss:0.8228\n",
      "epoch [237/500], loss:0.8509\n",
      "epoch [238/500], loss:0.7882\n",
      "epoch [239/500], loss:0.8197\n",
      "epoch [240/500], loss:0.7953\n",
      "epoch [241/500], loss:0.7577\n",
      "epoch [242/500], loss:0.8686\n",
      "epoch [243/500], loss:0.7944\n",
      "epoch [244/500], loss:0.8735\n",
      "epoch [245/500], loss:0.8860\n",
      "epoch [246/500], loss:0.9681\n",
      "epoch [247/500], loss:0.9559\n",
      "epoch [248/500], loss:0.9293\n",
      "epoch [249/500], loss:0.7848\n",
      "epoch [250/500], loss:0.7068\n",
      "epoch [251/500], loss:0.9322\n",
      "epoch [252/500], loss:0.8102\n",
      "epoch [253/500], loss:0.6784\n",
      "epoch [254/500], loss:0.7986\n",
      "epoch [255/500], loss:0.8858\n",
      "epoch [256/500], loss:0.8253\n",
      "epoch [257/500], loss:0.7187\n",
      "epoch [258/500], loss:0.7902\n",
      "epoch [259/500], loss:1.1192\n",
      "epoch [260/500], loss:0.7580\n",
      "epoch [261/500], loss:0.7392\n",
      "epoch [262/500], loss:0.8517\n",
      "epoch [263/500], loss:0.8303\n",
      "epoch [264/500], loss:0.7771\n",
      "epoch [265/500], loss:0.8042\n",
      "epoch [266/500], loss:0.9396\n",
      "epoch [267/500], loss:0.9184\n",
      "epoch [268/500], loss:0.7812\n",
      "epoch [269/500], loss:0.7637\n",
      "epoch [270/500], loss:0.9164\n",
      "epoch [271/500], loss:0.8186\n",
      "epoch [272/500], loss:0.8269\n",
      "epoch [273/500], loss:0.8562\n",
      "epoch [274/500], loss:0.8666\n",
      "epoch [275/500], loss:0.7906\n",
      "epoch [276/500], loss:0.7229\n",
      "epoch [277/500], loss:0.7532\n",
      "epoch [278/500], loss:0.8533\n",
      "epoch [279/500], loss:0.8290\n",
      "epoch [280/500], loss:0.7845\n",
      "epoch [281/500], loss:0.7801\n",
      "epoch [282/500], loss:0.8407\n",
      "epoch [283/500], loss:0.7575\n",
      "epoch [284/500], loss:0.7435\n",
      "epoch [285/500], loss:0.7765\n",
      "epoch [286/500], loss:0.7124\n",
      "epoch [287/500], loss:0.9842\n",
      "epoch [288/500], loss:0.7120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [289/500], loss:0.7843\n",
      "epoch [290/500], loss:0.8147\n",
      "epoch [291/500], loss:0.7521\n",
      "epoch [292/500], loss:0.8340\n",
      "epoch [293/500], loss:0.8372\n",
      "epoch [294/500], loss:0.7579\n",
      "epoch [295/500], loss:0.7880\n",
      "epoch [296/500], loss:0.7863\n",
      "epoch [297/500], loss:0.8724\n",
      "epoch [298/500], loss:0.7893\n",
      "epoch [299/500], loss:0.7415\n",
      "epoch [300/500], loss:1.0133\n",
      "epoch [301/500], loss:0.8886\n",
      "epoch [302/500], loss:0.7823\n",
      "epoch [303/500], loss:0.7203\n",
      "epoch [304/500], loss:0.8413\n",
      "epoch [305/500], loss:0.7969\n",
      "epoch [306/500], loss:0.7811\n",
      "epoch [307/500], loss:0.7865\n",
      "epoch [308/500], loss:0.7281\n",
      "epoch [309/500], loss:0.8404\n",
      "epoch [310/500], loss:0.7482\n",
      "epoch [311/500], loss:0.7880\n",
      "epoch [312/500], loss:0.8008\n",
      "epoch [313/500], loss:0.7599\n",
      "epoch [314/500], loss:0.7360\n",
      "epoch [315/500], loss:0.9037\n",
      "epoch [316/500], loss:0.7090\n",
      "epoch [317/500], loss:0.8999\n",
      "epoch [318/500], loss:0.8781\n",
      "epoch [319/500], loss:0.8292\n",
      "epoch [320/500], loss:0.8205\n",
      "epoch [321/500], loss:0.9139\n",
      "epoch [322/500], loss:0.8569\n",
      "epoch [323/500], loss:0.8310\n",
      "epoch [324/500], loss:0.8750\n",
      "epoch [325/500], loss:0.8171\n",
      "epoch [326/500], loss:0.8798\n",
      "epoch [327/500], loss:0.7614\n",
      "epoch [328/500], loss:0.7591\n",
      "epoch [329/500], loss:0.8844\n",
      "epoch [330/500], loss:0.8331\n",
      "epoch [331/500], loss:0.8477\n",
      "epoch [332/500], loss:0.7747\n",
      "epoch [333/500], loss:0.8195\n",
      "epoch [334/500], loss:0.8939\n",
      "epoch [335/500], loss:0.8824\n",
      "epoch [336/500], loss:0.8805\n",
      "epoch [337/500], loss:0.7480\n",
      "epoch [338/500], loss:0.8194\n",
      "epoch [339/500], loss:1.0116\n",
      "epoch [340/500], loss:0.8019\n",
      "epoch [341/500], loss:0.8696\n",
      "epoch [342/500], loss:0.9008\n",
      "epoch [343/500], loss:0.8809\n",
      "epoch [344/500], loss:0.8194\n",
      "epoch [345/500], loss:0.8420\n",
      "epoch [346/500], loss:0.8813\n",
      "epoch [347/500], loss:0.7880\n",
      "epoch [348/500], loss:0.7737\n",
      "epoch [349/500], loss:0.7343\n",
      "epoch [350/500], loss:0.8598\n",
      "epoch [351/500], loss:0.7999\n",
      "epoch [352/500], loss:0.8314\n",
      "epoch [353/500], loss:0.7645\n",
      "epoch [354/500], loss:0.8062\n",
      "epoch [355/500], loss:0.8191\n",
      "epoch [356/500], loss:0.7848\n",
      "epoch [357/500], loss:0.7312\n",
      "epoch [358/500], loss:0.7040\n",
      "epoch [359/500], loss:0.8434\n",
      "epoch [360/500], loss:0.7362\n",
      "epoch [361/500], loss:0.7907\n",
      "epoch [362/500], loss:0.7912\n",
      "epoch [363/500], loss:0.8109\n",
      "epoch [364/500], loss:0.7198\n",
      "epoch [365/500], loss:0.7643\n",
      "epoch [366/500], loss:0.9002\n",
      "epoch [367/500], loss:0.9225\n",
      "epoch [368/500], loss:0.8297\n",
      "epoch [369/500], loss:0.7684\n",
      "epoch [370/500], loss:0.8641\n",
      "epoch [371/500], loss:0.7695\n",
      "epoch [372/500], loss:0.8870\n",
      "epoch [373/500], loss:0.7740\n",
      "epoch [374/500], loss:0.9028\n",
      "epoch [375/500], loss:0.7514\n",
      "epoch [376/500], loss:0.8343\n",
      "epoch [377/500], loss:0.7104\n",
      "epoch [378/500], loss:0.7620\n",
      "epoch [379/500], loss:0.7819\n",
      "epoch [380/500], loss:1.0632\n",
      "epoch [381/500], loss:0.8139\n",
      "epoch [382/500], loss:0.8429\n",
      "epoch [383/500], loss:0.8348\n",
      "epoch [384/500], loss:0.9036\n",
      "epoch [385/500], loss:0.8014\n",
      "epoch [386/500], loss:0.8222\n",
      "epoch [387/500], loss:0.7897\n",
      "epoch [388/500], loss:0.7617\n",
      "epoch [389/500], loss:0.7485\n",
      "epoch [390/500], loss:0.7418\n",
      "epoch [391/500], loss:0.7723\n",
      "epoch [392/500], loss:0.8782\n",
      "epoch [393/500], loss:0.8797\n",
      "epoch [394/500], loss:0.7153\n",
      "epoch [395/500], loss:0.9308\n",
      "epoch [396/500], loss:0.7517\n",
      "epoch [397/500], loss:0.8368\n",
      "epoch [398/500], loss:0.7866\n",
      "epoch [399/500], loss:0.7865\n",
      "epoch [400/500], loss:0.7890\n",
      "epoch [401/500], loss:0.7857\n",
      "epoch [402/500], loss:0.7175\n",
      "epoch [403/500], loss:0.8762\n",
      "epoch [404/500], loss:0.7596\n",
      "epoch [405/500], loss:0.9027\n",
      "epoch [406/500], loss:0.9073\n",
      "epoch [407/500], loss:0.7968\n",
      "epoch [408/500], loss:0.7642\n",
      "epoch [409/500], loss:0.8626\n",
      "epoch [410/500], loss:0.7208\n",
      "epoch [411/500], loss:0.8444\n",
      "epoch [412/500], loss:0.7362\n",
      "epoch [413/500], loss:0.7336\n",
      "epoch [414/500], loss:0.8623\n",
      "epoch [415/500], loss:0.8268\n",
      "epoch [416/500], loss:0.9172\n",
      "epoch [417/500], loss:0.8731\n",
      "epoch [418/500], loss:0.6923\n",
      "epoch [419/500], loss:0.7245\n",
      "epoch [420/500], loss:0.7640\n",
      "epoch [421/500], loss:0.9241\n",
      "epoch [422/500], loss:0.8694\n",
      "epoch [423/500], loss:0.8888\n",
      "epoch [424/500], loss:0.8319\n",
      "epoch [425/500], loss:0.7690\n",
      "epoch [426/500], loss:0.7982\n",
      "epoch [427/500], loss:0.8057\n",
      "epoch [428/500], loss:0.8152\n",
      "epoch [429/500], loss:0.7389\n",
      "epoch [430/500], loss:0.8185\n",
      "epoch [431/500], loss:0.8283\n",
      "epoch [432/500], loss:0.9877\n",
      "epoch [433/500], loss:0.8351\n",
      "epoch [434/500], loss:1.0129\n",
      "epoch [435/500], loss:1.0838\n",
      "epoch [436/500], loss:0.7860\n",
      "epoch [437/500], loss:0.7088\n",
      "epoch [438/500], loss:0.9369\n",
      "epoch [439/500], loss:0.7774\n",
      "epoch [440/500], loss:0.7348\n",
      "epoch [441/500], loss:0.8603\n",
      "epoch [442/500], loss:0.7666\n",
      "epoch [443/500], loss:0.8422\n",
      "epoch [444/500], loss:0.7999\n",
      "epoch [445/500], loss:0.8737\n",
      "epoch [446/500], loss:0.7757\n",
      "epoch [447/500], loss:0.8891\n",
      "epoch [448/500], loss:0.7442\n",
      "epoch [449/500], loss:0.8169\n",
      "epoch [450/500], loss:0.9218\n",
      "epoch [451/500], loss:0.7183\n",
      "epoch [452/500], loss:0.8759\n",
      "epoch [453/500], loss:0.9035\n",
      "epoch [454/500], loss:0.8349\n",
      "epoch [455/500], loss:0.8083\n",
      "epoch [456/500], loss:0.9625\n",
      "epoch [457/500], loss:0.7325\n",
      "epoch [458/500], loss:0.7397\n",
      "epoch [459/500], loss:0.7745\n",
      "epoch [460/500], loss:0.9005\n",
      "epoch [461/500], loss:0.7894\n",
      "epoch [462/500], loss:0.7555\n",
      "epoch [463/500], loss:0.8728\n",
      "epoch [464/500], loss:0.7660\n",
      "epoch [465/500], loss:0.7822\n",
      "epoch [466/500], loss:0.7724\n",
      "epoch [467/500], loss:0.8665\n",
      "epoch [468/500], loss:0.7792\n",
      "epoch [469/500], loss:0.9030\n",
      "epoch [470/500], loss:0.9226\n",
      "epoch [471/500], loss:0.9178\n",
      "epoch [472/500], loss:0.8312\n",
      "epoch [473/500], loss:0.7695\n",
      "epoch [474/500], loss:0.7331\n",
      "epoch [475/500], loss:0.9843\n",
      "epoch [476/500], loss:0.8012\n",
      "epoch [477/500], loss:0.8334\n",
      "epoch [478/500], loss:0.7475\n",
      "epoch [479/500], loss:0.7947\n",
      "epoch [480/500], loss:0.7918\n",
      "epoch [481/500], loss:0.8555\n",
      "epoch [482/500], loss:0.8116\n",
      "epoch [483/500], loss:0.8382\n",
      "epoch [484/500], loss:0.8461\n",
      "epoch [485/500], loss:0.8835\n",
      "epoch [486/500], loss:0.7618\n",
      "epoch [487/500], loss:0.8325\n",
      "epoch [488/500], loss:0.8227\n",
      "epoch [489/500], loss:0.7584\n",
      "epoch [490/500], loss:0.7564\n",
      "epoch [491/500], loss:0.7475\n",
      "epoch [492/500], loss:0.8141\n",
      "epoch [493/500], loss:0.8162\n",
      "epoch [494/500], loss:0.8436\n",
      "epoch [495/500], loss:0.8725\n",
      "epoch [496/500], loss:0.8839\n",
      "epoch [497/500], loss:0.7437\n",
      "epoch [498/500], loss:0.9132\n",
      "epoch [499/500], loss:0.7834\n",
      "epoch [500/500], loss:0.8126\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPOCHS):\n",
    "    for b in range(0,X_train.shape[0],batch_size):\n",
    "        batch_data = X_train[b:b+batch_size,6:6+3*256]\n",
    "        batch_label = y_train[b:b+batch_size]\n",
    "        output = mlp(torch.Tensor(batch_data).cuda())\n",
    "        batch_label = torch.LongTensor(batch_label).cuda()\n",
    "        loss = loss_fn(output,batch_label)\n",
    "        l2_norm = torch.norm(mlp.layers[-1].weight, p=2)\n",
    "        loss += l2_norm*0.1\n",
    "        l2_norm = torch.norm(mlp.layers[0].weight, p=2)\n",
    "        loss += l2_norm*0.05\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(e + 1, EPOCHS, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mlp = torch.nn.Softmax(dim = 1)(mlp(torch.Tensor(X_test[:,6:6+3*256]).cuda())).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.395625</td>\n",
       "      <td>0.146977</td>\n",
       "      <td>0.457398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.998218</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.001116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.036819</td>\n",
       "      <td>0.917897</td>\n",
       "      <td>0.045284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.057491</td>\n",
       "      <td>0.214837</td>\n",
       "      <td>0.727672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.977308</td>\n",
       "      <td>0.022623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID         A         B   NEITHER\n",
       "0  development-1  0.395625  0.146977  0.457398\n",
       "1  development-2  0.998218  0.000666  0.001116\n",
       "2  development-3  0.036819  0.917897  0.045284\n",
       "3  development-4  0.057491  0.214837  0.727672\n",
       "4  development-5  0.000069  0.977308  0.022623"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.read_csv(\"./test_and_submit/sample_submission_stage_1.csv\")\n",
    "sub_df.loc[:, ['A','B','NEITHER']] = pred_mlp\n",
    "sub_df.to_csv(\"./test_and_submit/submission+model+mlp@\"+str(datetime.datetime.now())+\".csv\", index=False)\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7498553991317749"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(sub_df,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
