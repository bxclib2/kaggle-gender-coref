{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train= pd.read_pickle('./temp_result/train_kaggle_processed_PPA_PCA_PPA_neither')\n",
    "gap_test= pd.read_pickle('./temp_result/test_kaggle_processed_PPA_PCA_PPA_neither')\n",
    "gap_valid= pd.read_pickle('./temp_result/valid_kaggle_processed_PPA_PCA_PPA_neither')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = gap_train.count().values[0]\n",
    "NUM_TEST = gap_test.count().values[0]\n",
    "NUM_VALID = gap_valid.count().values[0]\n",
    "def label(A,B):\n",
    "    if A is True:\n",
    "        return 0\n",
    "    if B is True:\n",
    "        return 1\n",
    "    return 2\n",
    "def switch_label(l):\n",
    "    if l==2:\n",
    "        return 2\n",
    "    return 1-l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prediction(pred):\n",
    "    s = pred.shape[0]//2\n",
    "    pred0 = pred[0:s,:]\n",
    "    pred1 = pred[s:,:]\n",
    "    pred1 = pred1[:,[1,0,2]]\n",
    "    pred_out = pred0+pred1\n",
    "    return pred_out/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_A_B(df):\n",
    "    columnsTitles = [\"B_dist\",\"A_dist\",\"B_pos\", \"A_pos\",\"pron_pos\", \"B_vector\", \"A_vector\",\"pron_vector\",\"product_vector_B\",\"product_vector_A\",\"label\"]\n",
    "    df2=df.reindex(columns=columnsTitles).copy()\n",
    "    df2.columns = df.columns\n",
    "    df2.label = df2.label.map(switch_label)\n",
    "    return pd.concat([df,df2],axis = 0, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compute_loss(sub_df,test_data):\n",
    "    pred = torch.Tensor(np.log(sub_df.loc[:,['A','B','NEITHER']].values))\n",
    "    label = torch.LongTensor(list(test_data.label))\n",
    "    loss = torch.nn.NLLLoss()\n",
    "    loss_value = loss(pred,label).item()\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gap_train.drop(columns = ['ID', 'Text', 'Pronoun','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map', 'pron_idx',\"name_list\"])\n",
    "train_data.A_vector = train_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data.B_vector = train_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data.pron_vector = train_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data[\"product_vector_A\"] = train_data.A_vector*train_data.pron_vector\n",
    "train_data[\"product_vector_B\"] = train_data.B_vector*train_data.pron_vector\n",
    "train_data[\"label\"] = train_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "train_data = train_data.drop(columns= [\"A-coref\",\"B-coref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = gap_test.drop(columns = ['ID', 'Text', 'Pronoun','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map', 'pron_idx',\"name_list\"])\n",
    "test_data.A_vector = test_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data.B_vector = test_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data.pron_vector = test_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data[\"product_vector_A\"] = test_data.A_vector*test_data.pron_vector\n",
    "test_data[\"product_vector_B\"] = test_data.B_vector*test_data.pron_vector\n",
    "test_data[\"label\"] = test_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "test_data = test_data.drop(columns= [\"A-coref\",\"B-coref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = gap_valid.drop(columns = ['ID', 'Text', 'Pronoun','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map', 'pron_idx',\"name_list\"])\n",
    "valid_data.A_vector = valid_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data.B_vector = valid_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data.pron_vector = valid_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data[\"product_vector_A\"] = valid_data.A_vector*valid_data.pron_vector\n",
    "valid_data[\"product_vector_B\"] = valid_data.B_vector*valid_data.pron_vector\n",
    "valid_data[\"label\"] = valid_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "valid_data = valid_data.drop(columns= [\"A-coref\",\"B-coref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def masked_softmax(logits, mask, dim=-1, log_softmax=False):\n",
    "    \"\"\"Take the softmax of `logits` over given dimension, and set\n",
    "    entries to 0 wherever `mask` is 0.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Inputs to the softmax function.\n",
    "        mask (torch.Tensor): Same shape as `logits`, with 0 indicating\n",
    "            positions that should be assigned 0 probability in the output.\n",
    "        dim (int): Dimension over which to take softmax.\n",
    "        log_softmax (bool): Take log-softmax rather than regular softmax.\n",
    "            E.g., some PyTorch functions such as `F.nll_loss` expect log-softmax.\n",
    "\n",
    "    Returns:\n",
    "        probs (torch.Tensor): Result of taking masked softmax over the logits.\n",
    "    \"\"\"\n",
    "    mask = mask.type(torch.float32)\n",
    "    masked_logits = mask * logits + (1 - mask) * -1e30\n",
    "    softmax_fn = F.log_softmax if log_softmax else F.softmax\n",
    "    probs = softmax_fn(masked_logits, dim)\n",
    "\n",
    "    return probs\n",
    "class BiDAFAttention(nn.Module):\n",
    "    \"\"\"Bidirectional attention originally used by BiDAF.\n",
    "\n",
    "    Bidirectional attention computes attention in two directions:\n",
    "    The context attends to the query and the query attends to the context.\n",
    "    The output of this layer is the concatenation of [context, c2q_attention,\n",
    "    context * c2q_attention, context * q2c_attention]. This concatenation allows\n",
    "    the attention vector at each timestep, along with the embeddings from\n",
    "    previous layers, to flow through the attention layer to the modeling layer.\n",
    "    The output has shape (batch_size, context_len, 8 * hidden_size).\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): Size of hidden activations.\n",
    "        drop_prob (float): Probability of zero-ing out activations.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, drop_prob=0.60):\n",
    "        super(BiDAFAttention, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.c_weight = nn.Parameter(torch.zeros(hidden_size, 1))\n",
    "        self.q_weight = nn.Parameter(torch.zeros(hidden_size, 1))\n",
    "        self.cq_weight = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "        for weight in (self.c_weight, self.q_weight, self.cq_weight):\n",
    "            nn.init.xavier_uniform_(weight)\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        self.drop1 = nn.Dropout(self.drop_prob)  # (bs, c_len, hid_size)\n",
    "        self.drop2 = nn.Dropout(self.drop_prob)\n",
    "        self.output_layer1 = nn.Linear(4*hidden_size,64)\n",
    "        self.drop3 = nn.Dropout(self.drop_prob)\n",
    "        self.output_layer2 = nn.Linear(64,4)\n",
    "        self.drop4 = nn.Dropout(self.drop_prob)\n",
    "        self.output_layer3 = nn.Linear(4,1)\n",
    "        self.drop5 = nn.Dropout(self.drop_prob)\n",
    "        nn.init.xavier_uniform_(self.output_layer1.weight)\n",
    "        nn.init.xavier_uniform_(self.output_layer2.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, c, q, c_mask, q_mask):\n",
    "        batch_size, c_len, _ = c.size()\n",
    "        q_len = q.size(1)\n",
    "        s = self.get_similarity_matrix(c, q)        # (batch_size, c_len, q_len)\n",
    "        c_mask = c_mask.view(batch_size, c_len, 1)  # (batch_size, c_len, 1)\n",
    "        q_mask = q_mask.view(batch_size, 1, q_len)  # (batch_size, 1, q_len)\n",
    "        s1 = masked_softmax(s, q_mask, dim=2)       # (batch_size, c_len, q_len)\n",
    "        s2 = masked_softmax(s, c_mask, dim=1)       # (batch_size, c_len, q_len)\n",
    "\n",
    "        # (bs, c_len, q_len) x (bs, q_len, hid_size) => (bs, c_len, hid_size)\n",
    "        a = torch.bmm(s1, q)\n",
    "        # (bs, c_len, c_len) x (bs, c_len, hid_size) => (bs, c_len, hid_size)\n",
    "        b = torch.bmm(torch.bmm(s1, s2.transpose(1, 2)), c)\n",
    "\n",
    "        \n",
    "        x = torch.cat([c, a, c * a, c * b], dim=2)  # (bs, c_len, 4 * hid_size)\n",
    "        \n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        x = self.output_layer1(x)\n",
    "        \n",
    "        x = torch.nn.ELU()(x)\n",
    "        \n",
    "        x = self.drop4(x)\n",
    "        \n",
    "        x = self.output_layer2(x)\n",
    "        \n",
    "        x = self.drop5(x)\n",
    "        \n",
    "        x = self.output_layer3(x)\n",
    "        \n",
    "        return x.squeeze(-1)\n",
    "\n",
    "    def get_similarity_matrix(self, c, q):\n",
    "        \"\"\"Get the \"similarity matrix\" between context and query (using the\n",
    "        terminology of the BiDAF paper).\n",
    "\n",
    "        A naive implementation as described in BiDAF would concatenate the\n",
    "        three vectors then project the result with a single weight matrix. This\n",
    "        method is a more memory-efficient implementation of the same operation.\n",
    "\n",
    "        See Also:\n",
    "            Equation 1 in https://arxiv.org/abs/1611.01603\n",
    "        \"\"\"\n",
    "        c_len, q_len = c.size(1), q.size(1)\n",
    "        c = self.drop1(c)  # (bs, c_len, hid_size)\n",
    "        q = self.drop2(q)  # (bs, q_len, hid_size)\n",
    "        #print (c.size())\n",
    "        #print (q.size())\n",
    "        # Shapes: (batch_size, c_len, q_len)\n",
    "        s0 = torch.matmul(c, self.c_weight).expand([-1, -1, q_len])\n",
    "        s1 = torch.matmul(q, self.q_weight).transpose(1, 2)\\\n",
    "                                           .expand([-1, c_len, -1])\n",
    "        s2 = torch.matmul(c * self.cq_weight, q.transpose(1, 2))\n",
    "        s = s0 + s1 + s2 + self.bias\n",
    "\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidaf = BiDAFAttention(256).cuda()\n",
    "EPOCHS = 150\n",
    "batch_size = 25\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight = torch.Tensor([1.0,1.0,1.0])).cuda()\n",
    "opt = torch.optim.Adam(bidaf.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector</th>\n",
       "      <th>A_dist</th>\n",
       "      <th>B_dist</th>\n",
       "      <th>A_pos</th>\n",
       "      <th>B_pos</th>\n",
       "      <th>pron_pos</th>\n",
       "      <th>A_idx</th>\n",
       "      <th>B_idx</th>\n",
       "      <th>A_vector</th>\n",
       "      <th>B_vector</th>\n",
       "      <th>pron_vector</th>\n",
       "      <th>name_list_2</th>\n",
       "      <th>neither_idx</th>\n",
       "      <th>neither_idx_2</th>\n",
       "      <th>neither_vector</th>\n",
       "      <th>neither_vector_2</th>\n",
       "      <th>product_vector_A</th>\n",
       "      <th>product_vector_B</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.85816926, -0.45762736, -0.08299036, -0.275...</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.693548</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>[52, 53, 54, 55, 56, 57, 58, 59]</td>\n",
       "      <td>[63, 64, 65]</td>\n",
       "      <td>[0.20037952, 0.2805402, -0.11013528, -0.492183...</td>\n",
       "      <td>[0.6041415, -0.13538514, 0.15207358, -0.076126...</td>\n",
       "      <td>[0.8586822, -1.2192798, 0.09194927, -0.4327072...</td>\n",
       "      <td>[(III, 182)]</td>\n",
       "      <td>[39, 40, 41, 42, 43, 44, 45, 46]</td>\n",
       "      <td>[45, 46]</td>\n",
       "      <td>[[0.611566, -0.3358844, 0.46167314, -0.0807262...</td>\n",
       "      <td>[[-0.5177313, 0.72737604, -0.21791872, -1.0356...</td>\n",
       "      <td>[0.17206234, -0.342057, -0.010126858, 0.212971...</td>\n",
       "      <td>[0.51876557, 0.16507237, 0.013983054, 0.032940...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.2511816, 0.24685939, -0.3399855, -0.624934...</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>[34, 35]</td>\n",
       "      <td>[46, 47, 48]</td>\n",
       "      <td>[0.067682624, 0.42009318, -0.009991955, -0.659...</td>\n",
       "      <td>[-0.438681, 0.48596978, -0.02240046, 0.0839553...</td>\n",
       "      <td>[-0.67510414, -0.39920956, 0.07794422, 0.43378...</td>\n",
       "      <td>[(Nott, 9), (Philip, 58)]</td>\n",
       "      <td>[1, 2, 3, 4, 5, 19, 20, 21]</td>\n",
       "      <td>[4, 5, 19, 20, 21]</td>\n",
       "      <td>[[0.036648497, 0.8636709, -0.18831435, -0.6663...</td>\n",
       "      <td>[[-0.18851164, 0.22483511, -0.3515694, -0.3107...</td>\n",
       "      <td>[-0.04569282, -0.16770521, -0.0007788151, -0.2...</td>\n",
       "      <td>[0.29615536, -0.19400378, -0.0017459863, 0.036...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.8131293, 0.4440992, 0.7549018, 0.4792695, ...</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.405128</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>[89, 90, 91, 92, 93, 94, 95]</td>\n",
       "      <td>[99, 100]</td>\n",
       "      <td>[-0.40055174, -0.06318157, 0.08668772, -0.0661...</td>\n",
       "      <td>[0.67819536, 0.21544528, -0.39938855, 0.618967...</td>\n",
       "      <td>[0.72358626, -1.0747175, 0.050702423, -0.89089...</td>\n",
       "      <td>[(Tony, 52), (Todd, 57), (Angela, 179), (Angel...</td>\n",
       "      <td>[15, 16, 17, 18, 44, 45, 73, 74, 86, 87, 88, 9...</td>\n",
       "      <td>[15, 16, 17, 18, 44, 45, 73, 74, 86, 87, 88, 9...</td>\n",
       "      <td>[[0.5654289, 0.7731928, 0.3528939, 0.48137867,...</td>\n",
       "      <td>[[0.5654289, 0.7731928, 0.3528939, 0.48137867,...</td>\n",
       "      <td>[-0.28983372, 0.06790234, 0.0043952777, 0.0588...</td>\n",
       "      <td>[0.49073285, -0.23154281, -0.020249967, -0.551...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[1.033058, -0.108201064, -0.1557167, 0.394294...</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.201521</td>\n",
       "      <td>0.216730</td>\n",
       "      <td>0.224335</td>\n",
       "      <td>[66, 67, 68]</td>\n",
       "      <td>[73, 74, 75]</td>\n",
       "      <td>[-0.33478984, -0.63129133, -0.5514479, 0.59391...</td>\n",
       "      <td>[0.50680375, 0.71950006, -0.25772676, 0.375783...</td>\n",
       "      <td>[1.2850071, 0.049930945, -0.26918668, 0.253453...</td>\n",
       "      <td>[(Peter, 114), (Peter, 359)]</td>\n",
       "      <td>[23, 24, 25, 26, 27, 31, 32, 33, 47, 48, 49, 8...</td>\n",
       "      <td>[23, 24, 88, 89]</td>\n",
       "      <td>[[0.5191963, 1.1370064, 0.015690625, -0.098785...</td>\n",
       "      <td>[[0.5191963, 1.1370064, 0.015690625, -0.098785...</td>\n",
       "      <td>[-0.43020734, -0.031520974, 0.14844243, 0.1505...</td>\n",
       "      <td>[0.6512464, 0.035925318, 0.06937661, 0.0952435...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[1.3460386, 0.037629873, 0.55879486, -0.42726...</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.321839</td>\n",
       "      <td>0.362069</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>[71, 72, 73, 74]</td>\n",
       "      <td>[80, 81, 82, 83]</td>\n",
       "      <td>[0.25996214, -0.15495634, -0.5617872, -0.04960...</td>\n",
       "      <td>[0.2925861, 0.2669702, 0.03382411, -0.3578499,...</td>\n",
       "      <td>[-0.87992877, -0.07835998, 0.12989467, -0.2304...</td>\n",
       "      <td>[(Karen, 14), (Blixen, 20), (Marshall, 198), (...</td>\n",
       "      <td>[5, 6, 7, 8, 9, 23, 24, 25, 26, 46, 47, 48, 49...</td>\n",
       "      <td>[5, 6, 7, 8, 9, 46, 47, 48, 49, 50, 52, 53, 64...</td>\n",
       "      <td>[[0.49518594, 0.5576514, 0.16942936, -0.599541...</td>\n",
       "      <td>[[0.49518594, 0.5576514, 0.16942936, -0.599541...</td>\n",
       "      <td>[-0.22874817, 0.012142375, -0.07297316, 0.0114...</td>\n",
       "      <td>[-0.2574549, -0.020919777, 0.0043935715, 0.082...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              vector  A_dist  B_dist  \\\n",
       "0  [[0.85816926, -0.45762736, -0.08299036, -0.275...   0.018   0.004   \n",
       "1  [[0.2511816, 0.24685939, -0.3399855, -0.624934...   0.030   0.012   \n",
       "2  [[0.8131293, 0.4440992, 0.7549018, 0.4792695, ...   0.022   0.010   \n",
       "3  [[1.033058, -0.108201064, -0.1557167, 0.394294...   0.012   0.004   \n",
       "4  [[1.3460386, 0.037629873, 0.55879486, -0.42726...   0.056   0.042   \n",
       "\n",
       "      A_pos     B_pos  pron_pos                             A_idx  \\\n",
       "0  0.693548  0.806452  0.838710  [52, 53, 54, 55, 56, 57, 58, 59]   \n",
       "1  0.511111  0.711111  0.844444                          [34, 35]   \n",
       "2  0.405128  0.435897  0.461538      [89, 90, 91, 92, 93, 94, 95]   \n",
       "3  0.201521  0.216730  0.224335                      [66, 67, 68]   \n",
       "4  0.321839  0.362069  0.482759                  [71, 72, 73, 74]   \n",
       "\n",
       "              B_idx                                           A_vector  \\\n",
       "0      [63, 64, 65]  [0.20037952, 0.2805402, -0.11013528, -0.492183...   \n",
       "1      [46, 47, 48]  [0.067682624, 0.42009318, -0.009991955, -0.659...   \n",
       "2         [99, 100]  [-0.40055174, -0.06318157, 0.08668772, -0.0661...   \n",
       "3      [73, 74, 75]  [-0.33478984, -0.63129133, -0.5514479, 0.59391...   \n",
       "4  [80, 81, 82, 83]  [0.25996214, -0.15495634, -0.5617872, -0.04960...   \n",
       "\n",
       "                                            B_vector  \\\n",
       "0  [0.6041415, -0.13538514, 0.15207358, -0.076126...   \n",
       "1  [-0.438681, 0.48596978, -0.02240046, 0.0839553...   \n",
       "2  [0.67819536, 0.21544528, -0.39938855, 0.618967...   \n",
       "3  [0.50680375, 0.71950006, -0.25772676, 0.375783...   \n",
       "4  [0.2925861, 0.2669702, 0.03382411, -0.3578499,...   \n",
       "\n",
       "                                         pron_vector  \\\n",
       "0  [0.8586822, -1.2192798, 0.09194927, -0.4327072...   \n",
       "1  [-0.67510414, -0.39920956, 0.07794422, 0.43378...   \n",
       "2  [0.72358626, -1.0747175, 0.050702423, -0.89089...   \n",
       "3  [1.2850071, 0.049930945, -0.26918668, 0.253453...   \n",
       "4  [-0.87992877, -0.07835998, 0.12989467, -0.2304...   \n",
       "\n",
       "                                         name_list_2  \\\n",
       "0                                       [(III, 182)]   \n",
       "1                          [(Nott, 9), (Philip, 58)]   \n",
       "2  [(Tony, 52), (Todd, 57), (Angela, 179), (Angel...   \n",
       "3                       [(Peter, 114), (Peter, 359)]   \n",
       "4  [(Karen, 14), (Blixen, 20), (Marshall, 198), (...   \n",
       "\n",
       "                                         neither_idx  \\\n",
       "0                   [39, 40, 41, 42, 43, 44, 45, 46]   \n",
       "1                        [1, 2, 3, 4, 5, 19, 20, 21]   \n",
       "2  [15, 16, 17, 18, 44, 45, 73, 74, 86, 87, 88, 9...   \n",
       "3  [23, 24, 25, 26, 27, 31, 32, 33, 47, 48, 49, 8...   \n",
       "4  [5, 6, 7, 8, 9, 23, 24, 25, 26, 46, 47, 48, 49...   \n",
       "\n",
       "                                       neither_idx_2  \\\n",
       "0                                           [45, 46]   \n",
       "1                                 [4, 5, 19, 20, 21]   \n",
       "2  [15, 16, 17, 18, 44, 45, 73, 74, 86, 87, 88, 9...   \n",
       "3                                   [23, 24, 88, 89]   \n",
       "4  [5, 6, 7, 8, 9, 46, 47, 48, 49, 50, 52, 53, 64...   \n",
       "\n",
       "                                      neither_vector  \\\n",
       "0  [[0.611566, -0.3358844, 0.46167314, -0.0807262...   \n",
       "1  [[0.036648497, 0.8636709, -0.18831435, -0.6663...   \n",
       "2  [[0.5654289, 0.7731928, 0.3528939, 0.48137867,...   \n",
       "3  [[0.5191963, 1.1370064, 0.015690625, -0.098785...   \n",
       "4  [[0.49518594, 0.5576514, 0.16942936, -0.599541...   \n",
       "\n",
       "                                    neither_vector_2  \\\n",
       "0  [[-0.5177313, 0.72737604, -0.21791872, -1.0356...   \n",
       "1  [[-0.18851164, 0.22483511, -0.3515694, -0.3107...   \n",
       "2  [[0.5654289, 0.7731928, 0.3528939, 0.48137867,...   \n",
       "3  [[0.5191963, 1.1370064, 0.015690625, -0.098785...   \n",
       "4  [[0.49518594, 0.5576514, 0.16942936, -0.599541...   \n",
       "\n",
       "                                    product_vector_A  \\\n",
       "0  [0.17206234, -0.342057, -0.010126858, 0.212971...   \n",
       "1  [-0.04569282, -0.16770521, -0.0007788151, -0.2...   \n",
       "2  [-0.28983372, 0.06790234, 0.0043952777, 0.0588...   \n",
       "3  [-0.43020734, -0.031520974, 0.14844243, 0.1505...   \n",
       "4  [-0.22874817, 0.012142375, -0.07297316, 0.0114...   \n",
       "\n",
       "                                    product_vector_B  label  \n",
       "0  [0.51876557, 0.16507237, 0.013983054, 0.032940...      2  \n",
       "1  [0.29615536, -0.19400378, -0.0017459863, 0.036...      1  \n",
       "2  [0.49073285, -0.23154281, -0.020249967, -0.551...      1  \n",
       "3  [0.6512464, 0.035925318, 0.06937661, 0.0952435...      0  \n",
       "4  [-0.2574549, -0.020919777, 0.0043935715, 0.082...      1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/150], loss:0.6273\n",
      "epoch [2/150], loss:0.7719\n",
      "epoch [3/150], loss:0.7552\n",
      "epoch [4/150], loss:0.6792\n",
      "epoch [5/150], loss:0.4874\n",
      "epoch [6/150], loss:0.4495\n",
      "epoch [7/150], loss:0.4851\n",
      "epoch [8/150], loss:0.3694\n",
      "epoch [9/150], loss:0.4416\n",
      "epoch [10/150], loss:0.4401\n",
      "epoch [11/150], loss:0.7345\n",
      "epoch [12/150], loss:0.5248\n",
      "epoch [13/150], loss:0.5240\n",
      "epoch [14/150], loss:0.6663\n",
      "epoch [15/150], loss:0.5406\n",
      "epoch [16/150], loss:0.3770\n",
      "epoch [17/150], loss:0.5707\n",
      "epoch [18/150], loss:0.5034\n",
      "epoch [19/150], loss:0.5113\n",
      "epoch [20/150], loss:0.6704\n",
      "epoch [21/150], loss:0.5815\n",
      "epoch [22/150], loss:0.4312\n",
      "epoch [23/150], loss:0.4875\n",
      "epoch [24/150], loss:0.5080\n",
      "epoch [25/150], loss:0.4088\n",
      "epoch [26/150], loss:0.5779\n",
      "epoch [27/150], loss:0.5926\n",
      "epoch [28/150], loss:0.3611\n",
      "epoch [29/150], loss:0.6392\n",
      "epoch [30/150], loss:0.4640\n",
      "epoch [31/150], loss:0.4794\n",
      "epoch [32/150], loss:0.4954\n",
      "epoch [33/150], loss:0.3912\n",
      "epoch [34/150], loss:0.4188\n",
      "epoch [35/150], loss:0.4153\n",
      "epoch [36/150], loss:0.4470\n",
      "epoch [37/150], loss:0.4436\n",
      "epoch [38/150], loss:0.5733\n",
      "epoch [39/150], loss:0.6097\n",
      "epoch [40/150], loss:0.3282\n",
      "epoch [41/150], loss:0.4824\n",
      "epoch [42/150], loss:0.4295\n",
      "epoch [43/150], loss:0.3025\n",
      "epoch [44/150], loss:0.3489\n",
      "epoch [45/150], loss:0.3285\n",
      "epoch [46/150], loss:0.5201\n",
      "epoch [47/150], loss:0.5722\n",
      "epoch [48/150], loss:0.4874\n",
      "epoch [49/150], loss:0.3152\n",
      "epoch [50/150], loss:0.4208\n",
      "epoch [51/150], loss:0.2640\n",
      "epoch [52/150], loss:0.4372\n",
      "epoch [53/150], loss:0.4384\n",
      "epoch [54/150], loss:0.3232\n",
      "epoch [55/150], loss:0.4643\n",
      "epoch [56/150], loss:0.3925\n",
      "epoch [57/150], loss:0.5696\n",
      "epoch [58/150], loss:0.3805\n",
      "epoch [59/150], loss:0.3711\n",
      "epoch [60/150], loss:0.2336\n",
      "epoch [61/150], loss:0.3261\n",
      "epoch [62/150], loss:0.4781\n",
      "epoch [63/150], loss:0.3858\n",
      "epoch [64/150], loss:0.3939\n",
      "epoch [65/150], loss:0.4231\n",
      "epoch [66/150], loss:0.2795\n",
      "epoch [67/150], loss:0.4303\n",
      "epoch [68/150], loss:0.4465\n",
      "epoch [69/150], loss:0.3489\n",
      "epoch [70/150], loss:0.2940\n",
      "epoch [71/150], loss:0.4309\n",
      "epoch [72/150], loss:0.3772\n",
      "epoch [73/150], loss:0.3462\n",
      "epoch [74/150], loss:0.4616\n",
      "epoch [75/150], loss:0.2661\n",
      "epoch [76/150], loss:0.4336\n",
      "epoch [77/150], loss:0.4095\n",
      "epoch [78/150], loss:0.3660\n",
      "epoch [79/150], loss:0.3623\n",
      "epoch [80/150], loss:0.3905\n",
      "epoch [81/150], loss:0.4976\n",
      "epoch [82/150], loss:0.4520\n",
      "epoch [83/150], loss:0.4279\n",
      "epoch [84/150], loss:0.3886\n",
      "epoch [85/150], loss:0.3923\n",
      "epoch [86/150], loss:0.4140\n",
      "epoch [87/150], loss:0.3326\n",
      "epoch [88/150], loss:0.3945\n",
      "epoch [89/150], loss:0.5093\n",
      "epoch [90/150], loss:0.4042\n",
      "epoch [91/150], loss:0.3861\n",
      "epoch [92/150], loss:0.3493\n",
      "epoch [93/150], loss:0.3467\n",
      "epoch [94/150], loss:0.3689\n",
      "epoch [95/150], loss:0.2462\n",
      "epoch [96/150], loss:0.5898\n",
      "epoch [97/150], loss:0.4063\n",
      "epoch [98/150], loss:0.3619\n",
      "epoch [99/150], loss:0.3479\n",
      "epoch [100/150], loss:0.3608\n",
      "epoch [101/150], loss:0.3126\n",
      "epoch [102/150], loss:0.4414\n",
      "epoch [103/150], loss:0.3487\n",
      "epoch [104/150], loss:0.3335\n",
      "epoch [105/150], loss:0.3743\n",
      "epoch [106/150], loss:0.3189\n",
      "epoch [107/150], loss:0.3448\n",
      "epoch [108/150], loss:0.2388\n",
      "epoch [109/150], loss:0.6007\n",
      "epoch [110/150], loss:0.4339\n",
      "epoch [111/150], loss:0.5949\n",
      "epoch [112/150], loss:0.3630\n",
      "epoch [113/150], loss:0.2099\n",
      "epoch [114/150], loss:0.2514\n",
      "epoch [115/150], loss:0.2236\n",
      "epoch [116/150], loss:0.3846\n",
      "epoch [117/150], loss:0.1918\n",
      "epoch [118/150], loss:0.5478\n",
      "epoch [119/150], loss:0.4156\n",
      "epoch [120/150], loss:0.2149\n",
      "epoch [121/150], loss:0.2973\n",
      "epoch [122/150], loss:0.2656\n",
      "epoch [123/150], loss:0.3251\n",
      "epoch [124/150], loss:0.4862\n",
      "epoch [125/150], loss:0.5406\n",
      "epoch [126/150], loss:0.2038\n",
      "epoch [127/150], loss:0.3670\n",
      "epoch [128/150], loss:0.3223\n",
      "epoch [129/150], loss:0.4124\n",
      "epoch [130/150], loss:0.3684\n",
      "epoch [131/150], loss:0.3650\n",
      "epoch [132/150], loss:0.2539\n",
      "epoch [133/150], loss:0.4141\n",
      "epoch [134/150], loss:0.3589\n",
      "epoch [135/150], loss:0.4092\n",
      "epoch [136/150], loss:0.4182\n",
      "epoch [137/150], loss:0.3887\n",
      "epoch [138/150], loss:0.3317\n",
      "epoch [139/150], loss:0.4218\n",
      "epoch [140/150], loss:0.5787\n",
      "epoch [141/150], loss:0.4864\n",
      "epoch [142/150], loss:0.2610\n",
      "epoch [143/150], loss:0.3078\n",
      "epoch [144/150], loss:0.2534\n",
      "epoch [145/150], loss:0.2933\n",
      "epoch [146/150], loss:0.2323\n",
      "epoch [147/150], loss:0.2774\n",
      "epoch [148/150], loss:0.3194\n",
      "epoch [149/150], loss:0.2631\n",
      "epoch [150/150], loss:0.3017\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "for e in range(EPOCHS):\n",
    "    for b in range(0,train_data.shape[0],batch_size):\n",
    "        bidaf.train()\n",
    "        batch_data = train_data.vector[b:b+batch_size]\n",
    "        batch_label = train_data.label[b:b+batch_size]\n",
    "        batch_pron = train_data.pron_vector[b:b+batch_size]\n",
    "        batch_pron = torch.Tensor(np.array(list(batch_pron.values))).unsqueeze(1).cuda()\n",
    "        batch_data = pad_sequence([torch.Tensor(v) for v in batch_data]).cuda().transpose(0,1)\n",
    "        batch_padding = batch_data.mean(dim=1,keepdim = True)#torch.zeros(batch_data.size()[0],1,batch_data.size()[2]).cuda()*0.001\n",
    "        batch_data = torch.cat([batch_padding,batch_data],dim = 1)\n",
    "        batch_label = torch.LongTensor(list(batch_label)).cuda()\n",
    "        c_mask = torch.zeros_like(batch_data.mean(-1,keepdim = True)) != batch_data.mean(-1,keepdim = True)\n",
    "        q_mask = torch.zeros_like(batch_pron.mean(-1,keepdim = True)) != batch_pron.mean(-1,keepdim = True)\n",
    "        c_mask = c_mask.cuda()\n",
    "        q_mask = q_mask.cuda()\n",
    "        #print (batch_data.mean(-1,keepdim = True).shape)\n",
    "        output = bidaf(batch_data,batch_pron,c_mask,q_mask)\n",
    "        mask_A = [np.array(v)+1 for v in list(train_data.A_idx[b:b+batch_size])]\n",
    "        mask_B = [np.array(v)+1 for v in list(train_data.B_idx[b:b+batch_size])]\n",
    "        mask_neither = [np.array(v)+1 for v in list(train_data.neither_idx_2[b:b+batch_size])]\n",
    "        #neither_prob = output[:,0]\n",
    "        prob_list = []\n",
    "        for i,(v_A,v_B,v_neither) in enumerate(zip(mask_A,mask_B,mask_neither)):\n",
    "            v_A = torch.LongTensor(v_A).cuda()\n",
    "            A_prob_ = output[i,v_A].mean()\n",
    "            v_B = torch.LongTensor(v_B).cuda()\n",
    "            B_prob_ = output[i,v_B].mean()\n",
    "            #other_prob = output[i,:].sum() - A_prob_ - B_prob_\n",
    "            v_neither = list(v_neither)\n",
    "            v_neither.append(0)\n",
    "            #print (v_neither)\n",
    "            v_neither = torch.LongTensor(v_neither).cuda()\n",
    "            other_prob = output[i,v_neither].mean()\n",
    "            prob_list.append(torch.cat([A_prob_.view(1,1),B_prob_.view(1,1),other_prob.view(1,1)]).view(-1,3))\n",
    "        #print (prob_list)\n",
    "        pred_train = torch.cat(prob_list,dim = 0)\n",
    "        #print (pred_train.size())\n",
    "            \n",
    "        #batch_label = torch.LongTensor(batch_label).cuda()\n",
    "        loss = loss_fn(pred_train,batch_label)\n",
    "        #l2_norm = torch.norm(mlp.layers[-1].weight, p=2)\n",
    "        #loss += l2_norm*0.09\n",
    "        #l2_norm = torch.norm(mlp.layers[0].weight, p=2)\n",
    "        #loss += l2_norm*0.03\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(e + 1, EPOCHS, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bidaf_train = []    \n",
    "for b in range(0,train_data.shape[0],batch_size):\n",
    "    bidaf.eval()\n",
    "    batch_data = train_data.vector[b:b+batch_size]\n",
    "    batch_label = train_data.label[b:b+batch_size]\n",
    "    batch_pron = train_data.pron_vector[b:b+batch_size]\n",
    "    batch_pron = torch.Tensor(np.array(list(batch_pron.values))).unsqueeze(1).cuda()\n",
    "    batch_data = pad_sequence([torch.Tensor(v) for v in batch_data]).cuda().transpose(0,1)\n",
    "    batch_padding = batch_data.mean(dim=1,keepdim = True)#batch_padding = torch.zeros(batch_data.size()[0],1,batch_data.size()[2]).cuda()*0.001\n",
    "    batch_data = torch.cat([batch_padding,batch_data],dim = 1)\n",
    "    batch_label = torch.LongTensor(list(batch_label)).cuda()\n",
    "    c_mask = torch.zeros_like(batch_data.mean(-1,keepdim = True)) != batch_data.mean(-1,keepdim = True)\n",
    "    q_mask = torch.zeros_like(batch_pron.mean(-1,keepdim = True)) != batch_pron.mean(-1,keepdim = True)\n",
    "    c_mask = c_mask.cuda()\n",
    "    q_mask = q_mask.cuda()\n",
    "    #print (batch_data.mean(-1,keepdim = True).shape)\n",
    "    output = bidaf(batch_data,batch_pron,c_mask,q_mask)\n",
    "    mask_A = [np.array(v)+1 for v in list(train_data.A_idx[b:b+batch_size])]\n",
    "    mask_B = [np.array(v)+1 for v in list(train_data.B_idx[b:b+batch_size])]\n",
    "    mask_neither = [np.array(v)+1 for v in list(train_data.neither_idx_2[b:b+batch_size])]\n",
    "    #neither_prob = output[:,0]\n",
    "    prob_list = []\n",
    "    for i,(v_A,v_B,v_neither) in enumerate(zip(mask_A,mask_B,mask_neither)):\n",
    "        v_A = torch.LongTensor(v_A).cuda()\n",
    "        A_prob_ = output[i,v_A].mean()\n",
    "        v_B = torch.LongTensor(v_B).cuda()\n",
    "        B_prob_ = output[i,v_B].mean()\n",
    "        #other_prob = output[i,:].sum() - A_prob_ - B_prob_\n",
    "        v_neither = list(v_neither)\n",
    "        v_neither.append(0)\n",
    "        #print (v_neither)\n",
    "        v_neither = torch.LongTensor(v_neither).cuda()\n",
    "        other_prob = output[i,v_neither].mean()\n",
    "        prob_list.append(torch.cat([A_prob_.view(1,1),B_prob_.view(1,1),other_prob.view(1,1)]).view(-1,3))\n",
    "    #print (prob_list)\n",
    "    pred_bidaf_ = torch.cat(prob_list,dim = 0)\n",
    "    pred_bidaf_train.append(pred_bidaf_)\n",
    "pred_bidaf_train = torch.nn.Softmax(dim=1)(torch.cat(pred_bidaf_train,dim = 0)).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bidaf = []    \n",
    "for b in range(0,test_data.shape[0],batch_size):\n",
    "    bidaf.eval()\n",
    "    batch_data = test_data.vector[b:b+batch_size]\n",
    "    batch_label = test_data.label[b:b+batch_size]\n",
    "    batch_pron = test_data.pron_vector[b:b+batch_size]\n",
    "    batch_pron = torch.Tensor(np.array(list(batch_pron.values))).unsqueeze(1).cuda()\n",
    "    batch_data = pad_sequence([torch.Tensor(v) for v in batch_data]).cuda().transpose(0,1)\n",
    "    batch_padding = batch_data.mean(dim=1,keepdim = True)#batch_padding = torch.zeros(batch_data.size()[0],1,batch_data.size()[2]).cuda()*0.001\n",
    "    batch_data = torch.cat([batch_padding,batch_data],dim = 1)\n",
    "    batch_label = torch.LongTensor(list(batch_label)).cuda()\n",
    "    c_mask = torch.zeros_like(batch_data.mean(-1,keepdim = True)) != batch_data.mean(-1,keepdim = True)\n",
    "    q_mask = torch.zeros_like(batch_pron.mean(-1,keepdim = True)) != batch_pron.mean(-1,keepdim = True)\n",
    "    c_mask = c_mask.cuda()\n",
    "    q_mask = q_mask.cuda()\n",
    "    #print (batch_data.mean(-1,keepdim = True).shape)\n",
    "    output = bidaf(batch_data,batch_pron,c_mask,q_mask)\n",
    "    mask_A = [np.array(v)+1 for v in list(test_data.A_idx[b:b+batch_size])]\n",
    "    mask_B = [np.array(v)+1 for v in list(test_data.B_idx[b:b+batch_size])]\n",
    "    mask_neither = [np.array(v)+1 for v in list(test_data.neither_idx_2[b:b+batch_size])]\n",
    "    #neither_prob = output[:,0]\n",
    "    prob_list = []\n",
    "    for i,(v_A,v_B,v_neither) in enumerate(zip(mask_A,mask_B,mask_neither)):\n",
    "        v_A = torch.LongTensor(v_A).cuda()\n",
    "        A_prob_ = output[i,v_A].mean()\n",
    "        v_B = torch.LongTensor(v_B).cuda()\n",
    "        B_prob_ = output[i,v_B].mean()\n",
    "        #other_prob = output[i,:].sum() - A_prob_ - B_prob_\n",
    "        v_neither = list(v_neither)\n",
    "        v_neither.append(0)\n",
    "        #print (v_neither)\n",
    "        v_neither = torch.LongTensor(v_neither).cuda()\n",
    "        other_prob = output[i,v_neither].mean()\n",
    "        prob_list.append(torch.cat([A_prob_.view(1,1),B_prob_.view(1,1),other_prob.view(1,1)]).view(-1,3))\n",
    "    #print (prob_list)\n",
    "    pred_bidaf_ = torch.cat(prob_list,dim = 0)\n",
    "    pred_bidaf.append(pred_bidaf_)\n",
    "pred_bidaf = torch.nn.Softmax(dim=1)(torch.cat(pred_bidaf,dim = 0)).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open( \"./temp_result/biDAF_result_3\", \"wb\" )\n",
    "pickle.dump(pred_bidaf_train,  f)\n",
    "pickle.dump(pred_bidaf,  f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bidaf = np.clip(pred_bidaf,1e-15,1-1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.696359</td>\n",
       "      <td>0.171845</td>\n",
       "      <td>0.131796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.997086</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.001501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>0.984696</td>\n",
       "      <td>0.009369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.014006</td>\n",
       "      <td>0.734833</td>\n",
       "      <td>0.251161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.998676</td>\n",
       "      <td>0.001177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>development-6</td>\n",
       "      <td>0.998176</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.001452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>development-7</td>\n",
       "      <td>0.417333</td>\n",
       "      <td>0.203653</td>\n",
       "      <td>0.379014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>development-8</td>\n",
       "      <td>0.224061</td>\n",
       "      <td>0.753432</td>\n",
       "      <td>0.022508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>development-9</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.992536</td>\n",
       "      <td>0.005087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>development-10</td>\n",
       "      <td>0.189624</td>\n",
       "      <td>0.727594</td>\n",
       "      <td>0.082783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>development-11</td>\n",
       "      <td>0.060527</td>\n",
       "      <td>0.837631</td>\n",
       "      <td>0.101842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>development-12</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.047948</td>\n",
       "      <td>0.010852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>development-13</td>\n",
       "      <td>0.936623</td>\n",
       "      <td>0.053015</td>\n",
       "      <td>0.010362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>development-14</td>\n",
       "      <td>0.676689</td>\n",
       "      <td>0.263351</td>\n",
       "      <td>0.059960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>development-15</td>\n",
       "      <td>0.058416</td>\n",
       "      <td>0.860644</td>\n",
       "      <td>0.080941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>development-16</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.997947</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>development-17</td>\n",
       "      <td>0.972172</td>\n",
       "      <td>0.012076</td>\n",
       "      <td>0.015752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>development-18</td>\n",
       "      <td>0.121851</td>\n",
       "      <td>0.027425</td>\n",
       "      <td>0.850725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>development-19</td>\n",
       "      <td>0.281656</td>\n",
       "      <td>0.578627</td>\n",
       "      <td>0.139717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>development-20</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.993418</td>\n",
       "      <td>0.002659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>development-21</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.997309</td>\n",
       "      <td>0.002656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>development-22</td>\n",
       "      <td>0.998515</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.001317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>development-23</td>\n",
       "      <td>0.999888</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>development-24</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.990884</td>\n",
       "      <td>0.007491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>development-25</td>\n",
       "      <td>0.983681</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>0.012021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>development-26</td>\n",
       "      <td>0.022403</td>\n",
       "      <td>0.830535</td>\n",
       "      <td>0.147061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>development-27</td>\n",
       "      <td>0.547027</td>\n",
       "      <td>0.415365</td>\n",
       "      <td>0.037608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>development-28</td>\n",
       "      <td>0.564151</td>\n",
       "      <td>0.317819</td>\n",
       "      <td>0.118030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>development-29</td>\n",
       "      <td>0.246321</td>\n",
       "      <td>0.609138</td>\n",
       "      <td>0.144541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>development-30</td>\n",
       "      <td>0.999296</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>development-31</td>\n",
       "      <td>0.567295</td>\n",
       "      <td>0.162036</td>\n",
       "      <td>0.270669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>development-32</td>\n",
       "      <td>0.462743</td>\n",
       "      <td>0.038496</td>\n",
       "      <td>0.498760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>development-33</td>\n",
       "      <td>0.024162</td>\n",
       "      <td>0.967881</td>\n",
       "      <td>0.007957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>development-34</td>\n",
       "      <td>0.090498</td>\n",
       "      <td>0.121155</td>\n",
       "      <td>0.788347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>development-35</td>\n",
       "      <td>0.727287</td>\n",
       "      <td>0.045708</td>\n",
       "      <td>0.227005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>development-36</td>\n",
       "      <td>0.040472</td>\n",
       "      <td>0.932069</td>\n",
       "      <td>0.027460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>development-37</td>\n",
       "      <td>0.015155</td>\n",
       "      <td>0.980823</td>\n",
       "      <td>0.004022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>development-38</td>\n",
       "      <td>0.722394</td>\n",
       "      <td>0.083253</td>\n",
       "      <td>0.194353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID         A         B   NEITHER\n",
       "0    development-1  0.696359  0.171845  0.131796\n",
       "1    development-2  0.997086  0.001413  0.001501\n",
       "2    development-3  0.005935  0.984696  0.009369\n",
       "3    development-4  0.014006  0.734833  0.251161\n",
       "4    development-5  0.000147  0.998676  0.001177\n",
       "5    development-6  0.998176  0.000371  0.001452\n",
       "6    development-7  0.417333  0.203653  0.379014\n",
       "7    development-8  0.224061  0.753432  0.022508\n",
       "8    development-9  0.002378  0.992536  0.005087\n",
       "9   development-10  0.189624  0.727594  0.082783\n",
       "10  development-11  0.060527  0.837631  0.101842\n",
       "11  development-12  0.941200  0.047948  0.010852\n",
       "12  development-13  0.936623  0.053015  0.010362\n",
       "13  development-14  0.676689  0.263351  0.059960\n",
       "14  development-15  0.058416  0.860644  0.080941\n",
       "15  development-16  0.001798  0.997947  0.000255\n",
       "16  development-17  0.972172  0.012076  0.015752\n",
       "17  development-18  0.121851  0.027425  0.850725\n",
       "18  development-19  0.281656  0.578627  0.139717\n",
       "19  development-20  0.003923  0.993418  0.002659\n",
       "20  development-21  0.000035  0.997309  0.002656\n",
       "21  development-22  0.998515  0.000167  0.001317\n",
       "22  development-23  0.999888  0.000031  0.000081\n",
       "23  development-24  0.001625  0.990884  0.007491\n",
       "24  development-25  0.983681  0.004298  0.012021\n",
       "25  development-26  0.022403  0.830535  0.147061\n",
       "26  development-27  0.547027  0.415365  0.037608\n",
       "27  development-28  0.564151  0.317819  0.118030\n",
       "28  development-29  0.246321  0.609138  0.144541\n",
       "29  development-30  0.999296  0.000163  0.000541\n",
       "30  development-31  0.567295  0.162036  0.270669\n",
       "31  development-32  0.462743  0.038496  0.498760\n",
       "32  development-33  0.024162  0.967881  0.007957\n",
       "33  development-34  0.090498  0.121155  0.788347\n",
       "34  development-35  0.727287  0.045708  0.227005\n",
       "35  development-36  0.040472  0.932069  0.027460\n",
       "36  development-37  0.015155  0.980823  0.004022\n",
       "37  development-38  0.722394  0.083253  0.194353"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pred_lr = process_prediction(pred_lr)\n",
    "sub_df = pd.read_csv(\"./test_and_submit/sample_submission_stage_1.csv\")\n",
    "sub_df.loc[:, ['A','B','NEITHER']] = pred_bidaf\n",
    "\n",
    "\n",
    "sub_df.to_csv(\"./test_and_submit/submission+model+bidaf@\"+str(datetime.datetime.now())+\".csv\", index=False)\n",
    "\n",
    "sub_df.head(38)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46277451515197754"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(sub_df,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
