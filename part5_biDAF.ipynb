{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train= pd.read_pickle('./temp_result/train_kaggle_processed_PPA_PCA_PPA')\n",
    "gap_test= pd.read_pickle('./temp_result/test_kaggle_processed_PPA_PCA_PPA')\n",
    "gap_valid= pd.read_pickle('./temp_result/valid_kaggle_processed_PPA_PCA_PPA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = gap_train.count().values[0]\n",
    "NUM_TEST = gap_test.count().values[0]\n",
    "NUM_VALID = gap_valid.count().values[0]\n",
    "def label(A,B):\n",
    "    if A is True:\n",
    "        return 0\n",
    "    if B is True:\n",
    "        return 1\n",
    "    return 2\n",
    "def switch_label(l):\n",
    "    if l==2:\n",
    "        return 2\n",
    "    return 1-l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prediction(pred):\n",
    "    s = pred.shape[0]//2\n",
    "    pred0 = pred[0:s,:]\n",
    "    pred1 = pred[s:,:]\n",
    "    pred1 = pred1[:,[1,0,2]]\n",
    "    pred_out = pred0+pred1\n",
    "    return pred_out/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_A_B(df):\n",
    "    columnsTitles = [\"B_dist\",\"A_dist\",\"B_pos\", \"A_pos\",\"pron_pos\", \"B_vector\", \"A_vector\",\"pron_vector\",\"product_vector_B\",\"product_vector_A\",\"label\"]\n",
    "    df2=df.reindex(columns=columnsTitles).copy()\n",
    "    df2.columns = df.columns\n",
    "    df2.label = df2.label.map(switch_label)\n",
    "    return pd.concat([df,df2],axis = 0, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compute_loss(sub_df,test_data):\n",
    "    pred = torch.Tensor(np.log(sub_df.loc[:,['A','B','NEITHER']].values))\n",
    "    label = torch.LongTensor(list(test_data.label))\n",
    "    loss = torch.nn.NLLLoss()\n",
    "    loss_value = loss(pred,label).item()\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gap_train.drop(columns = ['ID', 'Text', 'Pronoun','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map', 'pron_idx'])\n",
    "train_data.A_vector = train_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data.B_vector = train_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data.pron_vector = train_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data[\"product_vector_A\"] = train_data.A_vector*train_data.pron_vector\n",
    "train_data[\"product_vector_B\"] = train_data.B_vector*train_data.pron_vector\n",
    "train_data[\"label\"] = train_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "train_data = train_data.drop(columns= [\"A-coref\",\"B-coref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = gap_test.drop(columns = ['ID', 'Text', 'Pronoun','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map', 'pron_idx'])\n",
    "test_data.A_vector = test_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data.B_vector = test_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data.pron_vector = test_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data[\"product_vector_A\"] = test_data.A_vector*test_data.pron_vector\n",
    "test_data[\"product_vector_B\"] = test_data.B_vector*test_data.pron_vector\n",
    "test_data[\"label\"] = test_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "test_data = test_data.drop(columns= [\"A-coref\",\"B-coref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = gap_valid.drop(columns = ['ID', 'Text', 'Pronoun','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map', 'pron_idx'])\n",
    "valid_data.A_vector = valid_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data.B_vector = valid_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data.pron_vector = valid_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data[\"product_vector_A\"] = valid_data.A_vector*valid_data.pron_vector\n",
    "valid_data[\"product_vector_B\"] = valid_data.B_vector*valid_data.pron_vector\n",
    "valid_data[\"label\"] = valid_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "valid_data = valid_data.drop(columns= [\"A-coref\",\"B-coref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def masked_softmax(logits, mask, dim=-1, log_softmax=False):\n",
    "    \"\"\"Take the softmax of `logits` over given dimension, and set\n",
    "    entries to 0 wherever `mask` is 0.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Inputs to the softmax function.\n",
    "        mask (torch.Tensor): Same shape as `logits`, with 0 indicating\n",
    "            positions that should be assigned 0 probability in the output.\n",
    "        dim (int): Dimension over which to take softmax.\n",
    "        log_softmax (bool): Take log-softmax rather than regular softmax.\n",
    "            E.g., some PyTorch functions such as `F.nll_loss` expect log-softmax.\n",
    "\n",
    "    Returns:\n",
    "        probs (torch.Tensor): Result of taking masked softmax over the logits.\n",
    "    \"\"\"\n",
    "    mask = mask.type(torch.float32)\n",
    "    masked_logits = mask * logits + (1 - mask) * -1e30\n",
    "    softmax_fn = F.log_softmax if log_softmax else F.softmax\n",
    "    probs = softmax_fn(masked_logits, dim)\n",
    "\n",
    "    return probs\n",
    "class BiDAFAttention(nn.Module):\n",
    "    \"\"\"Bidirectional attention originally used by BiDAF.\n",
    "\n",
    "    Bidirectional attention computes attention in two directions:\n",
    "    The context attends to the query and the query attends to the context.\n",
    "    The output of this layer is the concatenation of [context, c2q_attention,\n",
    "    context * c2q_attention, context * q2c_attention]. This concatenation allows\n",
    "    the attention vector at each timestep, along with the embeddings from\n",
    "    previous layers, to flow through the attention layer to the modeling layer.\n",
    "    The output has shape (batch_size, context_len, 8 * hidden_size).\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): Size of hidden activations.\n",
    "        drop_prob (float): Probability of zero-ing out activations.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, drop_prob=0.6):\n",
    "        super(BiDAFAttention, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.c_weight = nn.Parameter(torch.zeros(hidden_size, 1))\n",
    "        self.q_weight = nn.Parameter(torch.zeros(hidden_size, 1))\n",
    "        self.cq_weight = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "        for weight in (self.c_weight, self.q_weight, self.cq_weight):\n",
    "            nn.init.xavier_uniform_(weight)\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        self.drop1 = nn.Dropout(self.drop_prob)  # (bs, c_len, hid_size)\n",
    "        self.drop2 = nn.Dropout(self.drop_prob)\n",
    "        self.output_layer1 = nn.Linear(4*hidden_size,64)\n",
    "        self.drop3 = nn.Dropout(self.drop_prob)\n",
    "        self.output_layer2 = nn.Linear(64,1)\n",
    "        self.drop4 = nn.Dropout(self.drop_prob)\n",
    "        nn.init.xavier_uniform_(self.output_layer1.weight)\n",
    "        nn.init.xavier_uniform_(self.output_layer2.weight)\n",
    "\n",
    "        \n",
    "    def forward(self, c, q, c_mask, q_mask):\n",
    "        batch_size, c_len, _ = c.size()\n",
    "        q_len = q.size(1)\n",
    "        s = self.get_similarity_matrix(c, q)        # (batch_size, c_len, q_len)\n",
    "        c_mask = c_mask.view(batch_size, c_len, 1)  # (batch_size, c_len, 1)\n",
    "        q_mask = q_mask.view(batch_size, 1, q_len)  # (batch_size, 1, q_len)\n",
    "        s1 = masked_softmax(s, q_mask, dim=2)       # (batch_size, c_len, q_len)\n",
    "        s2 = masked_softmax(s, c_mask, dim=1)       # (batch_size, c_len, q_len)\n",
    "\n",
    "        # (bs, c_len, q_len) x (bs, q_len, hid_size) => (bs, c_len, hid_size)\n",
    "        a = torch.bmm(s1, q)\n",
    "        # (bs, c_len, c_len) x (bs, c_len, hid_size) => (bs, c_len, hid_size)\n",
    "        b = torch.bmm(torch.bmm(s1, s2.transpose(1, 2)), c)\n",
    "\n",
    "        \n",
    "        x = torch.cat([c, a, c * a, c * b], dim=2)  # (bs, c_len, 4 * hid_size)\n",
    "        \n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        x = self.output_layer1(x)\n",
    "        \n",
    "        x = torch.nn.ReLU()(x)\n",
    "        \n",
    "        x = self.drop4(x)\n",
    "        \n",
    "        x = self.output_layer2(x)\n",
    "        \n",
    "        return x.squeeze(-1)\n",
    "\n",
    "    def get_similarity_matrix(self, c, q):\n",
    "        \"\"\"Get the \"similarity matrix\" between context and query (using the\n",
    "        terminology of the BiDAF paper).\n",
    "\n",
    "        A naive implementation as described in BiDAF would concatenate the\n",
    "        three vectors then project the result with a single weight matrix. This\n",
    "        method is a more memory-efficient implementation of the same operation.\n",
    "\n",
    "        See Also:\n",
    "            Equation 1 in https://arxiv.org/abs/1611.01603\n",
    "        \"\"\"\n",
    "        c_len, q_len = c.size(1), q.size(1)\n",
    "        c = self.drop1(c)  # (bs, c_len, hid_size)\n",
    "        q = self.drop2(q)  # (bs, q_len, hid_size)\n",
    "        #print (c.size())\n",
    "        #print (q.size())\n",
    "        # Shapes: (batch_size, c_len, q_len)\n",
    "        s0 = torch.matmul(c, self.c_weight).expand([-1, -1, q_len])\n",
    "        s1 = torch.matmul(q, self.q_weight).transpose(1, 2)\\\n",
    "                                           .expand([-1, c_len, -1])\n",
    "        s2 = torch.matmul(c * self.cq_weight, q.transpose(1, 2))\n",
    "        s = s0 + s1 + s2 + self.bias\n",
    "\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidaf = BiDAFAttention(256).cuda()\n",
    "EPOCHS = 150\n",
    "batch_size = 25\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight = torch.Tensor([1.0,1.0,10.0])).cuda()\n",
    "opt = torch.optim.Adam(bidaf.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector</th>\n",
       "      <th>A_dist</th>\n",
       "      <th>B_dist</th>\n",
       "      <th>A_pos</th>\n",
       "      <th>B_pos</th>\n",
       "      <th>pron_pos</th>\n",
       "      <th>A_idx</th>\n",
       "      <th>B_idx</th>\n",
       "      <th>A_vector</th>\n",
       "      <th>B_vector</th>\n",
       "      <th>pron_vector</th>\n",
       "      <th>product_vector_A</th>\n",
       "      <th>product_vector_B</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.85816926, -0.45762736, -0.08299036, -0.275...</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.693548</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>[52, 53, 54, 55, 56, 57, 58, 59]</td>\n",
       "      <td>[63, 64, 65]</td>\n",
       "      <td>[0.20037952, 0.2805402, -0.11013528, -0.492183...</td>\n",
       "      <td>[0.6041415, -0.13538514, 0.15207358, -0.076126...</td>\n",
       "      <td>[0.8586822, -1.2192798, 0.09194927, -0.4327072...</td>\n",
       "      <td>[0.17206234, -0.342057, -0.010126858, 0.212971...</td>\n",
       "      <td>[0.51876557, 0.16507237, 0.013983054, 0.032940...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.2511816, 0.24685939, -0.3399855, -0.624934...</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>[34, 35]</td>\n",
       "      <td>[46, 47, 48]</td>\n",
       "      <td>[0.067682624, 0.42009318, -0.009991955, -0.659...</td>\n",
       "      <td>[-0.438681, 0.48596978, -0.02240046, 0.0839553...</td>\n",
       "      <td>[-0.67510414, -0.39920956, 0.07794422, 0.43378...</td>\n",
       "      <td>[-0.04569282, -0.16770521, -0.0007788151, -0.2...</td>\n",
       "      <td>[0.29615536, -0.19400378, -0.0017459863, 0.036...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.8131293, 0.4440992, 0.7549018, 0.4792695, ...</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.405128</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>[89, 90, 91, 92, 93, 94, 95]</td>\n",
       "      <td>[99, 100]</td>\n",
       "      <td>[-0.40055174, -0.06318157, 0.08668772, -0.0661...</td>\n",
       "      <td>[0.67819536, 0.21544528, -0.39938855, 0.618967...</td>\n",
       "      <td>[0.72358626, -1.0747175, 0.050702423, -0.89089...</td>\n",
       "      <td>[-0.28983372, 0.06790234, 0.0043952777, 0.0588...</td>\n",
       "      <td>[0.49073285, -0.23154281, -0.020249967, -0.551...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[1.033058, -0.108201064, -0.1557167, 0.394294...</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.201521</td>\n",
       "      <td>0.216730</td>\n",
       "      <td>0.224335</td>\n",
       "      <td>[66, 67, 68]</td>\n",
       "      <td>[73, 74, 75]</td>\n",
       "      <td>[-0.33478984, -0.63129133, -0.5514479, 0.59391...</td>\n",
       "      <td>[0.50680375, 0.71950006, -0.25772676, 0.375783...</td>\n",
       "      <td>[1.2850071, 0.049930945, -0.26918668, 0.253453...</td>\n",
       "      <td>[-0.43020734, -0.031520974, 0.14844243, 0.1505...</td>\n",
       "      <td>[0.6512464, 0.035925318, 0.06937661, 0.0952435...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[1.3460386, 0.037629873, 0.55879486, -0.42726...</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.321839</td>\n",
       "      <td>0.362069</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>[71, 72, 73, 74]</td>\n",
       "      <td>[80, 81, 82, 83]</td>\n",
       "      <td>[0.25996214, -0.15495634, -0.5617872, -0.04960...</td>\n",
       "      <td>[0.2925861, 0.2669702, 0.03382411, -0.3578499,...</td>\n",
       "      <td>[-0.87992877, -0.07835998, 0.12989467, -0.2304...</td>\n",
       "      <td>[-0.22874817, 0.012142375, -0.07297316, 0.0114...</td>\n",
       "      <td>[-0.2574549, -0.020919777, 0.0043935715, 0.082...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              vector  A_dist  B_dist  \\\n",
       "0  [[0.85816926, -0.45762736, -0.08299036, -0.275...   0.018   0.004   \n",
       "1  [[0.2511816, 0.24685939, -0.3399855, -0.624934...   0.030   0.012   \n",
       "2  [[0.8131293, 0.4440992, 0.7549018, 0.4792695, ...   0.022   0.010   \n",
       "3  [[1.033058, -0.108201064, -0.1557167, 0.394294...   0.012   0.004   \n",
       "4  [[1.3460386, 0.037629873, 0.55879486, -0.42726...   0.056   0.042   \n",
       "\n",
       "      A_pos     B_pos  pron_pos                             A_idx  \\\n",
       "0  0.693548  0.806452  0.838710  [52, 53, 54, 55, 56, 57, 58, 59]   \n",
       "1  0.511111  0.711111  0.844444                          [34, 35]   \n",
       "2  0.405128  0.435897  0.461538      [89, 90, 91, 92, 93, 94, 95]   \n",
       "3  0.201521  0.216730  0.224335                      [66, 67, 68]   \n",
       "4  0.321839  0.362069  0.482759                  [71, 72, 73, 74]   \n",
       "\n",
       "              B_idx                                           A_vector  \\\n",
       "0      [63, 64, 65]  [0.20037952, 0.2805402, -0.11013528, -0.492183...   \n",
       "1      [46, 47, 48]  [0.067682624, 0.42009318, -0.009991955, -0.659...   \n",
       "2         [99, 100]  [-0.40055174, -0.06318157, 0.08668772, -0.0661...   \n",
       "3      [73, 74, 75]  [-0.33478984, -0.63129133, -0.5514479, 0.59391...   \n",
       "4  [80, 81, 82, 83]  [0.25996214, -0.15495634, -0.5617872, -0.04960...   \n",
       "\n",
       "                                            B_vector  \\\n",
       "0  [0.6041415, -0.13538514, 0.15207358, -0.076126...   \n",
       "1  [-0.438681, 0.48596978, -0.02240046, 0.0839553...   \n",
       "2  [0.67819536, 0.21544528, -0.39938855, 0.618967...   \n",
       "3  [0.50680375, 0.71950006, -0.25772676, 0.375783...   \n",
       "4  [0.2925861, 0.2669702, 0.03382411, -0.3578499,...   \n",
       "\n",
       "                                         pron_vector  \\\n",
       "0  [0.8586822, -1.2192798, 0.09194927, -0.4327072...   \n",
       "1  [-0.67510414, -0.39920956, 0.07794422, 0.43378...   \n",
       "2  [0.72358626, -1.0747175, 0.050702423, -0.89089...   \n",
       "3  [1.2850071, 0.049930945, -0.26918668, 0.253453...   \n",
       "4  [-0.87992877, -0.07835998, 0.12989467, -0.2304...   \n",
       "\n",
       "                                    product_vector_A  \\\n",
       "0  [0.17206234, -0.342057, -0.010126858, 0.212971...   \n",
       "1  [-0.04569282, -0.16770521, -0.0007788151, -0.2...   \n",
       "2  [-0.28983372, 0.06790234, 0.0043952777, 0.0588...   \n",
       "3  [-0.43020734, -0.031520974, 0.14844243, 0.1505...   \n",
       "4  [-0.22874817, 0.012142375, -0.07297316, 0.0114...   \n",
       "\n",
       "                                    product_vector_B  label  \n",
       "0  [0.51876557, 0.16507237, 0.013983054, 0.032940...      2  \n",
       "1  [0.29615536, -0.19400378, -0.0017459863, 0.036...      1  \n",
       "2  [0.49073285, -0.23154281, -0.020249967, -0.551...      1  \n",
       "3  [0.6512464, 0.035925318, 0.06937661, 0.0952435...      0  \n",
       "4  [-0.2574549, -0.020919777, 0.0043935715, 0.082...      1  "
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/150], loss:1.7956\n",
      "epoch [2/150], loss:1.0731\n",
      "epoch [3/150], loss:0.7776\n",
      "epoch [4/150], loss:1.0059\n",
      "epoch [5/150], loss:0.7886\n",
      "epoch [6/150], loss:0.7306\n",
      "epoch [7/150], loss:0.7616\n",
      "epoch [8/150], loss:0.7532\n",
      "epoch [9/150], loss:0.4585\n",
      "epoch [10/150], loss:0.5371\n",
      "epoch [11/150], loss:0.7603\n",
      "epoch [12/150], loss:0.6833\n",
      "epoch [13/150], loss:0.5031\n",
      "epoch [14/150], loss:0.6288\n",
      "epoch [15/150], loss:0.5816\n",
      "epoch [16/150], loss:0.4186\n",
      "epoch [17/150], loss:0.4283\n",
      "epoch [18/150], loss:0.5451\n",
      "epoch [19/150], loss:0.3474\n",
      "epoch [20/150], loss:0.2954\n",
      "epoch [21/150], loss:0.7588\n",
      "epoch [22/150], loss:0.4110\n",
      "epoch [23/150], loss:0.4059\n",
      "epoch [24/150], loss:0.5502\n",
      "epoch [25/150], loss:0.5369\n",
      "epoch [26/150], loss:0.4051\n",
      "epoch [27/150], loss:0.6756\n",
      "epoch [28/150], loss:0.3830\n",
      "epoch [29/150], loss:0.4198\n",
      "epoch [30/150], loss:0.4422\n",
      "epoch [31/150], loss:0.4687\n",
      "epoch [32/150], loss:0.4130\n",
      "epoch [33/150], loss:0.3138\n",
      "epoch [34/150], loss:0.3460\n",
      "epoch [35/150], loss:0.7263\n",
      "epoch [36/150], loss:0.3040\n",
      "epoch [37/150], loss:0.4864\n",
      "epoch [38/150], loss:0.1922\n",
      "epoch [39/150], loss:0.3185\n",
      "epoch [40/150], loss:0.3143\n",
      "epoch [41/150], loss:0.3087\n",
      "epoch [42/150], loss:0.3477\n",
      "epoch [43/150], loss:0.2572\n",
      "epoch [44/150], loss:0.3586\n",
      "epoch [45/150], loss:0.6046\n",
      "epoch [46/150], loss:0.3150\n",
      "epoch [47/150], loss:0.5481\n",
      "epoch [48/150], loss:0.3584\n",
      "epoch [49/150], loss:0.2595\n",
      "epoch [50/150], loss:0.3387\n",
      "epoch [51/150], loss:0.4835\n",
      "epoch [52/150], loss:0.5634\n",
      "epoch [53/150], loss:0.2134\n",
      "epoch [54/150], loss:0.3773\n",
      "epoch [55/150], loss:0.2139\n",
      "epoch [56/150], loss:0.4761\n",
      "epoch [57/150], loss:0.2656\n",
      "epoch [58/150], loss:0.3588\n",
      "epoch [59/150], loss:0.3065\n",
      "epoch [60/150], loss:0.4291\n",
      "epoch [61/150], loss:0.3809\n",
      "epoch [62/150], loss:0.2335\n",
      "epoch [63/150], loss:0.3028\n",
      "epoch [64/150], loss:0.2930\n",
      "epoch [65/150], loss:0.3190\n",
      "epoch [66/150], loss:0.3840\n",
      "epoch [67/150], loss:0.1812\n",
      "epoch [68/150], loss:0.2667\n",
      "epoch [69/150], loss:0.2060\n",
      "epoch [70/150], loss:0.1429\n",
      "epoch [71/150], loss:0.2053\n",
      "epoch [72/150], loss:0.4235\n",
      "epoch [73/150], loss:0.1743\n",
      "epoch [74/150], loss:0.2464\n",
      "epoch [75/150], loss:0.1458\n",
      "epoch [76/150], loss:0.2033\n",
      "epoch [77/150], loss:0.2558\n",
      "epoch [78/150], loss:0.4250\n",
      "epoch [79/150], loss:0.2713\n",
      "epoch [80/150], loss:0.7537\n",
      "epoch [81/150], loss:0.3056\n",
      "epoch [82/150], loss:0.1873\n",
      "epoch [83/150], loss:0.5138\n",
      "epoch [84/150], loss:0.5392\n",
      "epoch [85/150], loss:0.4504\n",
      "epoch [86/150], loss:0.3690\n",
      "epoch [87/150], loss:0.1847\n",
      "epoch [88/150], loss:0.2355\n",
      "epoch [89/150], loss:0.3123\n",
      "epoch [90/150], loss:0.2778\n",
      "epoch [91/150], loss:0.4711\n",
      "epoch [92/150], loss:0.4537\n",
      "epoch [93/150], loss:0.3307\n",
      "epoch [94/150], loss:0.2918\n",
      "epoch [95/150], loss:0.2759\n",
      "epoch [96/150], loss:0.1731\n",
      "epoch [97/150], loss:0.3216\n",
      "epoch [98/150], loss:0.3231\n",
      "epoch [99/150], loss:0.1341\n",
      "epoch [100/150], loss:0.4183\n",
      "epoch [101/150], loss:0.3454\n",
      "epoch [102/150], loss:0.3077\n",
      "epoch [103/150], loss:0.2059\n",
      "epoch [104/150], loss:0.2860\n",
      "epoch [105/150], loss:0.1933\n",
      "epoch [106/150], loss:0.2091\n",
      "epoch [107/150], loss:0.1008\n",
      "epoch [108/150], loss:0.1659\n",
      "epoch [109/150], loss:0.3613\n",
      "epoch [110/150], loss:0.1212\n",
      "epoch [111/150], loss:0.2586\n",
      "epoch [112/150], loss:0.3458\n",
      "epoch [113/150], loss:0.2256\n",
      "epoch [114/150], loss:0.3082\n",
      "epoch [115/150], loss:0.1498\n",
      "epoch [116/150], loss:0.1308\n",
      "epoch [117/150], loss:0.2750\n",
      "epoch [118/150], loss:0.2681\n",
      "epoch [119/150], loss:0.4527\n",
      "epoch [120/150], loss:0.2205\n",
      "epoch [121/150], loss:0.1773\n",
      "epoch [122/150], loss:0.1664\n",
      "epoch [123/150], loss:0.1580\n",
      "epoch [124/150], loss:0.2640\n",
      "epoch [125/150], loss:0.2448\n",
      "epoch [126/150], loss:0.1059\n",
      "epoch [127/150], loss:0.4218\n",
      "epoch [128/150], loss:0.2093\n",
      "epoch [129/150], loss:0.2498\n",
      "epoch [130/150], loss:0.2776\n",
      "epoch [131/150], loss:0.1538\n",
      "epoch [132/150], loss:0.7067\n",
      "epoch [133/150], loss:0.3751\n",
      "epoch [134/150], loss:0.2339\n",
      "epoch [135/150], loss:0.2245\n",
      "epoch [136/150], loss:0.2142\n",
      "epoch [137/150], loss:0.4273\n",
      "epoch [138/150], loss:0.2094\n",
      "epoch [139/150], loss:0.2917\n",
      "epoch [140/150], loss:0.4319\n",
      "epoch [141/150], loss:0.4068\n",
      "epoch [142/150], loss:0.0911\n",
      "epoch [143/150], loss:0.1704\n",
      "epoch [144/150], loss:0.3804\n",
      "epoch [145/150], loss:0.1949\n",
      "epoch [146/150], loss:0.2937\n",
      "epoch [147/150], loss:0.2537\n",
      "epoch [148/150], loss:0.2027\n",
      "epoch [149/150], loss:0.2673\n",
      "epoch [150/150], loss:0.2827\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "for e in range(EPOCHS):\n",
    "    for b in range(0,train_data.shape[0],batch_size):\n",
    "        bidaf.train()\n",
    "        batch_data = train_data.vector[b:b+batch_size]\n",
    "        batch_label = train_data.label[b:b+batch_size]\n",
    "        batch_pron = train_data.pron_vector[b:b+batch_size]\n",
    "        batch_pron = torch.Tensor(np.array(list(batch_pron.values))).unsqueeze(1).cuda()\n",
    "        batch_data = pad_sequence([torch.Tensor(v) for v in batch_data]).cuda().transpose(0,1)\n",
    "        batch_padding = batch_data.mean(dim=1,keepdim = True)#torch.zeros(batch_data.size()[0],1,batch_data.size()[2]).cuda()*0.001\n",
    "        batch_data = torch.cat([batch_padding,batch_data],dim = 1)\n",
    "        batch_label = torch.LongTensor(list(batch_label)).cuda()\n",
    "        c_mask = torch.zeros_like(batch_data.mean(-1,keepdim = True)) != batch_data.mean(-1,keepdim = True)\n",
    "        q_mask = torch.zeros_like(batch_pron.mean(-1,keepdim = True)) != batch_pron.mean(-1,keepdim = True)\n",
    "        c_mask = c_mask.cuda()\n",
    "        q_mask = q_mask.cuda()\n",
    "        #print (batch_data.mean(-1,keepdim = True).shape)\n",
    "        output = bidaf(batch_data,batch_pron,c_mask,q_mask)\n",
    "        mask_A = [np.array(v)+1 for v in list(train_data.A_idx[b:b+batch_size])]\n",
    "        mask_B = [np.array(v)+1 for v in list(train_data.B_idx[b:b+batch_size])]\n",
    "        #neither_prob = output[:,0]\n",
    "        prob_list = []\n",
    "        for i,(v_A,v_B) in enumerate(zip(mask_A,mask_B)):\n",
    "            v_A = torch.LongTensor(v_A).cuda()\n",
    "            A_prob_ = output[i,v_A].sum()\n",
    "            v_B = torch.LongTensor(v_B).cuda()\n",
    "            B_prob_ = output[i,v_B].sum()\n",
    "            #other_prob = output[i,:].sum() - A_prob_ - B_prob_\n",
    "            other_prob = output[i,0].sum()\n",
    "            prob_list.append(torch.cat([A_prob_.view(1,1),B_prob_.view(1,1),other_prob.view(1,1)]).view(-1,3))\n",
    "        #print (prob_list)\n",
    "        pred_train = torch.cat(prob_list,dim = 0)\n",
    "        #print (pred_train.size())\n",
    "            \n",
    "        #batch_label = torch.LongTensor(batch_label).cuda()\n",
    "        loss = loss_fn(pred_train,batch_label)\n",
    "        #l2_norm = torch.norm(mlp.layers[-1].weight, p=2)\n",
    "        #loss += l2_norm*0.09\n",
    "        #l2_norm = torch.norm(mlp.layers[0].weight, p=2)\n",
    "        #loss += l2_norm*0.03\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(e + 1, EPOCHS, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bidaf_train = []    \n",
    "for b in range(0,train_data.shape[0],batch_size):\n",
    "    bidaf.eval()\n",
    "    batch_data = train_data.vector[b:b+batch_size]\n",
    "    batch_label = train_data.label[b:b+batch_size]\n",
    "    batch_pron = train_data.pron_vector[b:b+batch_size]\n",
    "    batch_pron = torch.Tensor(np.array(list(batch_pron.values))).unsqueeze(1).cuda()\n",
    "    batch_data = pad_sequence([torch.Tensor(v) for v in batch_data]).cuda().transpose(0,1)\n",
    "    batch_padding = batch_data.mean(dim=1,keepdim = True)#batch_padding = torch.zeros(batch_data.size()[0],1,batch_data.size()[2]).cuda()*0.001\n",
    "    batch_data = torch.cat([batch_padding,batch_data],dim = 1)\n",
    "    batch_label = torch.LongTensor(list(batch_label)).cuda()\n",
    "    c_mask = torch.zeros_like(batch_data.mean(-1,keepdim = True)) != batch_data.mean(-1,keepdim = True)\n",
    "    q_mask = torch.zeros_like(batch_pron.mean(-1,keepdim = True)) != batch_pron.mean(-1,keepdim = True)\n",
    "    c_mask = c_mask.cuda()\n",
    "    q_mask = q_mask.cuda()\n",
    "    #print (batch_data.mean(-1,keepdim = True).shape)\n",
    "    output = bidaf(batch_data,batch_pron,c_mask,q_mask)\n",
    "    mask_A = [np.array(v)+1 for v in list(train_data.A_idx[b:b+batch_size])]\n",
    "    mask_B = [np.array(v)+1 for v in list(train_data.B_idx[b:b+batch_size])]\n",
    "    #neither_prob = output[:,0]\n",
    "    prob_list = []\n",
    "    for i,(v_A,v_B) in enumerate(zip(mask_A,mask_B)):\n",
    "        v_A = torch.LongTensor(v_A).cuda()\n",
    "        A_prob_ = output[i,v_A].sum()\n",
    "        v_B = torch.LongTensor(v_B).cuda()\n",
    "        B_prob_ = output[i,v_B].sum()\n",
    "        #other_prob = output[i,:].sum() - A_prob_ - B_prob_\n",
    "        other_prob = output[i,0].sum()\n",
    "        prob_list.append(torch.cat([A_prob_.view(1,1),B_prob_.view(1,1),other_prob.view(1,1)]).view(-1,3))\n",
    "    #print (prob_list)\n",
    "    pred_bidaf_ = torch.cat(prob_list,dim = 0)\n",
    "    pred_bidaf_train.append(pred_bidaf_)\n",
    "pred_bidaf_train = torch.nn.Softmax(dim=1)(torch.cat(pred_bidaf_train,dim = 0)).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bidaf = []    \n",
    "for b in range(0,test_data.shape[0],batch_size):\n",
    "    bidaf.eval()\n",
    "    batch_data = test_data.vector[b:b+batch_size]\n",
    "    batch_label = test_data.label[b:b+batch_size]\n",
    "    batch_pron = test_data.pron_vector[b:b+batch_size]\n",
    "    batch_pron = torch.Tensor(np.array(list(batch_pron.values))).unsqueeze(1).cuda()\n",
    "    batch_data = pad_sequence([torch.Tensor(v) for v in batch_data]).cuda().transpose(0,1)\n",
    "    batch_padding = batch_data.mean(dim=1,keepdim = True)#batch_padding = torch.zeros(batch_data.size()[0],1,batch_data.size()[2]).cuda()*0.001\n",
    "    batch_data = torch.cat([batch_padding,batch_data],dim = 1)\n",
    "    batch_label = torch.LongTensor(list(batch_label)).cuda()\n",
    "    c_mask = torch.zeros_like(batch_data.mean(-1,keepdim = True)) != batch_data.mean(-1,keepdim = True)\n",
    "    q_mask = torch.zeros_like(batch_pron.mean(-1,keepdim = True)) != batch_pron.mean(-1,keepdim = True)\n",
    "    c_mask = c_mask.cuda()\n",
    "    q_mask = q_mask.cuda()\n",
    "    #print (batch_data.mean(-1,keepdim = True).shape)\n",
    "    output = bidaf(batch_data,batch_pron,c_mask,q_mask)\n",
    "    mask_A = [np.array(v)+1 for v in list(test_data.A_idx[b:b+batch_size])]\n",
    "    mask_B = [np.array(v)+1 for v in list(test_data.B_idx[b:b+batch_size])]\n",
    "    #neither_prob = output[:,0]\n",
    "    prob_list = []\n",
    "    for i,(v_A,v_B) in enumerate(zip(mask_A,mask_B)):\n",
    "        v_A = torch.LongTensor(v_A).cuda()\n",
    "        A_prob_ = output[i,v_A].sum()\n",
    "        v_B = torch.LongTensor(v_B).cuda()\n",
    "        B_prob_ = output[i,v_B].sum()\n",
    "        #other_prob = output[i,:].sum() - A_prob_ - B_prob_\n",
    "        other_prob = output[i,0].sum()\n",
    "        prob_list.append(torch.cat([A_prob_.view(1,1),B_prob_.view(1,1),other_prob.view(1,1)]).view(-1,3))\n",
    "    #print (prob_list)\n",
    "    pred_bidaf_ = torch.cat(prob_list,dim = 0)\n",
    "    pred_bidaf.append(pred_bidaf_)\n",
    "pred_bidaf = torch.nn.Softmax(dim=1)(torch.cat(pred_bidaf,dim = 0)).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open( \"./temp_result/biDAF_result\", \"wb\" )\n",
    "pickle.dump(pred_bidaf_train,  f)\n",
    "pickle.dump(pred_bidaf,  f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bidaf = np.clip(pred_bidaf,1e-15,1-1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.904248</td>\n",
       "      <td>0.047396</td>\n",
       "      <td>0.048356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.999467</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>0.932692</td>\n",
       "      <td>0.058803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.070019</td>\n",
       "      <td>0.735166</td>\n",
       "      <td>0.194815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.994062</td>\n",
       "      <td>0.005936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>development-6</td>\n",
       "      <td>0.997269</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.000898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>development-7</td>\n",
       "      <td>0.301015</td>\n",
       "      <td>0.294727</td>\n",
       "      <td>0.404258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>development-8</td>\n",
       "      <td>0.071945</td>\n",
       "      <td>0.914310</td>\n",
       "      <td>0.013745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>development-9</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.978538</td>\n",
       "      <td>0.019612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>development-10</td>\n",
       "      <td>0.246163</td>\n",
       "      <td>0.617636</td>\n",
       "      <td>0.136201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>development-11</td>\n",
       "      <td>0.015060</td>\n",
       "      <td>0.713552</td>\n",
       "      <td>0.271388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>development-12</td>\n",
       "      <td>0.779562</td>\n",
       "      <td>0.211222</td>\n",
       "      <td>0.009216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>development-13</td>\n",
       "      <td>0.497471</td>\n",
       "      <td>0.397399</td>\n",
       "      <td>0.105130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>development-14</td>\n",
       "      <td>0.929158</td>\n",
       "      <td>0.052434</td>\n",
       "      <td>0.018408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>development-15</td>\n",
       "      <td>0.138834</td>\n",
       "      <td>0.639807</td>\n",
       "      <td>0.221358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>development-16</td>\n",
       "      <td>0.007242</td>\n",
       "      <td>0.983892</td>\n",
       "      <td>0.008866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>development-17</td>\n",
       "      <td>0.972542</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.021297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>development-18</td>\n",
       "      <td>0.219343</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.780310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>development-19</td>\n",
       "      <td>0.299247</td>\n",
       "      <td>0.361628</td>\n",
       "      <td>0.339125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>development-20</td>\n",
       "      <td>0.030786</td>\n",
       "      <td>0.939035</td>\n",
       "      <td>0.030179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>development-21</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.985597</td>\n",
       "      <td>0.014380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>development-22</td>\n",
       "      <td>0.993150</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.006844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>development-23</td>\n",
       "      <td>0.999063</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>development-24</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.993341</td>\n",
       "      <td>0.006416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>development-25</td>\n",
       "      <td>0.980016</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.018363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>development-26</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.830667</td>\n",
       "      <td>0.165166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>development-27</td>\n",
       "      <td>0.539823</td>\n",
       "      <td>0.326387</td>\n",
       "      <td>0.133790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>development-28</td>\n",
       "      <td>0.711459</td>\n",
       "      <td>0.067163</td>\n",
       "      <td>0.221378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>development-29</td>\n",
       "      <td>0.243265</td>\n",
       "      <td>0.168843</td>\n",
       "      <td>0.587893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>development-30</td>\n",
       "      <td>0.998935</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>development-31</td>\n",
       "      <td>0.266125</td>\n",
       "      <td>0.472894</td>\n",
       "      <td>0.260981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>development-32</td>\n",
       "      <td>0.525051</td>\n",
       "      <td>0.066363</td>\n",
       "      <td>0.408586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>development-33</td>\n",
       "      <td>0.017174</td>\n",
       "      <td>0.955800</td>\n",
       "      <td>0.027026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>development-34</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.110290</td>\n",
       "      <td>0.889490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>development-35</td>\n",
       "      <td>0.927872</td>\n",
       "      <td>0.009426</td>\n",
       "      <td>0.062702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>development-36</td>\n",
       "      <td>0.007968</td>\n",
       "      <td>0.990382</td>\n",
       "      <td>0.001650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>development-37</td>\n",
       "      <td>0.044005</td>\n",
       "      <td>0.935905</td>\n",
       "      <td>0.020090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>development-38</td>\n",
       "      <td>0.706662</td>\n",
       "      <td>0.117909</td>\n",
       "      <td>0.175429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID         A         B   NEITHER\n",
       "0    development-1  0.904248  0.047396  0.048356\n",
       "1    development-2  0.999467  0.000196  0.000337\n",
       "2    development-3  0.008504  0.932692  0.058803\n",
       "3    development-4  0.070019  0.735166  0.194815\n",
       "4    development-5  0.000002  0.994062  0.005936\n",
       "5    development-6  0.997269  0.001833  0.000898\n",
       "6    development-7  0.301015  0.294727  0.404258\n",
       "7    development-8  0.071945  0.914310  0.013745\n",
       "8    development-9  0.001850  0.978538  0.019612\n",
       "9   development-10  0.246163  0.617636  0.136201\n",
       "10  development-11  0.015060  0.713552  0.271388\n",
       "11  development-12  0.779562  0.211222  0.009216\n",
       "12  development-13  0.497471  0.397399  0.105130\n",
       "13  development-14  0.929158  0.052434  0.018408\n",
       "14  development-15  0.138834  0.639807  0.221358\n",
       "15  development-16  0.007242  0.983892  0.008866\n",
       "16  development-17  0.972542  0.006161  0.021297\n",
       "17  development-18  0.219343  0.000347  0.780310\n",
       "18  development-19  0.299247  0.361628  0.339125\n",
       "19  development-20  0.030786  0.939035  0.030179\n",
       "20  development-21  0.000023  0.985597  0.014380\n",
       "21  development-22  0.993150  0.000006  0.006844\n",
       "22  development-23  0.999063  0.000014  0.000923\n",
       "23  development-24  0.000244  0.993341  0.006416\n",
       "24  development-25  0.980016  0.001621  0.018363\n",
       "25  development-26  0.004167  0.830667  0.165166\n",
       "26  development-27  0.539823  0.326387  0.133790\n",
       "27  development-28  0.711459  0.067163  0.221378\n",
       "28  development-29  0.243265  0.168843  0.587893\n",
       "29  development-30  0.998935  0.000001  0.001064\n",
       "30  development-31  0.266125  0.472894  0.260981\n",
       "31  development-32  0.525051  0.066363  0.408586\n",
       "32  development-33  0.017174  0.955800  0.027026\n",
       "33  development-34  0.000220  0.110290  0.889490\n",
       "34  development-35  0.927872  0.009426  0.062702\n",
       "35  development-36  0.007968  0.990382  0.001650\n",
       "36  development-37  0.044005  0.935905  0.020090\n",
       "37  development-38  0.706662  0.117909  0.175429"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pred_lr = process_prediction(pred_lr)\n",
    "sub_df = pd.read_csv(\"./test_and_submit/sample_submission_stage_1.csv\")\n",
    "sub_df.loc[:, ['A','B','NEITHER']] = pred_bidaf\n",
    "\n",
    "\n",
    "sub_df.to_csv(\"./test_and_submit/submission+model+bidaf@\"+str(datetime.datetime.now())+\".csv\", index=False)\n",
    "\n",
    "sub_df.head(38)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5305483937263489"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(sub_df,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
