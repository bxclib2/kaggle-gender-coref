{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train= pd.read_pickle('./temp_result/train_kaggle_processed_PPA_PCA_PPA')\n",
    "gap_test= pd.read_pickle('./temp_result/test_kaggle_processed_PPA_PCA_PPA')\n",
    "gap_valid= pd.read_pickle('./temp_result/valid_kaggle_processed_PPA_PCA_PPA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = gap_train.count().values[0]\n",
    "NUM_TEST = gap_test.count().values[0]\n",
    "NUM_VALID = gap_valid.count().values[0]\n",
    "def label(A,B):\n",
    "    if A is True:\n",
    "        return 0\n",
    "    if B is True:\n",
    "        return 1\n",
    "    return 2\n",
    "def switch_label(l):\n",
    "    if l==2:\n",
    "        return 2\n",
    "    return 1-l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prediction(pred):\n",
    "    s = pred.shape[0]//2\n",
    "    pred0 = pred[0:s,:]\n",
    "    pred1 = pred[s:,:]\n",
    "    pred1 = pred1[:,[1,0,2]]\n",
    "    pred_out = pred0+pred1\n",
    "    return pred_out/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_A_B(df):\n",
    "    columnsTitles = [\"B_dist\",\"A_dist\",\"B_pos\", \"A_pos\",\"pron_pos\", \"B_vector\", \"A_vector\",\"pron_vector\",\"product_vector_B\",\"product_vector_A\",\"label\"]\n",
    "    df2=df.reindex(columns=columnsTitles).copy()\n",
    "    df2.columns = df.columns\n",
    "    df2.label = df2.label.map(switch_label)\n",
    "    return pd.concat([df,df2],axis = 0, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compute_loss(sub_df,test_data):\n",
    "    pred = torch.Tensor(np.log(sub_df.loc[:,['A','B','NEITHER']].values))\n",
    "    label = torch.LongTensor(list(test_data.label))\n",
    "    loss = torch.nn.NLLLoss()\n",
    "    loss_value = loss(pred,label).item()\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gap_train.drop(columns = ['ID', 'Text', 'Pronoun','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map', 'pron_idx'])\n",
    "train_data.A_vector = train_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data.B_vector = train_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data.pron_vector = train_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "train_data[\"product_vector_A\"] = train_data.A_vector*train_data.pron_vector\n",
    "train_data[\"product_vector_B\"] = train_data.B_vector*train_data.pron_vector\n",
    "train_data[\"label\"] = train_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "train_data = train_data.drop(columns= [\"A-coref\",\"B-coref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = gap_test.drop(columns = ['ID', 'Text', 'Pronoun','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map', 'pron_idx'])\n",
    "test_data.A_vector = test_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data.B_vector = test_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data.pron_vector = test_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "test_data[\"product_vector_A\"] = test_data.A_vector*test_data.pron_vector\n",
    "test_data[\"product_vector_B\"] = test_data.B_vector*test_data.pron_vector\n",
    "test_data[\"label\"] = test_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "test_data = test_data.drop(columns= [\"A-coref\",\"B-coref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = gap_valid.drop(columns = ['ID', 'Text', 'Pronoun','Pronoun-offset', 'A', 'A-offset',\n",
    "       'B', 'B-offset', 'URL', 'tokens', 'token_map',\n",
    "       'sentence_map', 'pron_idx'])\n",
    "valid_data.A_vector = valid_data.A_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data.B_vector = valid_data.B_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data.pron_vector = valid_data.pron_vector.map(lambda x:np.mean(x,axis = 0))\n",
    "valid_data[\"product_vector_A\"] = valid_data.A_vector*valid_data.pron_vector\n",
    "valid_data[\"product_vector_B\"] = valid_data.B_vector*valid_data.pron_vector\n",
    "valid_data[\"label\"] = valid_data.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "valid_data = valid_data.drop(columns= [\"A-coref\",\"B-coref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def masked_softmax(logits, mask, dim=-1, log_softmax=False):\n",
    "    \"\"\"Take the softmax of `logits` over given dimension, and set\n",
    "    entries to 0 wherever `mask` is 0.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Inputs to the softmax function.\n",
    "        mask (torch.Tensor): Same shape as `logits`, with 0 indicating\n",
    "            positions that should be assigned 0 probability in the output.\n",
    "        dim (int): Dimension over which to take softmax.\n",
    "        log_softmax (bool): Take log-softmax rather than regular softmax.\n",
    "            E.g., some PyTorch functions such as `F.nll_loss` expect log-softmax.\n",
    "\n",
    "    Returns:\n",
    "        probs (torch.Tensor): Result of taking masked softmax over the logits.\n",
    "    \"\"\"\n",
    "    mask = mask.type(torch.float32)\n",
    "    masked_logits = mask * logits + (1 - mask) * -1e30\n",
    "    softmax_fn = F.log_softmax if log_softmax else F.softmax\n",
    "    probs = softmax_fn(masked_logits, dim)\n",
    "\n",
    "    return probs\n",
    "class BiDAFAttention(nn.Module):\n",
    "    \"\"\"Bidirectional attention originally used by BiDAF.\n",
    "\n",
    "    Bidirectional attention computes attention in two directions:\n",
    "    The context attends to the query and the query attends to the context.\n",
    "    The output of this layer is the concatenation of [context, c2q_attention,\n",
    "    context * c2q_attention, context * q2c_attention]. This concatenation allows\n",
    "    the attention vector at each timestep, along with the embeddings from\n",
    "    previous layers, to flow through the attention layer to the modeling layer.\n",
    "    The output has shape (batch_size, context_len, 8 * hidden_size).\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): Size of hidden activations.\n",
    "        drop_prob (float): Probability of zero-ing out activations.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, drop_prob=0.6):\n",
    "        super(BiDAFAttention, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.c_weight = nn.Parameter(torch.zeros(hidden_size, 1))\n",
    "        self.q_weight = nn.Parameter(torch.zeros(hidden_size, 1))\n",
    "        self.cq_weight = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "        for weight in (self.c_weight, self.q_weight, self.cq_weight):\n",
    "            nn.init.xavier_uniform_(weight)\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        self.drop1 = nn.Dropout(self.drop_prob)  # (bs, c_len, hid_size)\n",
    "        self.drop2 = nn.Dropout(self.drop_prob)\n",
    "        self.output_layer1 = nn.Linear(4*hidden_size,64)\n",
    "        self.drop3 = nn.Dropout(self.drop_prob)\n",
    "        self.output_layer2 = nn.Linear(64,1)\n",
    "        self.drop4 = nn.Dropout(self.drop_prob)\n",
    "        nn.init.xavier_uniform_(self.output_layer1.weight)\n",
    "        nn.init.xavier_uniform_(self.output_layer2.weight)\n",
    "\n",
    "        \n",
    "    def forward(self, c, q, c_mask, q_mask):\n",
    "        batch_size, c_len, _ = c.size()\n",
    "        q_len = q.size(1)\n",
    "        s = self.get_similarity_matrix(c, q)        # (batch_size, c_len, q_len)\n",
    "        c_mask = c_mask.view(batch_size, c_len, 1)  # (batch_size, c_len, 1)\n",
    "        q_mask = q_mask.view(batch_size, 1, q_len)  # (batch_size, 1, q_len)\n",
    "        s1 = masked_softmax(s, q_mask, dim=2)       # (batch_size, c_len, q_len)\n",
    "        s2 = masked_softmax(s, c_mask, dim=1)       # (batch_size, c_len, q_len)\n",
    "\n",
    "        # (bs, c_len, q_len) x (bs, q_len, hid_size) => (bs, c_len, hid_size)\n",
    "        a = torch.bmm(s1, q)\n",
    "        # (bs, c_len, c_len) x (bs, c_len, hid_size) => (bs, c_len, hid_size)\n",
    "        b = torch.bmm(torch.bmm(s1, s2.transpose(1, 2)), c)\n",
    "\n",
    "        \n",
    "        x = torch.cat([c, a, c * a, c * b], dim=2)  # (bs, c_len, 4 * hid_size)\n",
    "        \n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        x = self.output_layer1(x)\n",
    "        \n",
    "        x = torch.nn.ReLU()(x)\n",
    "        \n",
    "        x = self.drop4(x)\n",
    "        \n",
    "        x = self.output_layer2(x)\n",
    "        \n",
    "        return x.squeeze(-1)\n",
    "\n",
    "    def get_similarity_matrix(self, c, q):\n",
    "        \"\"\"Get the \"similarity matrix\" between context and query (using the\n",
    "        terminology of the BiDAF paper).\n",
    "\n",
    "        A naive implementation as described in BiDAF would concatenate the\n",
    "        three vectors then project the result with a single weight matrix. This\n",
    "        method is a more memory-efficient implementation of the same operation.\n",
    "\n",
    "        See Also:\n",
    "            Equation 1 in https://arxiv.org/abs/1611.01603\n",
    "        \"\"\"\n",
    "        c_len, q_len = c.size(1), q.size(1)\n",
    "        c = self.drop1(c)  # (bs, c_len, hid_size)\n",
    "        q = self.drop2(q)  # (bs, q_len, hid_size)\n",
    "        #print (c.size())\n",
    "        #print (q.size())\n",
    "        # Shapes: (batch_size, c_len, q_len)\n",
    "        s0 = torch.matmul(c, self.c_weight).expand([-1, -1, q_len])\n",
    "        s1 = torch.matmul(q, self.q_weight).transpose(1, 2)\\\n",
    "                                           .expand([-1, c_len, -1])\n",
    "        s2 = torch.matmul(c * self.cq_weight, q.transpose(1, 2))\n",
    "        s = s0 + s1 + s2 + self.bias\n",
    "\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidaf = BiDAFAttention(256).cuda()\n",
    "EPOCHS = 150\n",
    "batch_size = 25\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight = torch.Tensor([1.0,1.0,10.0])).cuda()\n",
    "opt = torch.optim.Adam(bidaf.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector</th>\n",
       "      <th>A_dist</th>\n",
       "      <th>B_dist</th>\n",
       "      <th>A_pos</th>\n",
       "      <th>B_pos</th>\n",
       "      <th>pron_pos</th>\n",
       "      <th>A_idx</th>\n",
       "      <th>B_idx</th>\n",
       "      <th>A_vector</th>\n",
       "      <th>B_vector</th>\n",
       "      <th>pron_vector</th>\n",
       "      <th>product_vector_A</th>\n",
       "      <th>product_vector_B</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.85816926, -0.45762736, -0.08299036, -0.275...</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.693548</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>[52, 53, 54, 55, 56, 57, 58, 59]</td>\n",
       "      <td>[63, 64, 65]</td>\n",
       "      <td>[0.20037952, 0.2805402, -0.11013528, -0.492183...</td>\n",
       "      <td>[0.6041415, -0.13538514, 0.15207358, -0.076126...</td>\n",
       "      <td>[0.8586822, -1.2192798, 0.09194927, -0.4327072...</td>\n",
       "      <td>[0.17206234, -0.342057, -0.010126858, 0.212971...</td>\n",
       "      <td>[0.51876557, 0.16507237, 0.013983054, 0.032940...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.2511816, 0.24685939, -0.3399855, -0.624934...</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>[34, 35]</td>\n",
       "      <td>[46, 47, 48]</td>\n",
       "      <td>[0.067682624, 0.42009318, -0.009991955, -0.659...</td>\n",
       "      <td>[-0.438681, 0.48596978, -0.02240046, 0.0839553...</td>\n",
       "      <td>[-0.67510414, -0.39920956, 0.07794422, 0.43378...</td>\n",
       "      <td>[-0.04569282, -0.16770521, -0.0007788151, -0.2...</td>\n",
       "      <td>[0.29615536, -0.19400378, -0.0017459863, 0.036...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.8131293, 0.4440992, 0.7549018, 0.4792695, ...</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.405128</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>[89, 90, 91, 92, 93, 94, 95]</td>\n",
       "      <td>[99, 100]</td>\n",
       "      <td>[-0.40055174, -0.06318157, 0.08668772, -0.0661...</td>\n",
       "      <td>[0.67819536, 0.21544528, -0.39938855, 0.618967...</td>\n",
       "      <td>[0.72358626, -1.0747175, 0.050702423, -0.89089...</td>\n",
       "      <td>[-0.28983372, 0.06790234, 0.0043952777, 0.0588...</td>\n",
       "      <td>[0.49073285, -0.23154281, -0.020249967, -0.551...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[1.033058, -0.108201064, -0.1557167, 0.394294...</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.201521</td>\n",
       "      <td>0.216730</td>\n",
       "      <td>0.224335</td>\n",
       "      <td>[66, 67, 68]</td>\n",
       "      <td>[73, 74, 75]</td>\n",
       "      <td>[-0.33478984, -0.63129133, -0.5514479, 0.59391...</td>\n",
       "      <td>[0.50680375, 0.71950006, -0.25772676, 0.375783...</td>\n",
       "      <td>[1.2850071, 0.049930945, -0.26918668, 0.253453...</td>\n",
       "      <td>[-0.43020734, -0.031520974, 0.14844243, 0.1505...</td>\n",
       "      <td>[0.6512464, 0.035925318, 0.06937661, 0.0952435...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[1.3460386, 0.037629873, 0.55879486, -0.42726...</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.321839</td>\n",
       "      <td>0.362069</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>[71, 72, 73, 74]</td>\n",
       "      <td>[80, 81, 82, 83]</td>\n",
       "      <td>[0.25996214, -0.15495634, -0.5617872, -0.04960...</td>\n",
       "      <td>[0.2925861, 0.2669702, 0.03382411, -0.3578499,...</td>\n",
       "      <td>[-0.87992877, -0.07835998, 0.12989467, -0.2304...</td>\n",
       "      <td>[-0.22874817, 0.012142375, -0.07297316, 0.0114...</td>\n",
       "      <td>[-0.2574549, -0.020919777, 0.0043935715, 0.082...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              vector  A_dist  B_dist  \\\n",
       "0  [[0.85816926, -0.45762736, -0.08299036, -0.275...   0.018   0.004   \n",
       "1  [[0.2511816, 0.24685939, -0.3399855, -0.624934...   0.030   0.012   \n",
       "2  [[0.8131293, 0.4440992, 0.7549018, 0.4792695, ...   0.022   0.010   \n",
       "3  [[1.033058, -0.108201064, -0.1557167, 0.394294...   0.012   0.004   \n",
       "4  [[1.3460386, 0.037629873, 0.55879486, -0.42726...   0.056   0.042   \n",
       "\n",
       "      A_pos     B_pos  pron_pos                             A_idx  \\\n",
       "0  0.693548  0.806452  0.838710  [52, 53, 54, 55, 56, 57, 58, 59]   \n",
       "1  0.511111  0.711111  0.844444                          [34, 35]   \n",
       "2  0.405128  0.435897  0.461538      [89, 90, 91, 92, 93, 94, 95]   \n",
       "3  0.201521  0.216730  0.224335                      [66, 67, 68]   \n",
       "4  0.321839  0.362069  0.482759                  [71, 72, 73, 74]   \n",
       "\n",
       "              B_idx                                           A_vector  \\\n",
       "0      [63, 64, 65]  [0.20037952, 0.2805402, -0.11013528, -0.492183...   \n",
       "1      [46, 47, 48]  [0.067682624, 0.42009318, -0.009991955, -0.659...   \n",
       "2         [99, 100]  [-0.40055174, -0.06318157, 0.08668772, -0.0661...   \n",
       "3      [73, 74, 75]  [-0.33478984, -0.63129133, -0.5514479, 0.59391...   \n",
       "4  [80, 81, 82, 83]  [0.25996214, -0.15495634, -0.5617872, -0.04960...   \n",
       "\n",
       "                                            B_vector  \\\n",
       "0  [0.6041415, -0.13538514, 0.15207358, -0.076126...   \n",
       "1  [-0.438681, 0.48596978, -0.02240046, 0.0839553...   \n",
       "2  [0.67819536, 0.21544528, -0.39938855, 0.618967...   \n",
       "3  [0.50680375, 0.71950006, -0.25772676, 0.375783...   \n",
       "4  [0.2925861, 0.2669702, 0.03382411, -0.3578499,...   \n",
       "\n",
       "                                         pron_vector  \\\n",
       "0  [0.8586822, -1.2192798, 0.09194927, -0.4327072...   \n",
       "1  [-0.67510414, -0.39920956, 0.07794422, 0.43378...   \n",
       "2  [0.72358626, -1.0747175, 0.050702423, -0.89089...   \n",
       "3  [1.2850071, 0.049930945, -0.26918668, 0.253453...   \n",
       "4  [-0.87992877, -0.07835998, 0.12989467, -0.2304...   \n",
       "\n",
       "                                    product_vector_A  \\\n",
       "0  [0.17206234, -0.342057, -0.010126858, 0.212971...   \n",
       "1  [-0.04569282, -0.16770521, -0.0007788151, -0.2...   \n",
       "2  [-0.28983372, 0.06790234, 0.0043952777, 0.0588...   \n",
       "3  [-0.43020734, -0.031520974, 0.14844243, 0.1505...   \n",
       "4  [-0.22874817, 0.012142375, -0.07297316, 0.0114...   \n",
       "\n",
       "                                    product_vector_B  label  \n",
       "0  [0.51876557, 0.16507237, 0.013983054, 0.032940...      2  \n",
       "1  [0.29615536, -0.19400378, -0.0017459863, 0.036...      1  \n",
       "2  [0.49073285, -0.23154281, -0.020249967, -0.551...      1  \n",
       "3  [0.6512464, 0.035925318, 0.06937661, 0.0952435...      0  \n",
       "4  [-0.2574549, -0.020919777, 0.0043935715, 0.082...      1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/150], loss:1.0879\n",
      "epoch [2/150], loss:1.1244\n",
      "epoch [3/150], loss:0.8314\n",
      "epoch [4/150], loss:0.6250\n",
      "epoch [5/150], loss:0.9473\n",
      "epoch [6/150], loss:0.9767\n",
      "epoch [7/150], loss:0.4623\n",
      "epoch [8/150], loss:0.6780\n",
      "epoch [9/150], loss:0.6725\n",
      "epoch [10/150], loss:0.5636\n",
      "epoch [11/150], loss:0.5046\n",
      "epoch [12/150], loss:0.8632\n",
      "epoch [13/150], loss:0.4844\n",
      "epoch [14/150], loss:0.4487\n",
      "epoch [15/150], loss:0.6339\n",
      "epoch [16/150], loss:0.6400\n",
      "epoch [17/150], loss:0.6340\n",
      "epoch [18/150], loss:0.2961\n",
      "epoch [19/150], loss:0.5056\n",
      "epoch [20/150], loss:0.4926\n",
      "epoch [21/150], loss:0.4274\n",
      "epoch [22/150], loss:0.4474\n",
      "epoch [23/150], loss:0.4875\n",
      "epoch [24/150], loss:0.4926\n",
      "epoch [25/150], loss:0.6400\n",
      "epoch [26/150], loss:0.5452\n",
      "epoch [27/150], loss:0.4732\n",
      "epoch [28/150], loss:0.5090\n",
      "epoch [29/150], loss:0.2909\n",
      "epoch [30/150], loss:0.5318\n",
      "epoch [31/150], loss:0.9475\n",
      "epoch [32/150], loss:0.5565\n",
      "epoch [33/150], loss:0.3499\n",
      "epoch [34/150], loss:0.3302\n",
      "epoch [35/150], loss:0.4981\n",
      "epoch [36/150], loss:0.2509\n",
      "epoch [37/150], loss:0.3744\n",
      "epoch [38/150], loss:0.4542\n",
      "epoch [39/150], loss:0.2223\n",
      "epoch [40/150], loss:0.3993\n",
      "epoch [41/150], loss:0.4919\n",
      "epoch [42/150], loss:0.4104\n",
      "epoch [43/150], loss:0.4013\n",
      "epoch [44/150], loss:0.2513\n",
      "epoch [45/150], loss:0.2735\n",
      "epoch [46/150], loss:0.7509\n",
      "epoch [47/150], loss:0.4954\n",
      "epoch [48/150], loss:0.1645\n",
      "epoch [49/150], loss:0.4252\n",
      "epoch [50/150], loss:0.3698\n",
      "epoch [51/150], loss:0.4038\n",
      "epoch [52/150], loss:0.3114\n",
      "epoch [53/150], loss:0.2108\n",
      "epoch [54/150], loss:0.2973\n",
      "epoch [55/150], loss:0.2622\n",
      "epoch [56/150], loss:0.2492\n",
      "epoch [57/150], loss:0.2901\n",
      "epoch [58/150], loss:0.2918\n",
      "epoch [59/150], loss:0.5904\n",
      "epoch [60/150], loss:0.2917\n",
      "epoch [61/150], loss:0.2589\n",
      "epoch [62/150], loss:0.2728\n",
      "epoch [63/150], loss:0.4243\n",
      "epoch [64/150], loss:0.3576\n",
      "epoch [65/150], loss:0.3110\n",
      "epoch [66/150], loss:0.4046\n",
      "epoch [67/150], loss:0.2348\n",
      "epoch [68/150], loss:0.2980\n",
      "epoch [69/150], loss:0.4330\n",
      "epoch [70/150], loss:0.2897\n",
      "epoch [71/150], loss:0.4328\n",
      "epoch [72/150], loss:0.2658\n",
      "epoch [73/150], loss:0.3200\n",
      "epoch [74/150], loss:0.2489\n",
      "epoch [75/150], loss:0.2919\n",
      "epoch [76/150], loss:0.2180\n",
      "epoch [77/150], loss:0.3163\n",
      "epoch [78/150], loss:0.3830\n",
      "epoch [79/150], loss:0.5325\n",
      "epoch [80/150], loss:0.2432\n",
      "epoch [81/150], loss:0.3160\n",
      "epoch [82/150], loss:0.2743\n",
      "epoch [83/150], loss:0.4846\n",
      "epoch [84/150], loss:0.4155\n",
      "epoch [85/150], loss:0.2489\n",
      "epoch [86/150], loss:0.3633\n",
      "epoch [87/150], loss:0.2069\n",
      "epoch [88/150], loss:0.3568\n",
      "epoch [89/150], loss:0.2428\n",
      "epoch [90/150], loss:0.2771\n",
      "epoch [91/150], loss:0.2564\n",
      "epoch [92/150], loss:0.1495\n",
      "epoch [93/150], loss:0.2768\n",
      "epoch [94/150], loss:0.4124\n",
      "epoch [95/150], loss:0.3923\n",
      "epoch [96/150], loss:0.4361\n",
      "epoch [97/150], loss:0.1600\n",
      "epoch [98/150], loss:0.2258\n",
      "epoch [99/150], loss:0.2562\n",
      "epoch [100/150], loss:0.2224\n",
      "epoch [101/150], loss:0.2513\n",
      "epoch [102/150], loss:0.1299\n",
      "epoch [103/150], loss:0.3141\n",
      "epoch [104/150], loss:0.1771\n",
      "epoch [105/150], loss:0.3686\n",
      "epoch [106/150], loss:0.3168\n",
      "epoch [107/150], loss:0.3447\n",
      "epoch [108/150], loss:0.2197\n",
      "epoch [109/150], loss:0.1761\n",
      "epoch [110/150], loss:0.3226\n",
      "epoch [111/150], loss:0.2443\n",
      "epoch [112/150], loss:0.2933\n",
      "epoch [113/150], loss:0.2712\n",
      "epoch [114/150], loss:0.3363\n",
      "epoch [115/150], loss:0.2364\n",
      "epoch [116/150], loss:0.1488\n",
      "epoch [117/150], loss:0.3051\n",
      "epoch [118/150], loss:0.2253\n",
      "epoch [119/150], loss:0.4148\n",
      "epoch [120/150], loss:0.5657\n",
      "epoch [121/150], loss:0.2974\n",
      "epoch [122/150], loss:0.3242\n",
      "epoch [123/150], loss:0.3131\n",
      "epoch [124/150], loss:0.1680\n",
      "epoch [125/150], loss:0.1619\n",
      "epoch [126/150], loss:0.1521\n",
      "epoch [127/150], loss:0.2154\n",
      "epoch [128/150], loss:0.3745\n",
      "epoch [129/150], loss:0.2925\n",
      "epoch [130/150], loss:0.2771\n",
      "epoch [131/150], loss:0.3163\n",
      "epoch [132/150], loss:0.4614\n",
      "epoch [133/150], loss:0.2981\n",
      "epoch [134/150], loss:0.5825\n",
      "epoch [135/150], loss:0.3943\n",
      "epoch [136/150], loss:0.1198\n",
      "epoch [137/150], loss:0.2579\n",
      "epoch [138/150], loss:0.4229\n",
      "epoch [139/150], loss:0.2986\n",
      "epoch [140/150], loss:0.1817\n",
      "epoch [141/150], loss:0.2454\n",
      "epoch [142/150], loss:0.1815\n",
      "epoch [143/150], loss:0.0819\n",
      "epoch [144/150], loss:0.1915\n",
      "epoch [145/150], loss:0.2725\n",
      "epoch [146/150], loss:0.3046\n",
      "epoch [147/150], loss:0.4696\n",
      "epoch [148/150], loss:0.3730\n",
      "epoch [149/150], loss:0.3383\n",
      "epoch [150/150], loss:0.2365\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "for e in range(EPOCHS):\n",
    "    for b in range(0,train_data.shape[0],batch_size):\n",
    "        bidaf.train()\n",
    "        batch_data = train_data.vector[b:b+batch_size]\n",
    "        batch_label = train_data.label[b:b+batch_size]\n",
    "        batch_pron = train_data.pron_vector[b:b+batch_size]\n",
    "        batch_pron = torch.Tensor(np.array(list(batch_pron.values))).unsqueeze(1).cuda()\n",
    "        batch_data = pad_sequence([torch.Tensor(v) for v in batch_data]).cuda().transpose(0,1)\n",
    "        batch_padding = batch_data.mean(dim=1,keepdim = True)#torch.zeros(batch_data.size()[0],1,batch_data.size()[2]).cuda()*0.001\n",
    "        batch_data = torch.cat([batch_padding,batch_data],dim = 1)\n",
    "        batch_label = torch.LongTensor(list(batch_label)).cuda()\n",
    "        c_mask = torch.zeros_like(batch_data.mean(-1,keepdim = True)) != batch_data.mean(-1,keepdim = True)\n",
    "        q_mask = torch.zeros_like(batch_pron.mean(-1,keepdim = True)) != batch_pron.mean(-1,keepdim = True)\n",
    "        c_mask = c_mask.cuda()\n",
    "        q_mask = q_mask.cuda()\n",
    "        #print (batch_data.mean(-1,keepdim = True).shape)\n",
    "        output = bidaf(batch_data,batch_pron,c_mask,q_mask)\n",
    "        mask_A = [np.array(v)+1 for v in list(train_data.A_idx[b:b+batch_size])]\n",
    "        mask_B = [np.array(v)+1 for v in list(train_data.B_idx[b:b+batch_size])]\n",
    "        #neither_prob = output[:,0]\n",
    "        prob_list = []\n",
    "        for i,(v_A,v_B) in enumerate(zip(mask_A,mask_B)):\n",
    "            v_A = torch.LongTensor(v_A).cuda()\n",
    "            A_prob_ = output[i,v_A].sum()\n",
    "            v_B = torch.LongTensor(v_B).cuda()\n",
    "            B_prob_ = output[i,v_B].sum()\n",
    "            #other_prob = output[i,:].sum() - A_prob_ - B_prob_\n",
    "            other_prob = output[i,0].sum()\n",
    "            prob_list.append(torch.cat([A_prob_.view(1,1),B_prob_.view(1,1),other_prob.view(1,1)]).view(-1,3))\n",
    "        #print (prob_list)\n",
    "        pred_train = torch.cat(prob_list,dim = 0)\n",
    "        #print (pred_train.size())\n",
    "            \n",
    "        #batch_label = torch.LongTensor(batch_label).cuda()\n",
    "        loss = loss_fn(pred_train,batch_label)\n",
    "        #l2_norm = torch.norm(mlp.layers[-1].weight, p=2)\n",
    "        #loss += l2_norm*0.09\n",
    "        #l2_norm = torch.norm(mlp.layers[0].weight, p=2)\n",
    "        #loss += l2_norm*0.03\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(e + 1, EPOCHS, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bidaf_train = []    \n",
    "for b in range(0,train_data.shape[0],batch_size):\n",
    "    bidaf.eval()\n",
    "    batch_data = train_data.vector[b:b+batch_size]\n",
    "    batch_label = train_data.label[b:b+batch_size]\n",
    "    batch_pron = train_data.pron_vector[b:b+batch_size]\n",
    "    batch_pron = torch.Tensor(np.array(list(batch_pron.values))).unsqueeze(1).cuda()\n",
    "    batch_data = pad_sequence([torch.Tensor(v) for v in batch_data]).cuda().transpose(0,1)\n",
    "    batch_padding = batch_data.mean(dim=1,keepdim = True)#batch_padding = torch.zeros(batch_data.size()[0],1,batch_data.size()[2]).cuda()*0.001\n",
    "    batch_data = torch.cat([batch_padding,batch_data],dim = 1)\n",
    "    batch_label = torch.LongTensor(list(batch_label)).cuda()\n",
    "    c_mask = torch.zeros_like(batch_data.mean(-1,keepdim = True)) != batch_data.mean(-1,keepdim = True)\n",
    "    q_mask = torch.zeros_like(batch_pron.mean(-1,keepdim = True)) != batch_pron.mean(-1,keepdim = True)\n",
    "    c_mask = c_mask.cuda()\n",
    "    q_mask = q_mask.cuda()\n",
    "    #print (batch_data.mean(-1,keepdim = True).shape)\n",
    "    output = bidaf(batch_data,batch_pron,c_mask,q_mask)\n",
    "    mask_A = [np.array(v)+1 for v in list(train_data.A_idx[b:b+batch_size])]\n",
    "    mask_B = [np.array(v)+1 for v in list(train_data.B_idx[b:b+batch_size])]\n",
    "    #neither_prob = output[:,0]\n",
    "    prob_list = []\n",
    "    for i,(v_A,v_B) in enumerate(zip(mask_A,mask_B)):\n",
    "        v_A = torch.LongTensor(v_A).cuda()\n",
    "        A_prob_ = output[i,v_A].sum()\n",
    "        v_B = torch.LongTensor(v_B).cuda()\n",
    "        B_prob_ = output[i,v_B].sum()\n",
    "        #other_prob = output[i,:].sum() - A_prob_ - B_prob_\n",
    "        other_prob = output[i,0].sum()\n",
    "        prob_list.append(torch.cat([A_prob_.view(1,1),B_prob_.view(1,1),other_prob.view(1,1)]).view(-1,3))\n",
    "    #print (prob_list)\n",
    "    pred_bidaf_ = torch.cat(prob_list,dim = 0)\n",
    "    pred_bidaf_train.append(pred_bidaf_)\n",
    "pred_bidaf_train = torch.nn.Softmax(dim=1)(torch.cat(pred_bidaf_train,dim = 0)).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bidaf = []    \n",
    "for b in range(0,test_data.shape[0],batch_size):\n",
    "    bidaf.eval()\n",
    "    batch_data = test_data.vector[b:b+batch_size]\n",
    "    batch_label = test_data.label[b:b+batch_size]\n",
    "    batch_pron = test_data.pron_vector[b:b+batch_size]\n",
    "    batch_pron = torch.Tensor(np.array(list(batch_pron.values))).unsqueeze(1).cuda()\n",
    "    batch_data = pad_sequence([torch.Tensor(v) for v in batch_data]).cuda().transpose(0,1)\n",
    "    batch_padding = batch_data.mean(dim=1,keepdim = True)#batch_padding = torch.zeros(batch_data.size()[0],1,batch_data.size()[2]).cuda()*0.001\n",
    "    batch_data = torch.cat([batch_padding,batch_data],dim = 1)\n",
    "    batch_label = torch.LongTensor(list(batch_label)).cuda()\n",
    "    c_mask = torch.zeros_like(batch_data.mean(-1,keepdim = True)) != batch_data.mean(-1,keepdim = True)\n",
    "    q_mask = torch.zeros_like(batch_pron.mean(-1,keepdim = True)) != batch_pron.mean(-1,keepdim = True)\n",
    "    c_mask = c_mask.cuda()\n",
    "    q_mask = q_mask.cuda()\n",
    "    #print (batch_data.mean(-1,keepdim = True).shape)\n",
    "    output = bidaf(batch_data,batch_pron,c_mask,q_mask)\n",
    "    mask_A = [np.array(v)+1 for v in list(test_data.A_idx[b:b+batch_size])]\n",
    "    mask_B = [np.array(v)+1 for v in list(test_data.B_idx[b:b+batch_size])]\n",
    "    #neither_prob = output[:,0]\n",
    "    prob_list = []\n",
    "    for i,(v_A,v_B) in enumerate(zip(mask_A,mask_B)):\n",
    "        v_A = torch.LongTensor(v_A).cuda()\n",
    "        A_prob_ = output[i,v_A].sum()\n",
    "        v_B = torch.LongTensor(v_B).cuda()\n",
    "        B_prob_ = output[i,v_B].sum()\n",
    "        #other_prob = output[i,:].sum() - A_prob_ - B_prob_\n",
    "        other_prob = output[i,0].sum()\n",
    "        prob_list.append(torch.cat([A_prob_.view(1,1),B_prob_.view(1,1),other_prob.view(1,1)]).view(-1,3))\n",
    "    #print (prob_list)\n",
    "    pred_bidaf_ = torch.cat(prob_list,dim = 0)\n",
    "    pred_bidaf.append(pred_bidaf_)\n",
    "pred_bidaf = torch.nn.Softmax(dim=1)(torch.cat(pred_bidaf,dim = 0)).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open( \"./temp_result/biDAF_result_base\", \"wb\" )\n",
    "pickle.dump(pred_bidaf_train,  f)\n",
    "pickle.dump(pred_bidaf,  f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bidaf = np.clip(pred_bidaf,1e-15,1-1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>9.126874e-01</td>\n",
       "      <td>4.990904e-02</td>\n",
       "      <td>0.037404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>9.975595e-01</td>\n",
       "      <td>5.647483e-04</td>\n",
       "      <td>0.001876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>1.082062e-02</td>\n",
       "      <td>9.656751e-01</td>\n",
       "      <td>0.023504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>1.281967e-01</td>\n",
       "      <td>4.960704e-01</td>\n",
       "      <td>0.375733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>2.326543e-07</td>\n",
       "      <td>9.984231e-01</td>\n",
       "      <td>0.001577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>development-6</td>\n",
       "      <td>9.995282e-01</td>\n",
       "      <td>3.123801e-04</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>development-7</td>\n",
       "      <td>3.293931e-01</td>\n",
       "      <td>1.664367e-01</td>\n",
       "      <td>0.504170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>development-8</td>\n",
       "      <td>1.444150e-02</td>\n",
       "      <td>9.832012e-01</td>\n",
       "      <td>0.002357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>development-9</td>\n",
       "      <td>7.350466e-05</td>\n",
       "      <td>9.980374e-01</td>\n",
       "      <td>0.001889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>development-10</td>\n",
       "      <td>1.202069e-01</td>\n",
       "      <td>8.707060e-01</td>\n",
       "      <td>0.009087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>development-11</td>\n",
       "      <td>2.918506e-03</td>\n",
       "      <td>6.650535e-01</td>\n",
       "      <td>0.332028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>development-12</td>\n",
       "      <td>8.531080e-01</td>\n",
       "      <td>1.271959e-01</td>\n",
       "      <td>0.019696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>development-13</td>\n",
       "      <td>8.895031e-01</td>\n",
       "      <td>9.305702e-02</td>\n",
       "      <td>0.017440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>development-14</td>\n",
       "      <td>9.632010e-01</td>\n",
       "      <td>2.081319e-02</td>\n",
       "      <td>0.015986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>development-15</td>\n",
       "      <td>1.256028e-01</td>\n",
       "      <td>6.934826e-01</td>\n",
       "      <td>0.180914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>development-16</td>\n",
       "      <td>1.683799e-03</td>\n",
       "      <td>9.959726e-01</td>\n",
       "      <td>0.002344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>development-17</td>\n",
       "      <td>9.941980e-01</td>\n",
       "      <td>2.472591e-03</td>\n",
       "      <td>0.003329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>development-18</td>\n",
       "      <td>4.394324e-01</td>\n",
       "      <td>1.016898e-04</td>\n",
       "      <td>0.560466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>development-19</td>\n",
       "      <td>2.685195e-01</td>\n",
       "      <td>5.564572e-01</td>\n",
       "      <td>0.175023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>development-20</td>\n",
       "      <td>2.480851e-03</td>\n",
       "      <td>9.909397e-01</td>\n",
       "      <td>0.006579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>development-21</td>\n",
       "      <td>7.938166e-06</td>\n",
       "      <td>9.813406e-01</td>\n",
       "      <td>0.018651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>development-22</td>\n",
       "      <td>9.898484e-01</td>\n",
       "      <td>4.184300e-05</td>\n",
       "      <td>0.010110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>development-23</td>\n",
       "      <td>9.991666e-01</td>\n",
       "      <td>4.884182e-06</td>\n",
       "      <td>0.000828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>development-24</td>\n",
       "      <td>1.160969e-06</td>\n",
       "      <td>9.998839e-01</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>development-25</td>\n",
       "      <td>9.851010e-01</td>\n",
       "      <td>1.075129e-04</td>\n",
       "      <td>0.014791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>development-26</td>\n",
       "      <td>4.266799e-02</td>\n",
       "      <td>6.092968e-01</td>\n",
       "      <td>0.348035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>development-27</td>\n",
       "      <td>5.381328e-01</td>\n",
       "      <td>3.381739e-01</td>\n",
       "      <td>0.123693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>development-28</td>\n",
       "      <td>6.611884e-01</td>\n",
       "      <td>1.132341e-01</td>\n",
       "      <td>0.225577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>development-29</td>\n",
       "      <td>2.517617e-01</td>\n",
       "      <td>5.832466e-01</td>\n",
       "      <td>0.164992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>development-30</td>\n",
       "      <td>9.978979e-01</td>\n",
       "      <td>2.900472e-07</td>\n",
       "      <td>0.002102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>development-31</td>\n",
       "      <td>4.911422e-01</td>\n",
       "      <td>3.776830e-01</td>\n",
       "      <td>0.131175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>development-32</td>\n",
       "      <td>3.919264e-01</td>\n",
       "      <td>1.015435e-01</td>\n",
       "      <td>0.506530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>development-33</td>\n",
       "      <td>1.682672e-02</td>\n",
       "      <td>9.665658e-01</td>\n",
       "      <td>0.016608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>development-34</td>\n",
       "      <td>2.027096e-03</td>\n",
       "      <td>2.541127e-01</td>\n",
       "      <td>0.743860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>development-35</td>\n",
       "      <td>6.222361e-01</td>\n",
       "      <td>7.568012e-02</td>\n",
       "      <td>0.302084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>development-36</td>\n",
       "      <td>7.295349e-02</td>\n",
       "      <td>9.121819e-01</td>\n",
       "      <td>0.014865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>development-37</td>\n",
       "      <td>4.953003e-03</td>\n",
       "      <td>9.935852e-01</td>\n",
       "      <td>0.001462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>development-38</td>\n",
       "      <td>7.026998e-01</td>\n",
       "      <td>1.374643e-01</td>\n",
       "      <td>0.159836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID             A             B   NEITHER\n",
       "0    development-1  9.126874e-01  4.990904e-02  0.037404\n",
       "1    development-2  9.975595e-01  5.647483e-04  0.001876\n",
       "2    development-3  1.082062e-02  9.656751e-01  0.023504\n",
       "3    development-4  1.281967e-01  4.960704e-01  0.375733\n",
       "4    development-5  2.326543e-07  9.984231e-01  0.001577\n",
       "5    development-6  9.995282e-01  3.123801e-04  0.000159\n",
       "6    development-7  3.293931e-01  1.664367e-01  0.504170\n",
       "7    development-8  1.444150e-02  9.832012e-01  0.002357\n",
       "8    development-9  7.350466e-05  9.980374e-01  0.001889\n",
       "9   development-10  1.202069e-01  8.707060e-01  0.009087\n",
       "10  development-11  2.918506e-03  6.650535e-01  0.332028\n",
       "11  development-12  8.531080e-01  1.271959e-01  0.019696\n",
       "12  development-13  8.895031e-01  9.305702e-02  0.017440\n",
       "13  development-14  9.632010e-01  2.081319e-02  0.015986\n",
       "14  development-15  1.256028e-01  6.934826e-01  0.180914\n",
       "15  development-16  1.683799e-03  9.959726e-01  0.002344\n",
       "16  development-17  9.941980e-01  2.472591e-03  0.003329\n",
       "17  development-18  4.394324e-01  1.016898e-04  0.560466\n",
       "18  development-19  2.685195e-01  5.564572e-01  0.175023\n",
       "19  development-20  2.480851e-03  9.909397e-01  0.006579\n",
       "20  development-21  7.938166e-06  9.813406e-01  0.018651\n",
       "21  development-22  9.898484e-01  4.184300e-05  0.010110\n",
       "22  development-23  9.991666e-01  4.884182e-06  0.000828\n",
       "23  development-24  1.160969e-06  9.998839e-01  0.000115\n",
       "24  development-25  9.851010e-01  1.075129e-04  0.014791\n",
       "25  development-26  4.266799e-02  6.092968e-01  0.348035\n",
       "26  development-27  5.381328e-01  3.381739e-01  0.123693\n",
       "27  development-28  6.611884e-01  1.132341e-01  0.225577\n",
       "28  development-29  2.517617e-01  5.832466e-01  0.164992\n",
       "29  development-30  9.978979e-01  2.900472e-07  0.002102\n",
       "30  development-31  4.911422e-01  3.776830e-01  0.131175\n",
       "31  development-32  3.919264e-01  1.015435e-01  0.506530\n",
       "32  development-33  1.682672e-02  9.665658e-01  0.016608\n",
       "33  development-34  2.027096e-03  2.541127e-01  0.743860\n",
       "34  development-35  6.222361e-01  7.568012e-02  0.302084\n",
       "35  development-36  7.295349e-02  9.121819e-01  0.014865\n",
       "36  development-37  4.953003e-03  9.935852e-01  0.001462\n",
       "37  development-38  7.026998e-01  1.374643e-01  0.159836"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pred_lr = process_prediction(pred_lr)\n",
    "sub_df = pd.read_csv(\"./test_and_submit/sample_submission_stage_1.csv\")\n",
    "sub_df.loc[:, ['A','B','NEITHER']] = pred_bidaf\n",
    "\n",
    "\n",
    "sub_df.to_csv(\"./test_and_submit/submission+model+bidaf@\"+str(datetime.datetime.now())+\".csv\", index=False)\n",
    "\n",
    "sub_df.head(38)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5191367864608765"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(sub_df,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
