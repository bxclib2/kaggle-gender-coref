{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train= pd.read_pickle('./temp_result/train_kaggle_processed')\n",
    "gap_test= pd.read_pickle('./temp_result/test_kaggle_processed')\n",
    "gap_valid= pd.read_pickle('./temp_result/valid_kaggle_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(A,B):\n",
    "    if A is True:\n",
    "        return 0\n",
    "    if B is True:\n",
    "        return 1\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>...</th>\n",
       "      <th>B_dist</th>\n",
       "      <th>A_pos</th>\n",
       "      <th>B_pos</th>\n",
       "      <th>pron_pos</th>\n",
       "      <th>A_idx</th>\n",
       "      <th>B_idx</th>\n",
       "      <th>pron_idx</th>\n",
       "      <th>A_vector</th>\n",
       "      <th>B_vector</th>\n",
       "      <th>pron_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>test-1996</td>\n",
       "      <td>The sole exception was Wimbledon, where she pl...</td>\n",
       "      <td>She</td>\n",
       "      <td>479</td>\n",
       "      <td>Goolagong Cawley</td>\n",
       "      <td>400</td>\n",
       "      <td>True</td>\n",
       "      <td>Peggy Michel</td>\n",
       "      <td>432</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>[102, 103, 104, 105, 106]</td>\n",
       "      <td>[110, 111, 112, 113, 114]</td>\n",
       "      <td>[123]</td>\n",
       "      <td>[[-0.3187313, 0.46942553, 0.541859, 0.0963608,...</td>\n",
       "      <td>[[-0.37300664, -0.315253, 0.040081125, 0.53844...</td>\n",
       "      <td>[[-0.7649605, -0.4366843, 0.38759252, 0.534622...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>test-1997</td>\n",
       "      <td>According to news reports, both Moore and Fily...</td>\n",
       "      <td>her</td>\n",
       "      <td>338</td>\n",
       "      <td>Esther Sheryl Wood</td>\n",
       "      <td>263</td>\n",
       "      <td>True</td>\n",
       "      <td>Barbara Morgan</td>\n",
       "      <td>404</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.790476</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>[62, 63, 64, 65, 66]</td>\n",
       "      <td>[96, 97, 98, 99, 100]</td>\n",
       "      <td>[80]</td>\n",
       "      <td>[[-0.211019, -0.036700837, -0.027644457, -0.34...</td>\n",
       "      <td>[[-0.2204119, 0.5043704, -0.4220593, 0.2550880...</td>\n",
       "      <td>[[-0.14545152, -0.01811517, 0.8394235, -0.9302...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>test-1998</td>\n",
       "      <td>In June 2009, due to the popularity of the Sab...</td>\n",
       "      <td>She</td>\n",
       "      <td>328</td>\n",
       "      <td>Kayla</td>\n",
       "      <td>364</td>\n",
       "      <td>True</td>\n",
       "      <td>Natasha Henstridge</td>\n",
       "      <td>412</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.461078</td>\n",
       "      <td>0.520958</td>\n",
       "      <td>0.419162</td>\n",
       "      <td>[97, 98]</td>\n",
       "      <td>[111, 112, 113, 114, 115, 116]</td>\n",
       "      <td>[90]</td>\n",
       "      <td>[[0.20733312, 0.7342792, 0.03844169, 0.3875529...</td>\n",
       "      <td>[[-0.21739139, 0.05455912, 0.1606792, 0.440639...</td>\n",
       "      <td>[[-0.8260292, -0.59548515, 0.04561778, -0.1782...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>test-1999</td>\n",
       "      <td>She was delivered to the Norwegian passenger s...</td>\n",
       "      <td>she</td>\n",
       "      <td>305</td>\n",
       "      <td>Irma</td>\n",
       "      <td>255</td>\n",
       "      <td>True</td>\n",
       "      <td>Bergen</td>\n",
       "      <td>274</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.495327</td>\n",
       "      <td>[67, 68]</td>\n",
       "      <td>[72, 73]</td>\n",
       "      <td>[80]</td>\n",
       "      <td>[[-0.805637, -0.5994673, -0.5566493, 0.0688710...</td>\n",
       "      <td>[[0.1469631, 0.8217377, 0.33704713, 0.3077713,...</td>\n",
       "      <td>[[-0.97279143, 0.2389887, -0.22996032, -0.0032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>test-2000</td>\n",
       "      <td>Meg and Vicky each have three siblings, and ha...</td>\n",
       "      <td>her</td>\n",
       "      <td>275</td>\n",
       "      <td>Vicky Austin</td>\n",
       "      <td>217</td>\n",
       "      <td>True</td>\n",
       "      <td>Polly O'Keefe</td>\n",
       "      <td>260</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.361538</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>[48, 49, 50, 51, 52, 53]</td>\n",
       "      <td>[60, 61, 62, 63, 64, 65, 66]</td>\n",
       "      <td>[68]</td>\n",
       "      <td>[[-0.2613115, -0.83725923, -0.47856662, 0.4980...</td>\n",
       "      <td>[[-0.20457774, 0.9843961, -0.32598388, 0.59570...</td>\n",
       "      <td>[[-0.56508684, 0.04315004, -0.17839415, -0.330...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                               Text Pronoun  \\\n",
       "1995  test-1996  The sole exception was Wimbledon, where she pl...     She   \n",
       "1996  test-1997  According to news reports, both Moore and Fily...     her   \n",
       "1997  test-1998  In June 2009, due to the popularity of the Sab...     She   \n",
       "1998  test-1999  She was delivered to the Norwegian passenger s...     she   \n",
       "1999  test-2000  Meg and Vicky each have three siblings, and ha...     her   \n",
       "\n",
       "      Pronoun-offset                   A  A-offset  A-coref  \\\n",
       "1995             479    Goolagong Cawley       400     True   \n",
       "1996             338  Esther Sheryl Wood       263     True   \n",
       "1997             328               Kayla       364     True   \n",
       "1998             305                Irma       255     True   \n",
       "1999             275        Vicky Austin       217     True   \n",
       "\n",
       "                       B  B-offset  B-coref  ... B_dist     A_pos     B_pos  \\\n",
       "1995        Peggy Michel       432    False  ...  0.020  0.220779  0.233766   \n",
       "1996      Barbara Morgan       404    False  ... -0.028  0.504762  0.790476   \n",
       "1997  Natasha Henstridge       412    False  ... -0.034  0.461078  0.520958   \n",
       "1998              Bergen       274    False  ...  0.008  0.420561  0.457944   \n",
       "1999       Polly O'Keefe       260    False  ...  0.006  0.300000  0.361538   \n",
       "\n",
       "      pron_pos                      A_idx                           B_idx  \\\n",
       "1995  0.259740  [102, 103, 104, 105, 106]       [110, 111, 112, 113, 114]   \n",
       "1996  0.657143       [62, 63, 64, 65, 66]           [96, 97, 98, 99, 100]   \n",
       "1997  0.419162                   [97, 98]  [111, 112, 113, 114, 115, 116]   \n",
       "1998  0.495327                   [67, 68]                        [72, 73]   \n",
       "1999  0.384615   [48, 49, 50, 51, 52, 53]    [60, 61, 62, 63, 64, 65, 66]   \n",
       "\n",
       "      pron_idx                                           A_vector  \\\n",
       "1995     [123]  [[-0.3187313, 0.46942553, 0.541859, 0.0963608,...   \n",
       "1996      [80]  [[-0.211019, -0.036700837, -0.027644457, -0.34...   \n",
       "1997      [90]  [[0.20733312, 0.7342792, 0.03844169, 0.3875529...   \n",
       "1998      [80]  [[-0.805637, -0.5994673, -0.5566493, 0.0688710...   \n",
       "1999      [68]  [[-0.2613115, -0.83725923, -0.47856662, 0.4980...   \n",
       "\n",
       "                                               B_vector  \\\n",
       "1995  [[-0.37300664, -0.315253, 0.040081125, 0.53844...   \n",
       "1996  [[-0.2204119, 0.5043704, -0.4220593, 0.2550880...   \n",
       "1997  [[-0.21739139, 0.05455912, 0.1606792, 0.440639...   \n",
       "1998  [[0.1469631, 0.8217377, 0.33704713, 0.3077713,...   \n",
       "1999  [[-0.20457774, 0.9843961, -0.32598388, 0.59570...   \n",
       "\n",
       "                                            pron_vector  \n",
       "1995  [[-0.7649605, -0.4366843, 0.38759252, 0.534622...  \n",
       "1996  [[-0.14545152, -0.01811517, 0.8394235, -0.9302...  \n",
       "1997  [[-0.8260292, -0.59548515, 0.04561778, -0.1782...  \n",
       "1998  [[-0.97279143, 0.2389887, -0.22996032, -0.0032...  \n",
       "1999  [[-0.56508684, 0.04315004, -0.17839415, -0.330...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gap_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train[\"seq_length\"] = gap_train.vector.map(lambda x: x.shape[0])\n",
    "gap_valid[\"seq_length\"] = gap_valid.vector.map(lambda x: x.shape[0])\n",
    "gap_test[\"seq_length\"] = gap_test.vector.map(lambda x: x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train = gap_train.sort_values(by=['seq_length'], ascending=False)\n",
    "gap_valid = gap_train.sort_values(by=['seq_length'], ascending=False)\n",
    "gap_test = gap_train.sort_values(by=['seq_length'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train = gap_train.reset_index(drop = True)\n",
    "gap_valid = gap_valid.reset_index(drop = True)\n",
    "gap_test = gap_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_train[\"label\"] = gap_train.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "gap_test[\"label\"] = gap_test.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)\n",
    "gap_valid[\"label\"] = gap_valid.apply(lambda x:label(x[\"A-coref\"],x[\"B-coref\"]),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        #print (\"scores\")\n",
    "        #print (scores.shape)\n",
    "        #print (\"scores\")\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "            #print (\"inside\")\n",
    "            #print (mask.size())\n",
    "            #print (\"inside\")\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        #print (x.size())\n",
    "        #print (mask.size())\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "c = copy.deepcopy\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "attn = MultiHeadedAttention(h = 8, d_model = 1024)\n",
    "ff = PositionwiseFeedForward(d_model = 1024, d_ff = 1024, dropout = 0.5)\n",
    "enc = Encoder(EncoderLayer(size = 1024, self_attn = c(attn),feed_forward = c(ff), dropout = 0.5), N = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(df,start,batch_size,dim = 1024):\n",
    "    num = df.shape[0]\n",
    "    dim = df.vector.iloc[0].shape[1]\n",
    "    #print (dim)\n",
    "    end = min(num,start + batch_size)\n",
    "    batch_data = df.iloc[start:end,:]\n",
    "    max_seq_length = df.loc[start,\"seq_length\"]\n",
    "    if max_seq_length<32:\n",
    "        max_seq_length = 32\n",
    "    padded = np.zeros((end-start,max_seq_length,dim))\n",
    "    sentence_vector = list(batch_data.vector)\n",
    "    for i,v in enumerate(sentence_vector):\n",
    "        padded[i,0:v.shape[0],:] = v   \n",
    "    label = batch_data.label\n",
    "    A_mask = np.zeros((end-start,max_seq_length,max_seq_length))\n",
    "    B_mask = np.zeros((end-start,max_seq_length,max_seq_length))\n",
    "    pron_mask = np.zeros((end-start,max_seq_length,max_seq_length))\n",
    "    for i in range(0,end-start):\n",
    "        A_mask[i,df.A_idx.loc[start+i],df.A_idx.loc[start+i]] = 0.001      \n",
    "        B_mask[i,df.B_idx.loc[start+i],df.B_idx.loc[start+i]] = 0.001\n",
    "        pron_mask[i,df.pron_idx.iloc[start+i],df.pron_idx.iloc[start+i]] = 0.001\n",
    "    A_mask = torch.tensor(A_mask).unsqueeze(-3)\n",
    "    B_mask = torch.tensor(B_mask).unsqueeze(-3)\n",
    "    pron_mask = torch.tensor(pron_mask).unsqueeze(-3)\n",
    "    return torch.Tensor(padded),torch.LongTensor(np.array(label)),torch.cat((A_mask,B_mask,pron_mask),dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "class CNN(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, enc):\n",
    "        super(CNN, self).__init__()\n",
    "        self.enc = enc\n",
    "        self.cnn = nn.Sequential(\n",
    "  nn.Conv2d(19, 64, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2)),\n",
    "  nn.ReLU(),\n",
    "  nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "  nn.Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2)),\n",
    "  nn.ReLU(),\n",
    "  nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "  nn.Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "  nn.ReLU(),\n",
    "  nn.Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "  nn.ReLU(),\n",
    "  nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "  nn.ReLU(),\n",
    "  nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "  nn.Conv2d(256, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "  nn.AdaptiveAvgPool2d(output_size = (1,1))\n",
    ")\n",
    "    def forward(self, padded,mask_feature):\n",
    "        mask = (padded.mean(dim = -1) != 0).unsqueeze(-2)\n",
    "        self.out = enc(padded,mask)\n",
    "        cnn_input = torch.cat((enc.layers[0].self_attn.attn,enc.layers[1].self_attn.attn,mask_feature.float()),dim = 1)\n",
    "        cnn_out = self.cnn(cnn_input)\n",
    "        return cnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN(enc)\n",
    "# This was important from their code. \n",
    "# Initialize parameters with Glorot / fan_avg.\n",
    "for p in cnn_model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "cnn_model.cuda()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "loss_fn.cuda()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at the end of epoch 0\n",
      "1.05144464969635\n",
      "loss at the end of epoch 1\n",
      "1.05144464969635\n",
      "loss at the end of epoch 2\n",
      "1.05144464969635\n",
      "loss at the end of epoch 3\n",
      "1.05144464969635\n",
      "loss at the end of epoch 4\n",
      "1.05144464969635\n",
      "loss at the end of epoch 5\n",
      "1.05144464969635\n",
      "loss at the end of epoch 6\n",
      "1.05144464969635\n",
      "loss at the end of epoch 7\n",
      "1.05144464969635\n",
      "loss at the end of epoch 8\n",
      "1.05144464969635\n",
      "loss at the end of epoch 9\n",
      "1.05144464969635\n",
      "loss at the end of epoch 10\n",
      "1.05144464969635\n",
      "loss at the end of epoch 11\n",
      "1.05144464969635\n",
      "loss at the end of epoch 12\n",
      "1.05144464969635\n",
      "loss at the end of epoch 13\n",
      "1.05144464969635\n",
      "loss at the end of epoch 14\n",
      "1.05144464969635\n",
      "loss at the end of epoch 15\n",
      "1.05144464969635\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5cd890f8d219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mdata_num\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgap_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-ff30b9e589a7>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(df, start, batch_size, dim)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msentence_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCH = 20\n",
    "BATCH_SIZE = 8\n",
    "start = 0\n",
    "data_num = 2000\n",
    "for e in range(EPOCH):\n",
    "    start = 0\n",
    "    while start <= data_num-1:\n",
    "        padded, label, mask_feature = get_batch(gap_train,start,BATCH_SIZE)\n",
    "        padded = padded.cuda()\n",
    "        label = label.cuda()\n",
    "        cnn_model.train()\n",
    "        mask_feature = mask_feature.cuda()\n",
    "        start = start + BATCH_SIZE\n",
    "        cnn_out = cnn_model(padded,mask_feature)\n",
    "        cnn_out = cnn_out.view(-1,3)\n",
    "        #print (label.size())\n",
    "        cnn_out = torch.nn.Softmax(dim = -1)(cnn_out)\n",
    "        loss = loss_fn(cnn_out,label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print (\"loss at the end of epoch \"+str(e))\n",
    "    print (loss.item())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
